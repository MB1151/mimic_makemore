{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to train a simple neural network to generate names?\n",
    "# 2) How to encode characters into one-hot encoding?\n",
    "# 3) What is the similarity between rule-based name generator and neural network-based name generator?\n",
    "#\n",
    "# Resources:\n",
    "# 1) ADD RESOURCES TO UNDERSTAND NEURAL NETWORKS\n",
    "# 2) https://github.com/MB1151/mimic_micro_autograd\n",
    "#       -- To understand how backpropagation works in detail.\n",
    "#       -- This is my other repository where I have implemented autograd from scratch.\n",
    "#       -- Not everything is needed from this repository, but it is a good resource to understand how backpropagation works.\n",
    "# 3) makemore_part1/building_makemore_step_by_step/step_3_model_quality.ipynb\n",
    "#       -- To understand how to calculate loss and use it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../Data/names.txt\"\n",
    "BOUND_CHARACTER = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['albonsha',\n",
       " 'beenapreethi',\n",
       " 'thushniha',\n",
       " 'aakaksha',\n",
       " 'dumeethran',\n",
       " 'luhit',\n",
       " 'valam',\n",
       " 'harinyai',\n",
       " 'sakthikaa',\n",
       " 'kaveetha']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATA_PATH, \"r\") as f:\n",
    "    names = [name.strip() for name in f.readlines()]\n",
    "\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Create character to integer mapping.\n",
    "char_to_int = {char: idx + 1 for idx, char in enumerate(string.ascii_lowercase)}\n",
    "char_to_int[BOUND_CHARACTER] = 0\n",
    "print(char_to_int)\n",
    "print(\"-\" * 100)\n",
    "int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
    "print(int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our name-generator neural network, we will use one-hot encoding to represent characters.\n",
    "# One-hot encoding is a way to represent characters in a number format so that the neural network can understand them.\n",
    "#\n",
    "# In our text, we have 27 characters (26 alphabets + 1 dot). As you can see, we have assigned a unique number to each \n",
    "# character in the `char_to_int` dictionary. However, this is not the best way to represent characters in a neural \n",
    "# network. This is because the neural network may think that the characters with higher numbers are more important\n",
    "# than the characters with lower numbers. This is not true. The numbers assigned to the characters are just for\n",
    "# representation purposes. They don't have any meaning. \n",
    "# For example, the character 'a' is represented as 1 and the character 'z' is represented as 26. But, this doesn't \n",
    "# mean that 'z' is more important than 'a'. So, we need to represent characters in a way that the neural network can \n",
    "# understand that all characters are equally important. This is where one-hot encoding comes into play.\n",
    "#\n",
    "# In one-hot encoding, we will represent each character as a 27-dimensional vector where all elements are zero except \n",
    "# the element corresponding to the character index which is 1.\n",
    "# For example, the character 'a' will be represented as [0, 1, 0, 0, ..., 0] where the second element is 1 and all\n",
    "# other elements are zero. Similarly, the character 'z' will be represented as [0, 0, 0, ..., 1] where the last \n",
    "# element is 1 and all other elements are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a very simple neural network for the name generation. We will improve this model later on. For now, \n",
    "# the idea is to build a neural network which is very similar to the rule-based model that we built in \n",
    "# `building_makemore_step_by_step/step_2_rule_based_name_generator.ipynb`\n",
    "#\n",
    "# The neural network will take a single character as input and predict a single character as output. As explained \n",
    "# above, every character is represented as a 27-dimensional vector. So, our network should take an input that has\n",
    "# 27 features. \n",
    "# We want the neural network to output a 27-dimensional vector, where each value in the vector corresponds to the\n",
    "# probability of the prediction to be the character corresponding to that particular position. \n",
    "# For now, we will only have 1 layer in the neural network.\n",
    "#\n",
    "# So, the architecture is as follows:\n",
    "# Number of layers = 1\n",
    "# Input = [27, 1] -- Ignoring batching in this calculation.\n",
    "# Number of neurons = Number of outputs = 27\n",
    "#\n",
    "# We will also not have bias in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_list: 545276\n",
      "input_list[:10]: [0, 1, 12, 2, 15, 14, 19, 8, 1, 0]\n",
      "target_list[:10]: [1, 12, 2, 15, 14, 19, 8, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Let's create the input and output as required by the neural network. We want to use backward to run the gradient\n",
    "# descent algorithm and train the model. Hence, we need to create all the inputs, targets in the form of tensors\n",
    "# and the computations between tensors.\n",
    "input_list = []\n",
    "target_list = []\n",
    "for name in names:\n",
    "    name = BOUND_CHARACTER + name + BOUND_CHARACTER\n",
    "    for first_char, second_char in zip(name, name[1:]):\n",
    "        first_char_idx = char_to_int[first_char]\n",
    "        second_char_idx = char_to_int[second_char]\n",
    "        input_list.append(first_char_idx)\n",
    "        target_list.append(second_char_idx)\n",
    "\n",
    "print(f\"shape of input_list: {len(input_list)}\")\n",
    "print(f\"input_list[:10]: {input_list[:10]}\")\n",
    "print(f\"target_list[:10]: {target_list[:10]}\")\n",
    "\n",
    "# Let's create tensors out of these lists to use with neural networks.\n",
    "inputs = torch.tensor(data=input_list, dtype=torch.int64)\n",
    "targets = torch.tensor(data=target_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded_inputs: torch.Size([545276, 27])\n",
      "encoded_inputs[:3]: tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Now, let's generate the one-hot encoding for the inputs.\n",
    "# We cast the one-hot vectors into float since we need to pass them through the neural network which\n",
    "# expects float inputs.\n",
    "encoded_inputs = F.one_hot(inputs, num_classes=len(char_to_int)).float()\n",
    "print(f\"shape of encoded_inputs: {encoded_inputs.shape}\")\n",
    "print(f\"encoded_inputs[:3]: {encoded_inputs[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a seed to keep our outputs consistent across multiple runs of the notebook.\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27])\n",
      "tensor([[-0.1117, -0.4966,  0.1631, -0.8817,  0.0539,  0.6684, -0.0597, -0.4675,\n",
      "         -0.2153,  0.8840, -0.7584, -0.3689, -0.3424, -1.4020,  0.3206, -1.0219,\n",
      "          0.7988, -0.0923, -0.7049, -1.6024,  0.2891,  0.4899, -0.3853, -0.7120,\n",
      "         -0.1706, -1.4594,  0.2207],\n",
      "        [ 0.2463, -1.3248,  0.6970, -0.6631,  1.2158, -1.4949,  0.8810, -1.1786,\n",
      "         -0.9340, -0.5675, -0.2772, -2.1834,  0.3668,  0.9380,  0.0078, -0.3139,\n",
      "         -1.1567,  1.8409, -1.0174,  1.2192,  0.1601,  1.5985, -0.0469, -1.5270,\n",
      "         -2.0143, -1.5173,  0.3877],\n",
      "        [-1.1849,  0.6897,  1.3232,  1.8169,  0.6808,  0.7244,  0.0323, -1.6593,\n",
      "         -1.8773,  0.7372,  0.9257,  0.9247,  0.1825, -0.0737,  0.3147, -1.0369,\n",
      "          0.2100,  0.6144,  0.0628, -0.3297, -1.7970,  0.8728,  0.7670, -0.1138,\n",
      "         -0.9428,  0.7540,  0.1407],\n",
      "        [-0.6937, -0.6159, -0.7295,  0.4308,  0.2862, -0.2481,  0.2040,  0.8519,\n",
      "         -1.4102, -0.1071, -0.8018,  0.2771,  2.5599, -1.6952,  0.1885,  0.7388,\n",
      "          1.5903, -0.1947, -0.2415,  1.3204,  1.5997, -1.0792, -0.3396, -0.6780,\n",
      "         -0.1261, -1.6770,  1.2068],\n",
      "        [ 0.5722,  0.0653, -0.0235,  0.8876,  0.9570, -0.5510,  2.6617,  2.1479,\n",
      "         -0.8555, -0.7208,  1.3755,  0.9988,  0.1469, -0.9053, -0.3587,  1.6374,\n",
      "          0.6897, -0.5844,  0.9078,  0.4848, -0.2632, -0.5432, -1.6406,  0.9295,\n",
      "          1.2907,  0.2612, -0.5862],\n",
      "        [-1.5105, -2.0155,  0.6964, -0.6676, -0.8424,  0.5289, -0.5447,  0.8097,\n",
      "          1.1226, -0.6129,  0.2229,  0.4593, -1.7031, -1.2777, -0.7428,  0.9711,\n",
      "          0.3551, -1.5174, -0.3566,  0.9455, -2.1716, -0.8039, -0.4530, -0.2217,\n",
      "         -2.0901, -0.1771, -0.8981],\n",
      "        [-0.9834,  0.7564,  0.3474,  0.2856, -0.0495,  0.6225, -1.9217, -1.3176,\n",
      "          1.2286,  0.7025, -0.0170, -0.2830, -0.6446, -0.0199,  0.1123, -0.6601,\n",
      "         -0.6243, -0.7416,  0.3422, -0.2873, -0.6726,  0.8843,  0.7211,  0.2214,\n",
      "         -0.2737,  0.8612, -0.0609],\n",
      "        [ 2.1073, -0.9930,  1.4080, -0.7945,  0.6144, -0.2730,  1.6561,  1.3940,\n",
      "          0.6060,  0.2209, -0.8245,  0.7289, -0.7336,  1.5624, -1.0208, -0.0175,\n",
      "         -0.9194, -0.9389, -1.9421,  0.2329, -1.1014, -1.2473, -0.7485, -0.9792,\n",
      "          0.8285, -0.2501,  0.1602],\n",
      "        [ 0.7295, -0.4441,  0.8214, -0.6015,  0.9069,  1.5691, -0.1108, -0.2573,\n",
      "          0.5631,  0.0629, -0.0639,  3.0292,  0.1858, -0.1704,  1.0270,  0.0704,\n",
      "          0.3586, -2.3594, -1.3954,  0.8675, -0.2252,  0.1193, -1.0772,  0.2516,\n",
      "         -0.2196,  0.6062,  0.0473],\n",
      "        [ 0.0124,  0.3775,  0.7332, -0.4915, -1.1049, -0.0391,  0.0416, -1.0468,\n",
      "          0.0335, -1.0545, -0.1973,  0.8522, -0.4066, -1.4113,  0.0200, -0.4115,\n",
      "         -0.3398, -0.5563, -0.6508, -0.2179,  0.7700, -0.9989,  1.1106,  0.0311,\n",
      "          1.5696,  1.5389,  0.2984],\n",
      "        [ 0.5561, -0.6868, -1.1067,  2.2689, -2.2510, -1.0115,  0.3828,  1.2363,\n",
      "         -1.0535, -0.9404,  0.8782, -0.8629, -0.9917,  0.8485,  0.7376, -1.3158,\n",
      "         -0.4562, -1.7884, -1.4166, -0.9863,  0.6091,  0.6668, -1.1206, -1.6680,\n",
      "          1.0685, -0.1831,  0.6445],\n",
      "        [-0.5592, -0.4480, -0.6476,  0.2083, -0.7378, -0.3218, -0.9491,  1.0704,\n",
      "         -1.0470,  1.3770,  0.3989, -1.2153, -0.0316, -0.8687, -0.3510, -0.2000,\n",
      "         -1.4447,  0.8402, -0.8668, -0.9728, -1.2452, -0.0115, -1.0545, -0.8748,\n",
      "          2.2994, -1.4453, -0.7350],\n",
      "        [-1.6073, -1.1039, -1.8680,  0.8119,  1.1538, -0.5540, -0.8932, -0.3465,\n",
      "         -0.5365, -0.2293, -0.2484,  0.3544, -0.6692, -2.3203,  0.5021, -0.1246,\n",
      "          0.6526, -1.3451,  0.9636,  0.8350, -2.7675,  1.4802,  0.2942,  1.3924,\n",
      "          2.1130, -0.0856,  1.2580],\n",
      "        [-1.8494, -1.2253, -0.6665, -0.5090, -0.5193,  0.4296, -1.3418, -0.3649,\n",
      "          1.2697,  0.5690,  0.4715,  0.4616, -1.3822,  0.8789, -0.0990,  1.2043,\n",
      "          0.1998, -0.0446, -0.2705,  1.1501,  1.4566,  2.7640, -0.3187, -0.1316,\n",
      "         -0.6221, -1.4301, -1.4317],\n",
      "        [ 2.1155, -1.1853,  1.2042, -0.3535,  0.3415,  1.3207, -0.5345,  2.2011,\n",
      "         -1.2147, -0.8642, -1.0132, -0.3668,  1.3064, -2.1880, -2.0552,  0.7384,\n",
      "          0.9454, -1.3522,  0.6629,  0.5436,  0.5328, -0.7291, -1.3583, -0.1869,\n",
      "         -1.3208,  1.4487, -1.5155],\n",
      "        [-1.2364, -0.6758, -0.0800, -0.4060,  0.2415,  0.2559, -0.0605, -0.1539,\n",
      "          1.4019, -0.0312,  1.4725, -0.4815, -0.8533,  0.2310,  0.7535, -0.6287,\n",
      "         -0.7702,  1.6713,  0.7765,  0.4891, -0.5015,  1.6710, -0.8182,  0.7551,\n",
      "          1.3663, -0.4598,  0.6668],\n",
      "        [-0.7839,  2.1911,  0.3531,  0.8087,  2.7847, -0.5777, -1.5113,  1.4643,\n",
      "         -0.5434,  1.1363,  0.7060,  0.5937,  0.1270,  0.6934, -0.5838,  0.6028,\n",
      "          0.1503,  1.1833, -0.7282, -1.2537, -0.6925, -2.8562, -0.8612, -0.4060,\n",
      "          0.2722,  0.2987,  0.4729],\n",
      "        [-0.4718, -1.1712, -1.9521, -0.7573, -0.1884,  0.2220,  0.0738,  1.0746,\n",
      "          0.7868, -0.3379, -0.1411, -0.1263, -0.9179,  1.0803, -0.7998, -0.8772,\n",
      "         -1.0245, -0.8813,  1.9939,  0.1048,  1.1134, -0.0314,  0.4465,  1.4797,\n",
      "         -1.0431,  0.7966,  0.4633],\n",
      "        [ 0.5287,  0.0931,  1.2819, -0.6290,  0.7272, -1.4299, -0.2680,  0.8157,\n",
      "          0.2061, -0.7030,  0.5768, -1.1972,  0.2659, -0.7091,  0.1462, -1.7971,\n",
      "         -0.3575, -1.4564,  1.7173, -1.0184, -2.1019, -1.6981, -1.0374,  0.9230,\n",
      "         -0.6661, -0.6180,  1.3107],\n",
      "        [ 1.4736, -0.2851, -0.3726, -1.6563, -1.4151, -0.1897,  1.7860,  1.8001,\n",
      "          0.9789, -0.4627, -1.4979, -0.4024, -0.1688, -0.7295, -2.3339, -0.4907,\n",
      "         -1.4583,  0.3990,  0.3334, -0.4139,  0.0513, -1.3811,  0.8958,  0.5972,\n",
      "          0.1110,  0.7745, -0.6644],\n",
      "        [ 0.6153, -0.5206, -1.7862,  0.0206, -0.2503, -0.1452,  1.3390, -2.4515,\n",
      "         -1.4909, -0.7994,  1.4661,  0.2993, -0.1399,  0.0665, -2.0364, -0.4900,\n",
      "          0.0224,  0.2284, -0.1856,  0.4137,  1.5247, -0.0229, -1.1519, -0.3090,\n",
      "         -1.4955, -0.7526, -0.6525],\n",
      "        [ 2.4224,  0.5197, -0.3198, -0.6821,  0.2706,  0.6929, -2.3074,  0.4205,\n",
      "          0.2500,  0.2796,  1.2181,  0.1727,  0.2069,  1.4007, -1.0216, -0.5113,\n",
      "         -0.5124, -0.8315, -1.9167, -0.3378, -0.8732,  0.6991, -1.7233,  0.0584,\n",
      "         -0.2683,  0.3429, -2.3405],\n",
      "        [-0.8842, -0.3780, -1.6314,  0.1557,  0.6457, -0.8066,  0.7882, -1.6625,\n",
      "         -0.7064,  0.4478, -0.4300,  1.1491,  0.0163,  0.0612, -2.6981, -0.5081,\n",
      "          0.8743, -0.2358,  0.2435,  0.5431,  0.7498,  0.1238,  0.1317,  0.1211,\n",
      "         -1.0315, -0.7186, -1.0173],\n",
      "        [ 1.2741,  0.1503,  0.3640, -0.3623,  0.9709,  0.8835, -0.0742, -0.6366,\n",
      "         -0.6977,  0.6453, -2.9267,  0.2747, -0.6125, -0.2925, -1.0978,  0.6955,\n",
      "         -0.7705,  1.2134,  0.8028, -0.4904,  0.2571, -0.2913, -0.6594, -1.3994,\n",
      "         -0.5482,  0.6185, -1.0135],\n",
      "        [-0.6152, -0.4278,  0.2729, -0.5790,  0.0437,  1.0218,  1.0363,  1.2032,\n",
      "         -0.1005, -0.8075,  0.3934, -0.6425,  0.2323,  0.7118,  0.1631, -0.9765,\n",
      "         -0.2103, -0.9933, -0.2486, -0.6136, -0.8202,  2.1842, -0.4091, -0.0667,\n",
      "          1.0655,  0.5185, -0.8217],\n",
      "        [-1.6376, -0.5362, -0.1288,  0.9668,  0.2971,  0.2009, -0.6132, -1.2794,\n",
      "         -0.4037,  0.0097, -0.4733,  1.2124, -0.1739, -0.7859, -0.6041, -0.2325,\n",
      "          1.0328, -0.3480,  1.4594, -0.0560, -1.2836,  0.7126,  1.1081,  0.0047,\n",
      "          0.6673, -0.5581, -0.9543],\n",
      "        [ 0.2181,  2.0906,  0.4160, -1.3930, -0.5809,  1.4967,  1.2538,  1.2667,\n",
      "         -0.0936,  0.1063, -0.0317, -0.9988,  0.0941,  1.1970, -0.9961, -1.6909,\n",
      "          0.3310,  0.6250, -0.7701, -0.2146,  0.7421,  0.0571,  0.0732, -0.4882,\n",
      "         -0.1455,  0.9776,  0.3895]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We need to set the generator everytime this code is run.\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# We have 27 inputs and 27 outputs. So, the weights should be of shape (27, 27).\n",
    "# We will initialize the weights randomly.\n",
    "# The 'requires_grad=True' argument is used to tell PyTorch that we want to compute the gradients of the weights\n",
    "# during the backward pass -- This will be explained below in detail.\n",
    "weights = torch.randn(size=(27, 27), dtype=torch.float32, requires_grad=True)\n",
    "print(weights.shape)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Let's do a sample run of the neural network to understand how it works.\n",
    "# Assume the input is 'b'. The one-hot encoding of 'b' is [0, 1, 0, 0, ..., 0].\n",
    "sample_input = torch.zeros(size=(1,27), dtype=torch.float32)\n",
    "sample_input[0][2] = 1\n",
    "print(sample_input.shape)\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27])\n",
      "tensor([[-1.1849,  0.6897,  1.3232,  1.8169,  0.6808,  0.7244,  0.0323, -1.6593,\n",
      "         -1.8773,  0.7372,  0.9257,  0.9247,  0.1825, -0.0737,  0.3147, -1.0369,\n",
      "          0.2100,  0.6144,  0.0628, -0.3297, -1.7970,  0.8728,  0.7670, -0.1138,\n",
      "         -0.9428,  0.7540,  0.1407]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Just to understand, this is multiplication of a row vector with a matrix. So, the output will be a row vector.\n",
    "# The output row vector will be the 2nd row of the weights matrix since the input has 1 only in the 2nd position.\n",
    "# \n",
    "# This is equivalent to the rule-based model where we use the pre-computed probabilities of the current character\n",
    "# to predict the next character. In the rule based model, if the current character is 'b', we simply take the row\n",
    "# 2nd row from the 'char_probs' tensor and use it to predict the next character. If you observe keenly, we are \n",
    "# doing the same thing here. Here, the current character is 'b' and we simply take the 2nd row of the weights. Our\n",
    "# model setup and one-hot encoding is just making this neural network equivalent to the rule-based model.\n",
    "sample_output = sample_input @ weights\n",
    "print(sample_output.shape)\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOFTMAX\n",
    "\n",
    "Softmax function is used to convert a list of values into probabilities.\n",
    "\n",
    "Let's say we have a list of values $[x_{0}, x_{1}, x_{2}, ..., x_{n - 1}]$. Now, we need to convert this <br>\n",
    "into a list of probabilities. \n",
    "\n",
    "$$input\\_list = [x_{0}, x_{1}, x_{2}, ..., x_{n - 1}]$$\n",
    "$$\\implies logits = [e^{x_{0}}, e^{x_{1}}, e^{x_{2}}, ..., e^{x_{n - 1}}]$$\n",
    "\n",
    "Applying exponentiation brings the value range to $(0, \\infty)$. Now, we normalize the $logits$ to obtain probabilities.\n",
    "\n",
    "First, let's calculate the sum of logits to normalize.\n",
    "\n",
    "$$logit\\_sum = \\sum_{i=0}^{n-1} e^{x_{i}}$$\n",
    "\n",
    "$$\\implies probabilities = [\\frac{e^{x_{0}}}{logit\\_sum}, \\frac{e^{x_{1}}}{logit\\_sum}, \\frac{e^{x_{2}}}{logit\\_sum}, ..., \\frac{e^{x_{n-1}}}{logit\\_sum}]$$\n",
    "\n",
    "This whole process to obtain probabilities from a list of values is called softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27])\n",
      "tensor([[0.3058, 1.9931, 3.7554, 6.1530, 1.9754, 2.0634, 1.0328, 0.1903, 0.1530,\n",
      "         2.0902, 2.5237, 2.5211, 1.2003, 0.9290, 1.3698, 0.3546, 1.2337, 1.8486,\n",
      "         1.0648, 0.7192, 0.1658, 2.3936, 2.1532, 0.8925, 0.3895, 2.1255, 1.1511]],\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The 'sample_output' is a row vector containing some values. We need to convert these values into probabilities. \n",
    "# As we know, the probabilities are always between 0 and 1. So, we need to limit the values of the output row \n",
    "# vector between 0 and 1. We can achieve this by using the softmax function.\n",
    "sample_run_logits = sample_output.exp()\n",
    "print(sample_run_logits.shape)\n",
    "print(sample_run_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27])\n",
      "tensor([[0.0072, 0.0466, 0.0878, 0.1439, 0.0462, 0.0483, 0.0242, 0.0045, 0.0036,\n",
      "         0.0489, 0.0590, 0.0590, 0.0281, 0.0217, 0.0320, 0.0083, 0.0289, 0.0432,\n",
      "         0.0249, 0.0168, 0.0039, 0.0560, 0.0504, 0.0209, 0.0091, 0.0497, 0.0269]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the logits to get probabilities.\n",
    "sample_run_probs = sample_run_logits / sample_run_logits.sum(dim=1, keepdim=True)\n",
    "print(sample_run_probs.shape)\n",
    "print(sample_run_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's verify that the sum of the probabilities is 1.\n",
    "print(sample_run_probs.sum(dim=1, keepdim=True).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14393503963947296\n"
     ]
    }
   ],
   "source": [
    "# Let's say the target is 'c' which is represented as integer 3. So, we extract the probability of the next \n",
    "# character being 'c' from the 'sample_run_probs'.\n",
    "sample_run_target_prob = sample_run_probs[0][3]\n",
    "print(sample_run_target_prob.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.938393235206604\n"
     ]
    }
   ],
   "source": [
    "# Now, let's calculate the loss using negative log-likelihood as we explained in the previous notebook ('step_3_model_quality.ipynb').\n",
    "sample_run_loss = -torch.log(sample_run_target_prob)\n",
    "print(sample_run_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last step is to update the weights using the gradients. Let's do this in the actual training loop below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CREATION CONTINUED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell just combines all the code explained in SAMPLE RUN SECTION above.\n",
    "model_output = encoded_inputs @ weights\n",
    "logits = model_output.exp()\n",
    "probs = logits / logits.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's verify that the sum of the probabilities is 1 for a few of the rows.\n",
    "print(probs.sum(dim=1, keepdim=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0357, 0.0243, 0.0470, 0.0165, 0.0421, 0.0779, 0.0376, 0.0250, 0.0322,\n",
      "         0.0967, 0.0187, 0.0276, 0.0284, 0.0098, 0.0550, 0.0144, 0.0888, 0.0364,\n",
      "         0.0197, 0.0080, 0.0533, 0.0652, 0.0272, 0.0196, 0.0337, 0.0093, 0.0498],\n",
      "        [0.0342, 0.0071, 0.0536, 0.0138, 0.0901, 0.0060, 0.0645, 0.0082, 0.0105,\n",
      "         0.0151, 0.0202, 0.0030, 0.0386, 0.0683, 0.0269, 0.0195, 0.0084, 0.1684,\n",
      "         0.0097, 0.0904, 0.0314, 0.1321, 0.0255, 0.0058, 0.0036, 0.0059, 0.0394],\n",
      "        [0.0045, 0.0075, 0.0035, 0.0510, 0.0718, 0.0130, 0.0093, 0.0160, 0.0132,\n",
      "         0.0180, 0.0177, 0.0323, 0.0116, 0.0022, 0.0374, 0.0200, 0.0435, 0.0059,\n",
      "         0.0593, 0.0522, 0.0014, 0.0995, 0.0304, 0.0911, 0.1873, 0.0208, 0.0797],\n",
      "        [0.0072, 0.0466, 0.0878, 0.1439, 0.0462, 0.0483, 0.0242, 0.0045, 0.0036,\n",
      "         0.0489, 0.0590, 0.0590, 0.0281, 0.0217, 0.0320, 0.0083, 0.0289, 0.0432,\n",
      "         0.0249, 0.0168, 0.0039, 0.0560, 0.0504, 0.0209, 0.0091, 0.0497, 0.0269],\n",
      "        [0.0064, 0.0111, 0.0202, 0.0146, 0.0279, 0.0283, 0.0206, 0.0188, 0.0889,\n",
      "         0.0212, 0.0954, 0.0135, 0.0093, 0.0276, 0.0465, 0.0117, 0.0101, 0.1164,\n",
      "         0.0476, 0.0357, 0.0133, 0.1164, 0.0097, 0.0466, 0.0858, 0.0138, 0.0426]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([ 1, 12,  2, 15, 14])\n"
     ]
    }
   ],
   "source": [
    "# Each row of the 'probs' tensor contains the probabilities for the next character for the corresponding input.\n",
    "# probs[0][0] = The probability of the next character being '.' for the zeroth input.\n",
    "# probs[0][1] = The probability of the next character being 'a' for the zeroth input.\n",
    "# probs[0][2] = The probability of the next character being 'b' for the zeroth input.\n",
    "# probs[2][3] = The probability of the next character being 'c' for the second input.\n",
    "# ...\n",
    "# In general, \n",
    "# probs[i][j] = The probability of the next character being 'int_to_char[j]' for the ith input.\n",
    "print(probs[:5])\n",
    "# Targets tensor contains the indices of the target characters or the true next characters for each input.\n",
    "print(targets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0243, 0.0386, 0.0035, 0.0083, 0.0465], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's extract the probabilities assigned by the model to the target characters for each input.\n",
    "target_probs = probs[range(len(targets)), targets]\n",
    "# Let's understand what the printed values mean.\n",
    "# In the above cell, the targets for the first 5 inputs are [1, 12, 2, 15, 14].\n",
    "#\n",
    "# Let's consider the first target which is 1. This means that the target character (or the true output) is 'a'.\n",
    "# For the zeroth input, the model assigned a probability of probs[0][1] to the character 'a'. So, the target_probs[0]\n",
    "# should be equal to probs[0][1] = 0.0243 which is correct.\n",
    "# \n",
    "# Now, let's consider the second target which is 12. This means that the target character (or the true output) is 'k'.\n",
    "# For the second input, the model assigned a probability of probs[1][12] to the character 'k'. So, the target_probs[1]\n",
    "# should be equal to probs[1][12] = 0.0386 which is correct.\n",
    "#\n",
    "# Similarly, we can verify the rest of the values.\n",
    "print(target_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.79728102684021\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate the loss for the entire dataset using negative log-likelihood as explained in the earlier\n",
    "# notebook (building_makemore_step_by_step/step_3_model_quality.ipynb).\n",
    "# We calculate the mean of the negative log-likelihoods for all the inputs.\n",
    "loss = -torch.log(target_probs).mean()\n",
    "# The loss is going to be a single positive number. Lower the loss, better the model.\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# The last step is to update the weights using the gradients. First, let's check what the gradients are.\n",
    "# It should be None since we haven't calculated the gradients yet. Let's verify.\n",
    "print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.9656e-03, -1.0170e-02,  1.3612e-03, -4.4705e-04, -1.3427e-03,\n",
      "          7.2652e-03,  3.8438e-03, -4.1995e-04,  3.5887e-04,  8.9341e-03,\n",
      "         -2.9020e-03, -5.7328e-03,  5.0600e-04, -7.0073e-03, -9.7385e-04,\n",
      "          1.1834e-03,  2.9324e-03,  4.0049e-03, -4.5081e-03, -1.0690e-02,\n",
      "          1.3846e-04,  5.9499e-03, -5.0418e-03,  1.9740e-03,  3.7370e-03,\n",
      "         -2.1661e-03,  5.2468e-03],\n",
      "        [-2.6616e-02, -6.9503e-03,  7.5360e-03,  2.0742e-03,  1.3182e-02,\n",
      "          2.4826e-04,  1.2590e-02, -1.6344e-03, -1.2588e-03, -1.1006e-03,\n",
      "         -1.6279e-04, -5.4089e-03, -1.0817e-03,  5.1614e-03, -4.4274e-02,\n",
      "          3.7938e-03, -3.6045e-04,  3.3272e-02, -1.9928e-02,  7.1790e-03,\n",
      "         -4.2244e-03,  2.5588e-02, -2.1162e-03,  7.9215e-04,  5.6764e-04,\n",
      "         -4.1262e-03,  7.2589e-03],\n",
      "        [-1.2791e-04, -3.2168e-03,  8.6052e-04,  1.5944e-03,  4.7394e-04,\n",
      "         -2.0195e-04,  2.6794e-04,  4.5694e-05, -2.6837e-03, -1.3577e-03,\n",
      "          6.3454e-04,  6.5036e-04,  2.8386e-04,  2.3916e-04,  3.2968e-04,\n",
      "         -3.5550e-04,  3.1455e-04,  4.7956e-04, -2.2260e-04,  1.7373e-04,\n",
      "          3.2009e-05,  1.4047e-04,  5.5859e-04,  2.2969e-04,  1.0105e-04,\n",
      "          4.5971e-04,  2.9679e-04],\n",
      "        [ 1.0927e-05, -3.2802e-04,  4.7698e-05,  1.2287e-04,  1.3172e-04,\n",
      "          1.8508e-05,  1.2132e-04,  2.3190e-04, -3.8124e-03, -3.3989e-05,\n",
      "          4.4373e-05, -1.0789e-04,  1.2687e-03,  1.8161e-05,  1.1762e-04,\n",
      "          1.5759e-04,  4.8530e-04,  8.1426e-05,  6.8538e-05,  3.6318e-04,\n",
      "          4.8253e-04,  1.8951e-05,  7.0445e-05,  5.0223e-05,  8.7215e-05,\n",
      "         -4.7527e-05,  3.3071e-04],\n",
      "        [-2.5699e-04, -3.7488e-03,  3.0520e-04,  7.5417e-04,  2.7312e-04,\n",
      "         -2.7221e-03,  4.7664e-03,  2.8316e-03, -5.8370e-03, -2.4092e-03,\n",
      "          1.3107e-03,  8.8607e-04,  2.9412e-04, -3.0342e-05,  1.7216e-04,\n",
      "          1.5275e-03,  6.6206e-04,  1.8567e-04, -8.0468e-04,  4.6572e-04,\n",
      "          2.2850e-04, -9.9490e-04, -3.8124e-05,  7.2457e-04,  1.2109e-03,\n",
      "          6.2076e-05,  1.8169e-04],\n",
      "        [-1.8299e-03, -1.7089e-04,  2.8168e-03,  7.7849e-04, -1.8935e-04,\n",
      "         -5.9631e-03,  9.0381e-04,  3.1484e-03,  4.3697e-03,  5.9967e-04,\n",
      "          1.7018e-03,  1.3123e-03, -1.9513e-03, -4.0824e-04, -5.4309e-03,\n",
      "          4.2452e-03,  1.3847e-03,  3.5187e-04, -1.4085e-03, -2.7073e-03,\n",
      "         -2.8742e-03,  7.2208e-04, -1.2397e-03,  1.2385e-03,  1.8403e-04,\n",
      "         -1.0516e-04,  5.2110e-04],\n",
      "        [-5.2484e-05, -2.8250e-04,  3.0410e-05,  2.8585e-05,  1.8612e-05,\n",
      "         -1.6816e-05, -1.8863e-05,  5.7529e-06,  6.9735e-05, -3.7323e-05,\n",
      "          1.9289e-05,  1.6189e-05,  7.6087e-06,  2.1061e-05,  2.0369e-05,\n",
      "         -3.5688e-06,  1.1508e-05,  1.0234e-05, -1.7431e-05, -3.8554e-07,\n",
      "         -3.7062e-06,  2.2678e-05,  4.4187e-05,  2.6809e-05,  1.6340e-05,\n",
      "          4.7165e-05,  1.6546e-05],\n",
      "        [ 1.9560e-03, -5.5704e-03,  1.0978e-03,  1.2030e-04,  4.6671e-04,\n",
      "         -9.1295e-04,  1.4162e-03,  1.0274e-03, -1.5369e-04, -9.9428e-04,\n",
      "          1.0386e-04,  5.1632e-04,  1.0596e-04,  1.2621e-03, -2.3270e-04,\n",
      "         -7.2469e-04,  9.3126e-05,  1.0572e-04, -1.9414e-04,  3.0639e-04,\n",
      "          8.8025e-05, -1.0392e-03,  9.3050e-05,  9.2367e-05,  6.1904e-04,\n",
      "          3.4451e-05,  3.1731e-04],\n",
      "        [-5.8747e-03, -3.4440e-02,  3.2505e-03,  7.4109e-04,  3.5267e-03,\n",
      "          2.1876e-03,  1.3027e-03,  1.0844e-03,  2.4715e-03, -1.4416e-02,\n",
      "          1.3398e-03,  2.9636e-02,  1.6401e-03,  2.6624e-04,  2.5246e-03,\n",
      "          4.8761e-04,  1.9607e-03,  1.3423e-04, -2.4187e-03,  3.2640e-03,\n",
      "          7.2512e-04, -3.3897e-03, -3.6123e-04,  8.1154e-04,  1.1718e-03,\n",
      "          8.5287e-04,  1.5210e-03],\n",
      "        [-1.6454e-02,  2.3858e-03,  4.0916e-03,  8.2096e-04, -9.6859e-04,\n",
      "          1.7918e-03,  2.3511e-03, -3.5977e-04,  1.5522e-03,  6.8669e-04,\n",
      "          9.9313e-04, -1.0292e-03, -2.7252e-03, -1.1415e-03, -8.0806e-03,\n",
      "          1.4981e-03,  7.0421e-04,  1.2841e-03, -4.2730e-03, -6.6343e-03,\n",
      "         -4.4652e-03,  8.2628e-04,  4.0325e-03,  2.3260e-03,  1.1096e-02,\n",
      "          6.8658e-03,  2.8243e-03],\n",
      "        [-5.0260e-04, -6.3937e-03,  1.0667e-04,  3.5483e-03,  2.9471e-05,\n",
      "         -2.3148e-03,  5.3814e-04,  1.2597e-03,  8.7696e-06, -1.5714e-03,\n",
      "          7.8782e-04,  1.1266e-04,  1.2879e-04,  8.4085e-04,  6.8662e-04,\n",
      "         -4.0772e-04,  2.1789e-04,  6.1369e-05,  4.6471e-06,  1.0752e-04,\n",
      "          6.6930e-04,  1.1698e-04,  6.4644e-05,  2.7041e-05,  1.0683e-03,\n",
      "          1.0568e-04,  6.9908e-04],\n",
      "        [-9.9728e-04, -1.2733e-02,  4.3896e-04,  1.0492e-03,  4.0718e-04,\n",
      "         -1.1102e-03,  3.3553e-04,  2.5189e-03, -8.1444e-04,  1.2987e-03,\n",
      "          1.2862e-03, -1.8117e-04,  8.2153e-04,  3.3062e-04,  5.4054e-04,\n",
      "         -4.8783e-04,  1.9342e-04,  2.0084e-03, -7.6539e-04, -2.2086e-03,\n",
      "         -6.9546e-05, -1.5987e-03,  2.8363e-04,  3.5776e-04,  8.6407e-03,\n",
      "          4.2905e-05,  4.1198e-04],\n",
      "        [-2.4564e-03, -9.8184e-03, -4.8057e-06,  1.1575e-03,  1.5845e-03,\n",
      "         -1.0433e-03,  2.0656e-04,  3.2684e-04,  2.6417e-04, -3.3222e-03,\n",
      "          3.7442e-04,  5.8247e-04, -1.4980e-03, -1.2795e-04,  7.5888e-04,\n",
      "         -3.7466e-04,  8.6913e-04,  1.3362e-04,  1.2933e-03,  1.0624e-03,\n",
      "         -2.1914e-05,  1.6462e-03,  4.3397e-05,  2.0898e-03,  4.3598e-03,\n",
      "          8.0307e-05,  1.8340e-03],\n",
      "        [-1.6629e-03, -1.1612e-02, -2.4594e-04,  2.9462e-04,  2.3994e-04,\n",
      "         -1.0003e-03,  1.4086e-04,  3.5397e-04,  1.8652e-03, -3.9721e-03,\n",
      "          8.0671e-04,  7.7073e-04,  6.3769e-05,  9.1283e-04,  3.8912e-04,\n",
      "          6.0506e-04,  3.3349e-04,  5.1544e-04, -7.2963e-05,  1.2695e-03,\n",
      "          2.2596e-03,  6.8897e-03,  3.4784e-04,  4.7246e-04,  2.8930e-04,\n",
      "         -3.7171e-04,  1.1775e-04],\n",
      "        [-1.1659e-02, -1.5335e-02,  5.3250e-03,  7.7671e-04, -2.1083e-03,\n",
      "          2.6050e-03,  9.7532e-04,  1.3213e-02,  3.5374e-04, -1.2766e-02,\n",
      "         -6.6731e-04, -1.7033e-04,  6.1233e-03, -2.6231e-04, -1.1014e-03,\n",
      "          2.4223e-03,  4.1253e-03,  4.2951e-04,  3.0286e-03,  1.7146e-03,\n",
      "         -3.1266e-03, -1.7357e-03,  1.0228e-04,  1.3229e-03,  4.4512e-04,\n",
      "          5.6361e-03,  3.3336e-04],\n",
      "        [-1.9260e-04,  1.1561e-04, -1.6782e-04,  1.1016e-04, -1.2777e-04,\n",
      "          3.6014e-04,  2.7050e-04, -4.3900e-04,  5.3589e-04,  2.1847e-04,\n",
      "          8.4228e-04, -3.5933e-04, -5.2932e-04, -3.4456e-04, -5.9979e-04,\n",
      "         -1.5862e-03, -3.1090e-04,  1.6096e-03, -3.1522e-04, -7.3830e-04,\n",
      "         -8.6920e-04,  1.0938e-03, -3.9816e-04,  1.6051e-04,  1.1750e-03,\n",
      "          2.4399e-05,  4.6179e-04],\n",
      "        [-4.2081e-04, -2.5323e-03,  3.2165e-04,  5.1698e-04,  3.7427e-03,\n",
      "         -3.4346e-04,  5.0988e-05,  9.9938e-04, -1.6161e-05, -3.0157e-04,\n",
      "          4.5169e-04,  3.8914e-04,  2.3490e-04,  4.5314e-04,  9.7723e-05,\n",
      "         -3.7182e-04,  1.1828e-05,  7.5460e-04, -3.9121e-03,  1.7822e-06,\n",
      "         -5.8591e-05, -1.2595e-03,  9.2178e-05,  1.5398e-04,  3.0341e-04,\n",
      "          2.6936e-04,  3.7086e-04],\n",
      "        [-3.4497e-05, -9.9194e-06, -3.1713e-06,  1.6400e-06,  1.0628e-06,\n",
      "          2.5328e-06,  3.7651e-06,  1.0243e-05,  7.6814e-06, -1.0343e-05,\n",
      "          3.0372e-06,  3.0823e-06,  1.3967e-06,  1.0302e-05,  1.5718e-06,\n",
      "          1.4547e-06, -5.7846e-07,  1.4488e-06,  2.5686e-05,  2.0500e-06,\n",
      "          1.0649e-05, -6.2632e-05,  5.4656e-06,  1.5359e-05,  1.2323e-06,\n",
      "          5.9233e-06,  5.5587e-06],\n",
      "        [-8.8603e-04, -1.9454e-02,  5.7673e-03,  7.2545e-04,  2.9903e-03,\n",
      "         -3.0180e-03,  1.2511e-03,  3.4955e-03,  1.8886e-03, -9.3448e-03,\n",
      "          2.5927e-03,  2.0642e-04,  1.8644e-03, -1.9847e-04,  9.0000e-04,\n",
      "         -1.0206e-03,  9.8029e-04,  3.8105e-04,  8.8358e-03, -2.0310e-03,\n",
      "         -1.8917e-03, -4.9169e-03, -3.3479e-04,  4.0613e-03,  8.4387e-04,\n",
      "          2.5095e-04,  6.0615e-03],\n",
      "        [ 4.8688e-03, -7.5734e-03,  8.8685e-04,  2.1996e-04,  3.0598e-04,\n",
      "         -2.5627e-04,  7.9654e-03,  8.0508e-03, -2.4617e-02, -2.1120e-03,\n",
      "          2.8387e-04,  6.0127e-04,  1.0472e-03,  1.6146e-04, -1.4569e-04,\n",
      "          1.6267e-04,  2.3175e-04,  1.9898e-03,  3.1936e-04,  5.4708e-04,\n",
      "          8.1855e-04, -2.6098e-03,  2.8045e-03,  1.2413e-03,  1.4920e-03,\n",
      "          2.6307e-03,  6.8524e-04],\n",
      "        [ 2.4431e-04, -3.5080e-03,  2.1476e-04,  1.2807e-03,  1.0896e-03,\n",
      "          5.4484e-04,  5.3484e-03,  1.1712e-04, -2.8538e-02, -2.0436e-03,\n",
      "          6.0659e-03,  1.7772e-03,  1.1051e-03,  1.3826e-03,  5.6390e-05,\n",
      "          6.6987e-04,  1.3712e-03,  1.7615e-03,  1.2814e-04,  1.9203e-03,\n",
      "          5.6914e-03,  8.1437e-04,  3.4216e-04,  9.9065e-04,  3.1420e-04,\n",
      "          1.2860e-04,  7.2998e-04],\n",
      "        [ 5.8227e-03,  1.1546e-03, -6.8968e-04,  1.4216e-04, -4.5282e-04,\n",
      "          1.4121e-03,  3.6433e-05,  9.5308e-05,  4.7753e-04,  9.2767e-04,\n",
      "          1.4822e-03, -5.3713e-04, -1.0497e-03,  4.1005e-04, -2.5799e-03,\n",
      "          4.3874e-04, -3.9616e-04,  3.1437e-04, -3.6167e-03, -3.4794e-03,\n",
      "         -2.1323e-03,  1.4598e-03, -1.2187e-03,  6.9816e-04,  4.4807e-04,\n",
      "          8.5911e-04, -2.6464e-05],\n",
      "        [-4.6816e-04, -1.1155e-02,  1.7233e-04,  1.0584e-03,  1.6879e-03,\n",
      "         -2.7633e-03,  1.9991e-03,  1.6321e-04,  4.0813e-04, -7.1311e-03,\n",
      "          5.6743e-04,  2.8241e-03,  9.0551e-04,  9.5529e-04, -5.6171e-05,\n",
      "          4.3679e-04,  2.1625e-03,  7.1798e-04,  9.0464e-04,  1.5224e-03,\n",
      "          1.9092e-03,  8.3981e-04,  9.5249e-04,  1.0223e-03,  3.2400e-04,\n",
      "         -2.8318e-04,  3.2315e-04],\n",
      "        [ 3.9573e-04, -2.4103e-03,  1.6595e-04,  8.2040e-05,  2.9652e-04,\n",
      "          1.1094e-04,  1.0944e-04,  5.5028e-05,  4.5826e-05, -3.4930e-04,\n",
      "          8.1270e-07,  1.4780e-04,  4.9211e-05,  4.5789e-05, -2.4866e-05,\n",
      "          2.2713e-04,  5.2709e-05,  3.9662e-04,  1.4566e-04, -1.0204e-04,\n",
      "          7.9061e-05,  8.4409e-05,  5.3622e-05,  2.9082e-05,  6.8125e-05,\n",
      "          2.0227e-04,  4.2779e-05],\n",
      "        [-8.7423e-06, -3.5407e-05,  9.9535e-06,  2.4123e-06,  7.9153e-06,\n",
      "          1.1879e-05,  2.1356e-05,  2.5236e-05,  1.3501e-06, -4.2470e-05,\n",
      "          1.1228e-05,  3.9849e-06,  9.5575e-06, -8.3593e-05,  7.0850e-06,\n",
      "          1.0196e-06,  6.1396e-06,  2.8061e-06,  5.9088e-06, -7.1090e-05,\n",
      "          3.3364e-06,  6.5470e-05,  5.0326e-06,  7.0878e-06,  2.1989e-05,\n",
      "          7.2233e-06,  3.3312e-06],\n",
      "        [-2.5034e-03, -1.4174e-02,  5.5079e-04,  1.6747e-03,  8.0969e-04,\n",
      "          3.0016e-05,  3.4722e-04,  1.6368e-04,  3.6946e-04,  1.3935e-04,\n",
      "          3.9017e-04,  2.0853e-03,  4.4891e-04,  2.2612e-04,  1.3032e-04,\n",
      "         -3.7953e-04,  1.7769e-03,  4.5266e-04,  2.6690e-03,  3.5311e-04,\n",
      "         -5.7839e-06,  1.5563e-04,  1.8902e-03,  6.3493e-04,  1.2494e-03,\n",
      "          2.6969e-04,  2.4504e-04],\n",
      "        [-8.2988e-05, -7.7988e-05,  4.9417e-05,  8.6957e-06,  1.9588e-05,\n",
      "          6.2889e-05,  1.2269e-04,  1.2428e-04, -6.8518e-04, -7.8426e-05,\n",
      "          3.3926e-05,  1.2898e-05,  3.4804e-05,  9.9408e-05,  9.5090e-08,\n",
      "         -2.4721e-05,  4.8759e-05,  6.5421e-05,  7.0416e-06,  2.6420e-05,\n",
      "          7.1712e-05, -3.0780e-05,  2.4838e-05,  1.0487e-05,  3.0277e-05,\n",
      "          8.3910e-05,  4.2523e-05]])\n"
     ]
    }
   ],
   "source": [
    "# Now, let's calculate the gradients. This can be done by calling the backward() function on the loss tensor.\n",
    "loss.backward()\n",
    "# Now, let's check the gradients. The gradients should be non-zero since we have calculated them.\n",
    "print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1121, -0.4956,  0.1629, -0.8816,  0.0540,  0.6676, -0.0600, -0.4675,\n",
      "         -0.2153,  0.8831, -0.7581, -0.3683, -0.3424, -1.4013,  0.3207, -1.0220,\n",
      "          0.7985, -0.0927, -0.7045, -1.6013,  0.2891,  0.4893, -0.3848, -0.7122,\n",
      "         -0.1710, -1.4592,  0.2201],\n",
      "        [ 0.2489, -1.3241,  0.6962, -0.6633,  1.2144, -1.4949,  0.8797, -1.1785,\n",
      "         -0.9339, -0.5674, -0.2772, -2.1829,  0.3669,  0.9375,  0.0122, -0.3142,\n",
      "         -1.1567,  1.8375, -1.0154,  1.2185,  0.1605,  1.5959, -0.0466, -1.5271,\n",
      "         -2.0143, -1.5169,  0.3870],\n",
      "        [-1.1848,  0.6900,  1.3231,  1.8168,  0.6807,  0.7244,  0.0323, -1.6593,\n",
      "         -1.8771,  0.7374,  0.9257,  0.9246,  0.1825, -0.0737,  0.3146, -1.0369,\n",
      "          0.2100,  0.6144,  0.0628, -0.3297, -1.7970,  0.8728,  0.7669, -0.1138,\n",
      "         -0.9428,  0.7540,  0.1407],\n",
      "        [-0.6937, -0.6159, -0.7296,  0.4308,  0.2862, -0.2481,  0.2039,  0.8518,\n",
      "         -1.4098, -0.1071, -0.8018,  0.2771,  2.5598, -1.6952,  0.1885,  0.7388,\n",
      "          1.5903, -0.1947, -0.2415,  1.3204,  1.5996, -1.0792, -0.3396, -0.6780,\n",
      "         -0.1261, -1.6770,  1.2068],\n",
      "        [ 0.5722,  0.0657, -0.0235,  0.8875,  0.9569, -0.5507,  2.6612,  2.1476,\n",
      "         -0.8549, -0.7206,  1.3753,  0.9988,  0.1469, -0.9053, -0.3588,  1.6372,\n",
      "          0.6896, -0.5845,  0.9079,  0.4848, -0.2632, -0.5431, -1.6406,  0.9294,\n",
      "          1.2906,  0.2612, -0.5862],\n",
      "        [-1.5103, -2.0154,  0.6961, -0.6677, -0.8424,  0.5295, -0.5447,  0.8094,\n",
      "          1.1221, -0.6130,  0.2228,  0.4592, -1.7029, -1.2776, -0.7423,  0.9707,\n",
      "          0.3550, -1.5175, -0.3564,  0.9457, -2.1713, -0.8039, -0.4529, -0.2218,\n",
      "         -2.0901, -0.1770, -0.8981],\n",
      "        [-0.9834,  0.7564,  0.3474,  0.2856, -0.0495,  0.6225, -1.9217, -1.3176,\n",
      "          1.2286,  0.7025, -0.0170, -0.2830, -0.6446, -0.0199,  0.1123, -0.6601,\n",
      "         -0.6243, -0.7416,  0.3422, -0.2873, -0.6726,  0.8843,  0.7211,  0.2214,\n",
      "         -0.2737,  0.8612, -0.0610],\n",
      "        [ 2.1071, -0.9924,  1.4079, -0.7946,  0.6143, -0.2729,  1.6559,  1.3939,\n",
      "          0.6060,  0.2210, -0.8245,  0.7288, -0.7336,  1.5623, -1.0208, -0.0175,\n",
      "         -0.9194, -0.9389, -1.9421,  0.2329, -1.1014, -1.2472, -0.7485, -0.9792,\n",
      "          0.8285, -0.2501,  0.1602],\n",
      "        [ 0.7301, -0.4406,  0.8210, -0.6016,  0.9066,  1.5689, -0.1110, -0.2574,\n",
      "          0.5629,  0.0643, -0.0641,  3.0262,  0.1856, -0.1704,  1.0268,  0.0704,\n",
      "          0.3584, -2.3594, -1.3952,  0.8671, -0.2252,  0.1196, -1.0771,  0.2515,\n",
      "         -0.2197,  0.6061,  0.0472],\n",
      "        [ 0.0141,  0.3773,  0.7328, -0.4915, -1.1048, -0.0393,  0.0414, -1.0468,\n",
      "          0.0333, -1.0546, -0.1974,  0.8523, -0.4063, -1.4111,  0.0208, -0.4117,\n",
      "         -0.3399, -0.5565, -0.6503, -0.2172,  0.7704, -0.9990,  1.1102,  0.0309,\n",
      "          1.5685,  1.5383,  0.2981],\n",
      "        [ 0.5561, -0.6862, -1.1067,  2.2686, -2.2510, -1.0113,  0.3828,  1.2361,\n",
      "         -1.0535, -0.9402,  0.8782, -0.8629, -0.9917,  0.8485,  0.7375, -1.3158,\n",
      "         -0.4562, -1.7884, -1.4166, -0.9863,  0.6091,  0.6667, -1.1206, -1.6680,\n",
      "          1.0684, -0.1831,  0.6444],\n",
      "        [-0.5591, -0.4467, -0.6476,  0.2082, -0.7378, -0.3216, -0.9492,  1.0701,\n",
      "         -1.0469,  1.3768,  0.3987, -1.2152, -0.0317, -0.8688, -0.3511, -0.1999,\n",
      "         -1.4447,  0.8400, -0.8668, -0.9725, -1.2452, -0.0113, -1.0546, -0.8748,\n",
      "          2.2985, -1.4453, -0.7351],\n",
      "        [-1.6070, -1.1029, -1.8680,  0.8118,  1.1536, -0.5539, -0.8932, -0.3465,\n",
      "         -0.5365, -0.2289, -0.2484,  0.3544, -0.6691, -2.3203,  0.5021, -0.1245,\n",
      "          0.6526, -1.3452,  0.9635,  0.8349, -2.7675,  1.4801,  0.2942,  1.3922,\n",
      "          2.1125, -0.0856,  1.2578],\n",
      "        [-1.8493, -1.2241, -0.6665, -0.5090, -0.5193,  0.4297, -1.3419, -0.3650,\n",
      "          1.2695,  0.5694,  0.4714,  0.4615, -1.3822,  0.8789, -0.0990,  1.2043,\n",
      "          0.1997, -0.0446, -0.2705,  1.1500,  1.4564,  2.7633, -0.3187, -0.1317,\n",
      "         -0.6221, -1.4301, -1.4317],\n",
      "        [ 2.1167, -1.1838,  1.2036, -0.3536,  0.3417,  1.3205, -0.5346,  2.1998,\n",
      "         -1.2147, -0.8629, -1.0131, -0.3667,  1.3058, -2.1880, -2.0551,  0.7382,\n",
      "          0.9450, -1.3523,  0.6626,  0.5434,  0.5331, -0.7289, -1.3583, -0.1870,\n",
      "         -1.3208,  1.4481, -1.5155],\n",
      "        [-1.2364, -0.6758, -0.0800, -0.4060,  0.2416,  0.2559, -0.0605, -0.1539,\n",
      "          1.4019, -0.0312,  1.4724, -0.4815, -0.8533,  0.2310,  0.7536, -0.6285,\n",
      "         -0.7702,  1.6711,  0.7766,  0.4892, -0.5014,  1.6709, -0.8181,  0.7551,\n",
      "          1.3661, -0.4598,  0.6668],\n",
      "        [-0.7839,  2.1913,  0.3531,  0.8086,  2.7843, -0.5777, -1.5113,  1.4642,\n",
      "         -0.5434,  1.1363,  0.7060,  0.5937,  0.1270,  0.6933, -0.5839,  0.6028,\n",
      "          0.1503,  1.1832, -0.7278, -1.2537, -0.6925, -2.8561, -0.8612, -0.4061,\n",
      "          0.2722,  0.2986,  0.4729],\n",
      "        [-0.4718, -1.1712, -1.9520, -0.7573, -0.1884,  0.2220,  0.0738,  1.0746,\n",
      "          0.7868, -0.3379, -0.1411, -0.1263, -0.9179,  1.0803, -0.7998, -0.8772,\n",
      "         -1.0245, -0.8813,  1.9939,  0.1048,  1.1134, -0.0314,  0.4465,  1.4797,\n",
      "         -1.0431,  0.7966,  0.4633],\n",
      "        [ 0.5288,  0.0950,  1.2813, -0.6290,  0.7269, -1.4296, -0.2681,  0.8153,\n",
      "          0.2059, -0.7021,  0.5765, -1.1972,  0.2657, -0.7091,  0.1461, -1.7970,\n",
      "         -0.3576, -1.4564,  1.7164, -1.0182, -2.1017, -1.6977, -1.0373,  0.9226,\n",
      "         -0.6662, -0.6180,  1.3101],\n",
      "        [ 1.4731, -0.2844, -0.3727, -1.6564, -1.4151, -0.1896,  1.7852,  1.7993,\n",
      "          0.9814, -0.4625, -1.4979, -0.4025, -0.1689, -0.7295, -2.3339, -0.4907,\n",
      "         -1.4583,  0.3988,  0.3334, -0.4139,  0.0512, -1.3808,  0.8955,  0.5971,\n",
      "          0.1109,  0.7742, -0.6645],\n",
      "        [ 0.6152, -0.5202, -1.7862,  0.0205, -0.2504, -0.1452,  1.3385, -2.4515,\n",
      "         -1.4881, -0.7992,  1.4655,  0.2991, -0.1401,  0.0663, -2.0364, -0.4901,\n",
      "          0.0222,  0.2282, -0.1857,  0.4135,  1.5241, -0.0230, -1.1519, -0.3091,\n",
      "         -1.4956, -0.7526, -0.6526],\n",
      "        [ 2.4219,  0.5196, -0.3198, -0.6821,  0.2707,  0.6928, -2.3074,  0.4205,\n",
      "          0.2500,  0.2796,  1.2179,  0.1728,  0.2070,  1.4007, -1.0213, -0.5114,\n",
      "         -0.5123, -0.8315, -1.9164, -0.3374, -0.8730,  0.6990, -1.7232,  0.0584,\n",
      "         -0.2683,  0.3428, -2.3405],\n",
      "        [-0.8841, -0.3769, -1.6314,  0.1556,  0.6456, -0.8063,  0.7880, -1.6626,\n",
      "         -0.7065,  0.4485, -0.4301,  1.1489,  0.0162,  0.0611, -2.6981, -0.5082,\n",
      "          0.8741, -0.2359,  0.2434,  0.5429,  0.7497,  0.1237,  0.1316,  0.1210,\n",
      "         -1.0316, -0.7185, -1.0173],\n",
      "        [ 1.2740,  0.1506,  0.3640, -0.3623,  0.9709,  0.8835, -0.0742, -0.6366,\n",
      "         -0.6977,  0.6453, -2.9267,  0.2747, -0.6125, -0.2926, -1.0978,  0.6955,\n",
      "         -0.7706,  1.2134,  0.8027, -0.4903,  0.2571, -0.2913, -0.6594, -1.3994,\n",
      "         -0.5482,  0.6185, -1.0135],\n",
      "        [-0.6152, -0.4278,  0.2729, -0.5790,  0.0437,  1.0218,  1.0363,  1.2032,\n",
      "         -0.1005, -0.8075,  0.3934, -0.6425,  0.2323,  0.7119,  0.1631, -0.9765,\n",
      "         -0.2103, -0.9933, -0.2486, -0.6136, -0.8202,  2.1842, -0.4091, -0.0667,\n",
      "          1.0655,  0.5185, -0.8217],\n",
      "        [-1.6373, -0.5348, -0.1288,  0.9666,  0.2970,  0.2009, -0.6132, -1.2794,\n",
      "         -0.4037,  0.0097, -0.4734,  1.2122, -0.1739, -0.7859, -0.6041, -0.2325,\n",
      "          1.0326, -0.3481,  1.4591, -0.0560, -1.2836,  0.7126,  1.1079,  0.0046,\n",
      "          0.6672, -0.5581, -0.9543],\n",
      "        [ 0.2182,  2.0906,  0.4160, -1.3930, -0.5809,  1.4967,  1.2538,  1.2667,\n",
      "         -0.0936,  0.1063, -0.0317, -0.9988,  0.0941,  1.1970, -0.9961, -1.6909,\n",
      "          0.3310,  0.6250, -0.7702, -0.2146,  0.7420,  0.0571,  0.0732, -0.4882,\n",
      "         -0.1455,  0.9776,  0.3895]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Just a point to remember - Apparently, there is a difference when you use weights.data and weights in the below\n",
    "# equation. weights.data is the actual tensor and weights is a wrapper around the tensor. Apparently, if you use\n",
    "# weights, this changes something in the computation graph and gradient calculation is impacted -- I didn't really\n",
    "# understand the details. But, it is better to use weights.data to update the weights.\n",
    "#\n",
    "# Now, let's just update the weights using the gradients. We will use a learning rate of 0.1 (randomly chosen).\n",
    "weights.data += -0.1 * weights.grad\n",
    "# Let's check the updated weights.\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7959, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now, let's compute the loss again. It should be lesser than what we have computed above since we did\n",
    "# one loop of optimization using gradient descent.\n",
    "model_output_iter2 = encoded_inputs @ weights\n",
    "logits_iter2 = model_output_iter2.exp()\n",
    "probs_iter2 = logits_iter2 / logits_iter2.sum(dim=1, keepdim=True)\n",
    "target_probs_iter2 = probs_iter2[range(len(targets)), targets]\n",
    "loss_iter2 = -torch.log(target_probs_iter2).mean()\n",
    "# Note that the loss before was '3.9736' and the loss now is '3.9685'. Looking at the tiny change, I \n",
    "# think we can use slightly higher learning rate.\n",
    "print(loss_iter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUTTING TRAINING LOOP TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the above steps need to be repeated multiple times to train the model. This is called a training loop.\n",
    "# Let's put all the above steps in a function and run the training loop and observe how the loss changes over time.\n",
    "def training_loop(inputs: Tensor, targets: Tensor, weights: Tensor, num_loops: int, learning_rate: float) -> Tuple[Tensor, Tensor]:\n",
    "    for iteration in range(num_loops):\n",
    "        # Forward Propagation\n",
    "        model_output = inputs @ weights\n",
    "        logits = model_output.exp()\n",
    "        probs = logits / logits.sum(dim=1, keepdim=True)\n",
    "        target_probs = probs[torch.arange(start=0, end=len(targets)), targets]\n",
    "        loss = -torch.log(target_probs).mean()\n",
    "        if iteration % 40 == 0:\n",
    "            print(f\"Loss after iteration {iteration} is {loss.item()}\")\n",
    "        # Back Propagation\n",
    "        # Always, zero the weights from the previous loop. Other it will update the gradients instead of over-writing.\n",
    "        weights.grad = None\n",
    "        loss.backward()\n",
    "        weights.data += -learning_rate * weights.grad\n",
    "    return weights, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 is 3.7958600521087646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 40 is 2.4877727031707764\n",
      "Loss after iteration 80 is 2.353663206100464\n",
      "Loss after iteration 120 is 2.3008828163146973\n",
      "Loss after iteration 160 is 2.2744455337524414\n",
      "Loss after iteration 200 is 2.259544610977173\n",
      "Loss after iteration 240 is 2.2501444816589355\n",
      "Loss after iteration 280 is 2.243635892868042\n",
      "Loss after iteration 320 is 2.238840341567993\n",
      "Loss after iteration 360 is 2.2351670265197754\n",
      "Loss after iteration 400 is 2.232278347015381\n",
      "Loss after iteration 440 is 2.2299587726593018\n",
      "Loss after iteration 480 is 2.2280585765838623\n",
      "Loss after iteration 520 is 2.2264750003814697\n",
      "Loss after iteration 560 is 2.2251334190368652\n",
      "Loss after iteration 600 is 2.223982572555542\n",
      "Loss after iteration 640 is 2.2229840755462646\n",
      "Loss after iteration 680 is 2.222109794616699\n",
      "Loss after iteration 720 is 2.2213375568389893\n",
      "Loss after iteration 760 is 2.220651149749756\n",
      "Loss after iteration 800 is 2.2200369834899902\n",
      "Loss after iteration 840 is 2.219484806060791\n",
      "Loss after iteration 880 is 2.218985080718994\n",
      "Loss after iteration 920 is 2.2185311317443848\n",
      "Loss after iteration 960 is 2.2181167602539062\n",
      "Loss after iteration 1000 is 2.2177369594573975\n",
      "Loss after iteration 1040 is 2.2173876762390137\n",
      "Loss after iteration 1080 is 2.2170655727386475\n",
      "Loss after iteration 1120 is 2.2167670726776123\n",
      "Loss after iteration 1160 is 2.2164900302886963\n",
      "Loss after iteration 1200 is 2.2162322998046875\n",
      "Loss after iteration 1240 is 2.2159910202026367\n",
      "Loss after iteration 1280 is 2.215766191482544\n",
      "Loss after iteration 1320 is 2.215554714202881\n",
      "Loss after iteration 1360 is 2.2153563499450684\n",
      "Loss after iteration 1400 is 2.2151694297790527\n",
      "Loss after iteration 1440 is 2.214993476867676\n",
      "Loss after iteration 1480 is 2.214827060699463\n",
      "Loss after iteration 1520 is 2.2146694660186768\n",
      "Loss after iteration 1560 is 2.2145204544067383\n",
      "Loss after iteration 1600 is 2.214378595352173\n",
      "Loss after iteration 1640 is 2.2142443656921387\n",
      "Loss after iteration 1680 is 2.214115858078003\n",
      "Loss after iteration 1720 is 2.213994264602661\n",
      "Loss after iteration 1760 is 2.2138779163360596\n",
      "Loss after iteration 1800 is 2.2137668132781982\n",
      "Loss after iteration 1840 is 2.213660955429077\n",
      "Loss after iteration 1880 is 2.213559627532959\n",
      "Loss after iteration 1920 is 2.2134618759155273\n",
      "Loss after iteration 1960 is 2.213369131088257\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss after iteration 0 is 2.2132797241210938\n",
      "Loss after iteration 40 is 2.213270902633667\n",
      "Loss after iteration 80 is 2.2132623195648193\n",
      "Loss after iteration 120 is 2.2132537364959717\n",
      "Loss after iteration 160 is 2.213245153427124\n",
      "Loss after iteration 200 is 2.2132365703582764\n",
      "Loss after iteration 240 is 2.2132277488708496\n",
      "Loss after iteration 280 is 2.213219165802002\n",
      "Loss after iteration 320 is 2.2132108211517334\n",
      "Loss after iteration 360 is 2.2132022380828857\n",
      "Loss after iteration 400 is 2.213193893432617\n",
      "Loss after iteration 440 is 2.2131855487823486\n",
      "Loss after iteration 480 is 2.21317720413208\n",
      "Loss after iteration 520 is 2.2131688594818115\n",
      "Loss after iteration 560 is 2.213160514831543\n",
      "Loss after iteration 600 is 2.2131524085998535\n",
      "Loss after iteration 640 is 2.213144063949585\n",
      "Loss after iteration 680 is 2.2131359577178955\n",
      "Loss after iteration 720 is 2.213127851486206\n",
      "Loss after iteration 760 is 2.2131192684173584\n",
      "Loss after iteration 800 is 2.213111639022827\n",
      "Loss after iteration 840 is 2.2131035327911377\n",
      "Loss after iteration 880 is 2.213095188140869\n",
      "Loss after iteration 920 is 2.213087320327759\n",
      "Loss after iteration 960 is 2.2130794525146484\n",
      "Loss after iteration 1000 is 2.213071584701538\n",
      "Loss after iteration 1040 is 2.2130637168884277\n",
      "Loss after iteration 1080 is 2.2130556106567383\n",
      "Loss after iteration 1120 is 2.213047742843628\n",
      "Loss after iteration 1160 is 2.2130401134490967\n",
      "Loss after iteration 1200 is 2.2130322456359863\n",
      "Loss after iteration 1240 is 2.213024377822876\n",
      "Loss after iteration 1280 is 2.213016986846924\n",
      "Loss after iteration 1320 is 2.2130093574523926\n",
      "Loss after iteration 1360 is 2.2130017280578613\n",
      "Loss after iteration 1400 is 2.212993621826172\n",
      "Loss after iteration 1440 is 2.2129862308502197\n",
      "Loss after iteration 1480 is 2.2129783630371094\n",
      "Loss after iteration 1520 is 2.212970733642578\n",
      "Loss after iteration 1560 is 2.212963342666626\n",
      "Loss after iteration 1600 is 2.212955951690674\n",
      "Loss after iteration 1640 is 2.2129485607147217\n",
      "Loss after iteration 1680 is 2.2129409313201904\n",
      "Loss after iteration 1720 is 2.2129337787628174\n",
      "Loss after iteration 1760 is 2.212926149368286\n",
      "Loss after iteration 1800 is 2.212918758392334\n",
      "Loss after iteration 1840 is 2.212911605834961\n",
      "Loss after iteration 1880 is 2.212904453277588\n",
      "Loss after iteration 1920 is 2.2128970623016357\n",
      "Loss after iteration 1960 is 2.2128899097442627\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss after iteration 0 is 2.2128825187683105\n",
      "Loss after iteration 40 is 2.212878704071045\n",
      "Loss after iteration 80 is 2.2128751277923584\n",
      "Loss after iteration 120 is 2.212871551513672\n",
      "Loss after iteration 160 is 2.2128682136535645\n",
      "Loss after iteration 200 is 2.2128641605377197\n",
      "Loss after iteration 240 is 2.2128610610961914\n",
      "Loss after iteration 280 is 2.212857484817505\n",
      "Loss after iteration 320 is 2.2128539085388184\n",
      "Loss after iteration 360 is 2.212850570678711\n",
      "Loss after iteration 400 is 2.2128467559814453\n",
      "Loss after iteration 440 is 2.212843418121338\n",
      "Loss after iteration 480 is 2.2128396034240723\n",
      "Loss after iteration 520 is 2.2128360271453857\n",
      "Loss after iteration 560 is 2.212832450866699\n",
      "Loss after iteration 600 is 2.212829351425171\n",
      "Loss after iteration 640 is 2.2128257751464844\n",
      "Loss after iteration 680 is 2.2128217220306396\n",
      "Loss after iteration 720 is 2.2128186225891113\n",
      "Loss after iteration 760 is 2.212815046310425\n",
      "Loss after iteration 800 is 2.2128117084503174\n",
      "Loss after iteration 840 is 2.21280837059021\n",
      "Loss after iteration 880 is 2.2128047943115234\n",
      "Loss after iteration 920 is 2.212801218032837\n",
      "Loss after iteration 960 is 2.2127976417541504\n",
      "Loss after iteration 1000 is 2.212794065475464\n",
      "Loss after iteration 1040 is 2.2127912044525146\n",
      "Loss after iteration 1080 is 2.212787389755249\n",
      "Loss after iteration 1120 is 2.2127842903137207\n",
      "Loss after iteration 1160 is 2.212780475616455\n",
      "Loss after iteration 1200 is 2.2127773761749268\n",
      "Loss after iteration 1240 is 2.2127737998962402\n",
      "Loss after iteration 1280 is 2.2127702236175537\n",
      "Loss after iteration 1320 is 2.2127671241760254\n",
      "Loss after iteration 1360 is 2.212763786315918\n",
      "Loss after iteration 1400 is 2.2127602100372314\n",
      "Loss after iteration 1440 is 2.212756872177124\n",
      "Loss after iteration 1480 is 2.2127532958984375\n",
      "Loss after iteration 1520 is 2.212750196456909\n",
      "Loss after iteration 1560 is 2.2127468585968018\n",
      "Loss after iteration 1600 is 2.2127435207366943\n",
      "Loss after iteration 1640 is 2.2127397060394287\n",
      "Loss after iteration 1680 is 2.2127368450164795\n",
      "Loss after iteration 1720 is 2.212733268737793\n",
      "Loss after iteration 1760 is 2.2127301692962646\n",
      "Loss after iteration 1800 is 2.212726593017578\n",
      "Loss after iteration 1840 is 2.2127232551574707\n",
      "Loss after iteration 1880 is 2.2127199172973633\n",
      "Loss after iteration 1920 is 2.212716817855835\n",
      "Loss after iteration 1960 is 2.2127137184143066\n"
     ]
    }
   ],
   "source": [
    "# Note that the loss we obtained here is very close to the loss we obtained in the rule-based model. This is because\n",
    "# the neural network we built is equivalent to the rule-based model as explained above. Ofcourse, this model seems\n",
    "# slightly better than the rule-based model.\n",
    "# Rule-based model loss: 2.207\n",
    "# Neural network model loss: 2.212\n",
    "# First, lets run the training loop for 300 iteration with a learning rate of 10.0.\n",
    "training_loop(inputs=encoded_inputs, targets=targets, weights=weights, num_loops=2000, learning_rate=10.0)\n",
    "print(\"-\" * 100)\n",
    "# Now, let's run the training loop for 300 iteration with a learning rate of 1.0.\n",
    "training_loop(inputs=encoded_inputs, targets=targets, weights=weights, num_loops=2000, learning_rate=1.0)\n",
    "print(\"-\" * 100)\n",
    "# Let's run the training loop for 300 iteration with a learning rate of 0.01.\n",
    "_, final_loss = training_loop(inputs=encoded_inputs, targets=targets, weights=weights, num_loops=2000, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 2.21271014213562\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final training loss: {final_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have seen how to train a simple neural network to generate names. Now, let's see how to generate names using this\n",
    "# trained model. The idea is to take a character as input, pass it through the neural network, get the probabilities\n",
    "def generate_names(weights: Tensor, char_to_int: dict, int_to_char: dict, num_names: int):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    for _ in range(num_names):\n",
    "        name = BOUND_CHARACTER\n",
    "        while True:\n",
    "            prev_char_idx = char_to_int[name[-1]]\n",
    "            input_tensor = torch.zeros(size=(1, 27), dtype=torch.float32)\n",
    "            input_tensor[0][prev_char_idx] = 1.0\n",
    "            output_tensor = input_tensor @ weights\n",
    "            logits = output_tensor.exp()\n",
    "            output_probs = logits / logits.sum(dim=1, keepdim=True)\n",
    "            output_char = int_to_char[torch.multinomial(output_probs, num_samples=1).item()]\n",
    "            if output_char == BOUND_CHARACTER:\n",
    "                break\n",
    "            name += output_char\n",
    "        print(name[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n",
      "velivadanilish\n",
      "svarar\n",
      "yaakumyin\n",
      "thethi\n",
      "kove\n",
      "gasubhumath\n",
      "thegarumady\n",
      "inthinurath\n",
      "arakujogohidumukokanusharina\n",
      "ven\n",
      "vanuksith\n",
      "ntheveg\n",
      "satiyavarakajahagenoraja\n",
      "asanthran\n",
      "ati\n",
      "ujinuganishina\n",
      "h\n",
      "hthavathaila\n",
      "shragoopradranthevahaa\n"
     ]
    }
   ],
   "source": [
    "# Note the names are also very similar to the names generated by the rule-based model.\n",
    "generate_names(weights=weights, char_to_int=char_to_int, int_to_char=int_to_char, num_names=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".makemore_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
