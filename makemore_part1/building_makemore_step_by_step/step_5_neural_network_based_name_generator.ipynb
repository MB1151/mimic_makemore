{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to train a simple neural network to generate names?\n",
    "# 2) How to encode characters into one-hot encoding?\n",
    "# 3) What is the similarity between rule-based name generator and neural network-based name generator?\n",
    "#\n",
    "# Resources:\n",
    "# 1) ADD RESOURCES TO UNDERSTAND NEURAL NETWORKS\n",
    "# 2) https://github.com/MB1151/mimic_micro_autograd\n",
    "#       -- To understand how backpropagation works in detail.\n",
    "#       -- This is my other repository where I have implemented autograd from scratch.\n",
    "#       -- Not everything is needed from this repository, but it is a good resource to understand how backpropagation works.\n",
    "# 3) makemore_part1/building_makemore_step_by_step/step_3_model_quality.ipynb\n",
    "#       -- To understand how to calculate loss and use it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../Data/names.txt\"\n",
    "BOUND_CHARACTER = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['albonsha',\n",
       " 'beenapreethi',\n",
       " 'thushniha',\n",
       " 'aakaksha',\n",
       " 'dumeethran',\n",
       " 'luhit',\n",
       " 'valam',\n",
       " 'harinyai',\n",
       " 'sakthikaa',\n",
       " 'kaveetha']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATA_PATH, \"r\") as f:\n",
    "    names = [name.strip() for name in f.readlines()]\n",
    "\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Create character to integer mapping.\n",
    "char_to_int = {char: idx + 1 for idx, char in enumerate(string.ascii_lowercase)}\n",
    "char_to_int[BOUND_CHARACTER] = 0\n",
    "print(char_to_int)\n",
    "print(\"-\" * 100)\n",
    "int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
    "print(int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our name-generator neural network, we will use one-hot encoding to represent characters.\n",
    "# One-hot encoding is a way to represent characters in a number format so that the neural network can understand them.\n",
    "#\n",
    "# In our text, we have 27 characters (26 alphabets + 1 dot). As you can see, we have assigned a unique number to each \n",
    "# character in the char_to_int dictionary. However, this is not the best way to represent characters in a neural \n",
    "# network. This is because the neural network may think that the characters with higher numbers are more important\n",
    "# than the characters with lower numbers. This is not true. The numbers assigned to the characters are just for\n",
    "# representation purposes. They don't have any meaning. \n",
    "# For example, the character 'a' is represented as 1 and the character 'z' is represented as 26. But, this doesn't \n",
    "# mean that 'z' is more important than 'a'. So, we need to represent characters in a way that the neural network can \n",
    "# understand that all characters are equally important. This is where one-hot encoding comes into play.\n",
    "#\n",
    "# In one-hot encoding, we will represent each character as a 27-dimensional vector where all elements are zero except \n",
    "# the element corresponding to the character index which is 1.\n",
    "# For example, the character 'a' will be represented as [0, 1, 0, 0, ..., 0] where the second element is 1 and all\n",
    "# other elements are zero. Similarly, the character 'z' will be represented as [0, 0, 0, ..., 1] where the last \n",
    "# element is 1 and all other elements are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a very simple neural network for the name generation. We will improve this model later on. For now, \n",
    "# the idea is to build a neural network which is very similar to the rule-based model that we built in \n",
    "# 'building_makemore_step_by_step/step_2_rule_based_name_generator.ipynb'\n",
    "#\n",
    "# The neural network will take a single character as input and predict a single character as output. As explained \n",
    "# above, every character is represented as a 27-dimensional vector. So, our network should take an input that has\n",
    "# 27 features. \n",
    "# We want the neural network to output a 27-dimensional vector, where each value in the vector corresponds to the\n",
    "# probability of the prediction to be the character corresponding to that particular position. \n",
    "# For now, we will only have 1 layer in the neural network.\n",
    "#\n",
    "# So, the architecture is as follows:\n",
    "# Number of layers = 1\n",
    "# Input = [27, 1] -- Ignoring batching in this calculation.\n",
    "# Number of neurons = Number of outputs = 27\n",
    "#\n",
    "# We will also not have bias in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_list: 545276\n",
      "input_list[:10]: [0, 1, 12, 2, 15, 14, 19, 8, 1, 0]\n",
      "target_list[:10]: [1, 12, 2, 15, 14, 19, 8, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Let's create the input and output as required by the neural network. We want to use backward to run the gradient\n",
    "# descent algorithm and train the model. Hence, we need to perform all the inputs, targets in the form of tensors\n",
    "# and the computations between tensors.\n",
    "input_list = []\n",
    "target_list = []\n",
    "for name in names:\n",
    "    name = BOUND_CHARACTER + name + BOUND_CHARACTER\n",
    "    for first_char, second_char in zip(name, name[1:]):\n",
    "        first_char_idx = char_to_int[first_char]\n",
    "        second_char_idx = char_to_int[second_char]\n",
    "        input_list.append(first_char_idx)\n",
    "        target_list.append(second_char_idx)\n",
    "\n",
    "print(f\"shape of input_list: {len(input_list)}\")\n",
    "print(f\"input_list[:10]: {input_list[:10]}\")\n",
    "print(f\"target_list[:10]: {target_list[:10]}\")\n",
    "\n",
    "# Let's create tensors out of these lists to use with neural networks.\n",
    "inputs = torch.tensor(data=input_list, dtype=torch.int64)\n",
    "targets = torch.tensor(data=target_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded_inputs: torch.Size([545276, 27])\n",
      "encoded_inputs[:3]: tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Now, let's generate the one-hot encoding for the inputs.\n",
    "# We cast the one-hot vectors into float since we need to pass them through the neural network which\n",
    "# expects float inputs.\n",
    "encoded_inputs = F.one_hot(inputs, num_classes=len(char_to_int)).float()\n",
    "print(f\"shape of encoded_inputs: {encoded_inputs.shape}\")\n",
    "print(f\"encoded_inputs[:3]: {encoded_inputs[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a seed to keep our outputs consistent across multiple runs of the notebook.\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27])\n",
      "tensor([[-0.1117, -0.4966,  0.1631, -0.8817,  0.0539,  0.6684, -0.0597, -0.4675,\n",
      "         -0.2153,  0.8840, -0.7584, -0.3689, -0.3424, -1.4020,  0.3206, -1.0219,\n",
      "          0.7988, -0.0923, -0.7049, -1.6024,  0.2891,  0.4899, -0.3853, -0.7120,\n",
      "         -0.1706, -1.4594,  0.2207],\n",
      "        [ 0.2463, -1.3248,  0.6970, -0.6631,  1.2158, -1.4949,  0.8810, -1.1786,\n",
      "         -0.9340, -0.5675, -0.2772, -2.1834,  0.3668,  0.9380,  0.0078, -0.3139,\n",
      "         -1.1567,  1.8409, -1.0174,  1.2192,  0.1601,  1.5985, -0.0469, -1.5270,\n",
      "         -2.0143, -1.5173,  0.3877],\n",
      "        [-1.1849,  0.6897,  1.3232,  1.8169,  0.6808,  0.7244,  0.0323, -1.6593,\n",
      "         -1.8773,  0.7372,  0.9257,  0.9247,  0.1825, -0.0737,  0.3147, -1.0369,\n",
      "          0.2100,  0.6144,  0.0628, -0.3297, -1.7970,  0.8728,  0.7670, -0.1138,\n",
      "         -0.9428,  0.7540,  0.1407],\n",
      "        [-0.6937, -0.6159, -0.7295,  0.4308,  0.2862, -0.2481,  0.2040,  0.8519,\n",
      "         -1.4102, -0.1071, -0.8018,  0.2771,  2.5599, -1.6952,  0.1885,  0.7388,\n",
      "          1.5903, -0.1947, -0.2415,  1.3204,  1.5997, -1.0792, -0.3396, -0.6780,\n",
      "         -0.1261, -1.6770,  1.2068],\n",
      "        [ 0.5722,  0.0653, -0.0235,  0.8876,  0.9570, -0.5510,  2.6617,  2.1479,\n",
      "         -0.8555, -0.7208,  1.3755,  0.9988,  0.1469, -0.9053, -0.3587,  1.6374,\n",
      "          0.6897, -0.5844,  0.9078,  0.4848, -0.2632, -0.5432, -1.6406,  0.9295,\n",
      "          1.2907,  0.2612, -0.5862],\n",
      "        [-1.5105, -2.0155,  0.6964, -0.6676, -0.8424,  0.5289, -0.5447,  0.8097,\n",
      "          1.1226, -0.6129,  0.2229,  0.4593, -1.7031, -1.2777, -0.7428,  0.9711,\n",
      "          0.3551, -1.5174, -0.3566,  0.9455, -2.1716, -0.8039, -0.4530, -0.2217,\n",
      "         -2.0901, -0.1771, -0.8981],\n",
      "        [-0.9834,  0.7564,  0.3474,  0.2856, -0.0495,  0.6225, -1.9217, -1.3176,\n",
      "          1.2286,  0.7025, -0.0170, -0.2830, -0.6446, -0.0199,  0.1123, -0.6601,\n",
      "         -0.6243, -0.7416,  0.3422, -0.2873, -0.6726,  0.8843,  0.7211,  0.2214,\n",
      "         -0.2737,  0.8612, -0.0609],\n",
      "        [ 2.1073, -0.9930,  1.4080, -0.7945,  0.6144, -0.2730,  1.6561,  1.3940,\n",
      "          0.6060,  0.2209, -0.8245,  0.7289, -0.7336,  1.5624, -1.0208, -0.0175,\n",
      "         -0.9194, -0.9389, -1.9421,  0.2329, -1.1014, -1.2473, -0.7485, -0.9792,\n",
      "          0.8285, -0.2501,  0.1602],\n",
      "        [ 0.7295, -0.4441,  0.8214, -0.6015,  0.9069,  1.5691, -0.1108, -0.2573,\n",
      "          0.5631,  0.0629, -0.0639,  3.0292,  0.1858, -0.1704,  1.0270,  0.0704,\n",
      "          0.3586, -2.3594, -1.3954,  0.8675, -0.2252,  0.1193, -1.0772,  0.2516,\n",
      "         -0.2196,  0.6062,  0.0473],\n",
      "        [ 0.0124,  0.3775,  0.7332, -0.4915, -1.1049, -0.0391,  0.0416, -1.0468,\n",
      "          0.0335, -1.0545, -0.1973,  0.8522, -0.4066, -1.4113,  0.0200, -0.4115,\n",
      "         -0.3398, -0.5563, -0.6508, -0.2179,  0.7700, -0.9989,  1.1106,  0.0311,\n",
      "          1.5696,  1.5389,  0.2984],\n",
      "        [ 0.5561, -0.6868, -1.1067,  2.2689, -2.2510, -1.0115,  0.3828,  1.2363,\n",
      "         -1.0535, -0.9404,  0.8782, -0.8629, -0.9917,  0.8485,  0.7376, -1.3158,\n",
      "         -0.4562, -1.7884, -1.4166, -0.9863,  0.6091,  0.6668, -1.1206, -1.6680,\n",
      "          1.0685, -0.1831,  0.6445],\n",
      "        [-0.5592, -0.4480, -0.6476,  0.2083, -0.7378, -0.3218, -0.9491,  1.0704,\n",
      "         -1.0470,  1.3770,  0.3989, -1.2153, -0.0316, -0.8687, -0.3510, -0.2000,\n",
      "         -1.4447,  0.8402, -0.8668, -0.9728, -1.2452, -0.0115, -1.0545, -0.8748,\n",
      "          2.2994, -1.4453, -0.7350],\n",
      "        [-1.6073, -1.1039, -1.8680,  0.8119,  1.1538, -0.5540, -0.8932, -0.3465,\n",
      "         -0.5365, -0.2293, -0.2484,  0.3544, -0.6692, -2.3203,  0.5021, -0.1246,\n",
      "          0.6526, -1.3451,  0.9636,  0.8350, -2.7675,  1.4802,  0.2942,  1.3924,\n",
      "          2.1130, -0.0856,  1.2580],\n",
      "        [-1.8494, -1.2253, -0.6665, -0.5090, -0.5193,  0.4296, -1.3418, -0.3649,\n",
      "          1.2697,  0.5690,  0.4715,  0.4616, -1.3822,  0.8789, -0.0990,  1.2043,\n",
      "          0.1998, -0.0446, -0.2705,  1.1501,  1.4566,  2.7640, -0.3187, -0.1316,\n",
      "         -0.6221, -1.4301, -1.4317],\n",
      "        [ 2.1155, -1.1853,  1.2042, -0.3535,  0.3415,  1.3207, -0.5345,  2.2011,\n",
      "         -1.2147, -0.8642, -1.0132, -0.3668,  1.3064, -2.1880, -2.0552,  0.7384,\n",
      "          0.9454, -1.3522,  0.6629,  0.5436,  0.5328, -0.7291, -1.3583, -0.1869,\n",
      "         -1.3208,  1.4487, -1.5155],\n",
      "        [-1.2364, -0.6758, -0.0800, -0.4060,  0.2415,  0.2559, -0.0605, -0.1539,\n",
      "          1.4019, -0.0312,  1.4725, -0.4815, -0.8533,  0.2310,  0.7535, -0.6287,\n",
      "         -0.7702,  1.6713,  0.7765,  0.4891, -0.5015,  1.6710, -0.8182,  0.7551,\n",
      "          1.3663, -0.4598,  0.6668],\n",
      "        [-0.7839,  2.1911,  0.3531,  0.8087,  2.7847, -0.5777, -1.5113,  1.4643,\n",
      "         -0.5434,  1.1363,  0.7060,  0.5937,  0.1270,  0.6934, -0.5838,  0.6028,\n",
      "          0.1503,  1.1833, -0.7282, -1.2537, -0.6925, -2.8562, -0.8612, -0.4060,\n",
      "          0.2722,  0.2987,  0.4729],\n",
      "        [-0.4718, -1.1712, -1.9521, -0.7573, -0.1884,  0.2220,  0.0738,  1.0746,\n",
      "          0.7868, -0.3379, -0.1411, -0.1263, -0.9179,  1.0803, -0.7998, -0.8772,\n",
      "         -1.0245, -0.8813,  1.9939,  0.1048,  1.1134, -0.0314,  0.4465,  1.4797,\n",
      "         -1.0431,  0.7966,  0.4633],\n",
      "        [ 0.5287,  0.0931,  1.2819, -0.6290,  0.7272, -1.4299, -0.2680,  0.8157,\n",
      "          0.2061, -0.7030,  0.5768, -1.1972,  0.2659, -0.7091,  0.1462, -1.7971,\n",
      "         -0.3575, -1.4564,  1.7173, -1.0184, -2.1019, -1.6981, -1.0374,  0.9230,\n",
      "         -0.6661, -0.6180,  1.3107],\n",
      "        [ 1.4736, -0.2851, -0.3726, -1.6563, -1.4151, -0.1897,  1.7860,  1.8001,\n",
      "          0.9789, -0.4627, -1.4979, -0.4024, -0.1688, -0.7295, -2.3339, -0.4907,\n",
      "         -1.4583,  0.3990,  0.3334, -0.4139,  0.0513, -1.3811,  0.8958,  0.5972,\n",
      "          0.1110,  0.7745, -0.6644],\n",
      "        [ 0.6153, -0.5206, -1.7862,  0.0206, -0.2503, -0.1452,  1.3390, -2.4515,\n",
      "         -1.4909, -0.7994,  1.4661,  0.2993, -0.1399,  0.0665, -2.0364, -0.4900,\n",
      "          0.0224,  0.2284, -0.1856,  0.4137,  1.5247, -0.0229, -1.1519, -0.3090,\n",
      "         -1.4955, -0.7526, -0.6525],\n",
      "        [ 2.4224,  0.5197, -0.3198, -0.6821,  0.2706,  0.6929, -2.3074,  0.4205,\n",
      "          0.2500,  0.2796,  1.2181,  0.1727,  0.2069,  1.4007, -1.0216, -0.5113,\n",
      "         -0.5124, -0.8315, -1.9167, -0.3378, -0.8732,  0.6991, -1.7233,  0.0584,\n",
      "         -0.2683,  0.3429, -2.3405],\n",
      "        [-0.8842, -0.3780, -1.6314,  0.1557,  0.6457, -0.8066,  0.7882, -1.6625,\n",
      "         -0.7064,  0.4478, -0.4300,  1.1491,  0.0163,  0.0612, -2.6981, -0.5081,\n",
      "          0.8743, -0.2358,  0.2435,  0.5431,  0.7498,  0.1238,  0.1317,  0.1211,\n",
      "         -1.0315, -0.7186, -1.0173],\n",
      "        [ 1.2741,  0.1503,  0.3640, -0.3623,  0.9709,  0.8835, -0.0742, -0.6366,\n",
      "         -0.6977,  0.6453, -2.9267,  0.2747, -0.6125, -0.2925, -1.0978,  0.6955,\n",
      "         -0.7705,  1.2134,  0.8028, -0.4904,  0.2571, -0.2913, -0.6594, -1.3994,\n",
      "         -0.5482,  0.6185, -1.0135],\n",
      "        [-0.6152, -0.4278,  0.2729, -0.5790,  0.0437,  1.0218,  1.0363,  1.2032,\n",
      "         -0.1005, -0.8075,  0.3934, -0.6425,  0.2323,  0.7118,  0.1631, -0.9765,\n",
      "         -0.2103, -0.9933, -0.2486, -0.6136, -0.8202,  2.1842, -0.4091, -0.0667,\n",
      "          1.0655,  0.5185, -0.8217],\n",
      "        [-1.6376, -0.5362, -0.1288,  0.9668,  0.2971,  0.2009, -0.6132, -1.2794,\n",
      "         -0.4037,  0.0097, -0.4733,  1.2124, -0.1739, -0.7859, -0.6041, -0.2325,\n",
      "          1.0328, -0.3480,  1.4594, -0.0560, -1.2836,  0.7126,  1.1081,  0.0047,\n",
      "          0.6673, -0.5581, -0.9543],\n",
      "        [ 0.2181,  2.0906,  0.4160, -1.3930, -0.5809,  1.4967,  1.2538,  1.2667,\n",
      "         -0.0936,  0.1063, -0.0317, -0.9988,  0.0941,  1.1970, -0.9961, -1.6909,\n",
      "          0.3310,  0.6250, -0.7701, -0.2146,  0.7421,  0.0571,  0.0732, -0.4882,\n",
      "         -0.1455,  0.9776,  0.3895]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We need to set the generator everytime this code is run.\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# We have 27 inputs and 27 outputs. So, the weights should be of shape (27, 27).\n",
    "# We will initialize the weights randomly.\n",
    "# The 'requires_grad=True' argument is used to tell PyTorch that we want to compute the gradients of the weights\n",
    "# during the backward pass -- This will be explained below in detail.\n",
    "weights = torch.randn(size=(27, 27), dtype=torch.float32, requires_grad=True)\n",
    "print(weights.shape)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Let's do a sample run of the neural network to understand how it works.\n",
    "# Assume the input is 'b'. The one-hot encoding of 'b' is [0, 1, 0, 0, ..., 0].\n",
    "sample_input = torch.zeros(size=(1,27), dtype=torch.float32)\n",
    "sample_input[0][2] = 1\n",
    "print(sample_input.shape)\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27])\n",
      "tensor([[-1.1849,  0.6897,  1.3232,  1.8169,  0.6808,  0.7244,  0.0323, -1.6593,\n",
      "         -1.8773,  0.7372,  0.9257,  0.9247,  0.1825, -0.0737,  0.3147, -1.0369,\n",
      "          0.2100,  0.6144,  0.0628, -0.3297, -1.7970,  0.8728,  0.7670, -0.1138,\n",
      "         -0.9428,  0.7540,  0.1407]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Just to understand, this is multiplication of a row vector with a matrix. So, the output will be a row vector.\n",
    "# The output row vector will be the 2nd row of the weights matrix since the input has 1 only in the 2nd position.\n",
    "# \n",
    "# This is equivalent to the rule-based model where we use the pre-computed probabilities of the current character\n",
    "# to predict the next character. In the rule based model, if the current character is 'b', we simply take the row\n",
    "# 2nd row from the 'char_probs' tensor and use it to predict the next character. If you observe keenly, we are \n",
    "# doing the same thing here. Here, the current character is 'b' and we simply take the 2nd row of the weights. Our\n",
    "# model setup and one-hot encoding is just making this neural network equivalent to the rule-based model.\n",
    "sample_output = sample_input @ weights\n",
    "print(sample_output.shape)\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOFTMAX\n",
    "\n",
    "Softmax function is used to convert a list of values into probabilities.\n",
    "\n",
    "Let's say we have a list of values $[x_{0}, x_{1}, x_{2}, ..., x_{n - 1}]$. Now, we need to convert this <br>\n",
    "into a list of probabilities. \n",
    "\n",
    "$$input\\_list = [x_{0}, x_{1}, x_{2}, ..., x_{n - 1}]$$\n",
    "$$\\implies logits = [e^{x_{0}}, e^{x_{1}}, e^{x_{2}}, ..., e^{x_{n - 1}}]$$\n",
    "\n",
    "Applying exponentiation brings the value range to $(0, \\infty)$. Now, we normalize the $logits$ to obtain probabilities.\n",
    "\n",
    "First, let's calculate the sum of logits to normalize.\n",
    "\n",
    "$$logit\\_sum = \\sum_{i=0}^{n-1} e^{x_{i}}$$\n",
    "\n",
    "$$\\implies probabilities = [\\frac{e^{x_{0}}}{logit\\_sum}, \\frac{e^{x_{1}}}{logit\\_sum}, \\frac{e^{x_{2}}}{logit\\_sum}, ..., \\frac{e^{x_{n-1}}}{logit\\_sum}]$$\n",
    "\n",
    "This whole process to obtain probabilities from a list of values is called softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27])\n",
      "tensor([[0.3058, 1.9931, 3.7554, 6.1530, 1.9754, 2.0634, 1.0328, 0.1903, 0.1530,\n",
      "         2.0902, 2.5237, 2.5211, 1.2003, 0.9290, 1.3698, 0.3546, 1.2337, 1.8486,\n",
      "         1.0648, 0.7192, 0.1658, 2.3936, 2.1532, 0.8925, 0.3895, 2.1255, 1.1511]],\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The 'sample_output' is a row vector containing some values. We need to convert these values into probabilities. \n",
    "# As we know, the probabilities are always between 0 and 1. So, we need to limit the values of the output row \n",
    "# vector between 0 and 1. We can achieve this by using the softmax function.\n",
    "sample_run_logits = sample_output.exp()\n",
    "print(sample_run_logits.shape)\n",
    "print(sample_run_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27])\n",
      "tensor([[0.0072, 0.0466, 0.0878, 0.1439, 0.0462, 0.0483, 0.0242, 0.0045, 0.0036,\n",
      "         0.0489, 0.0590, 0.0590, 0.0281, 0.0217, 0.0320, 0.0083, 0.0289, 0.0432,\n",
      "         0.0249, 0.0168, 0.0039, 0.0560, 0.0504, 0.0209, 0.0091, 0.0497, 0.0269]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the logits to get probabilities.\n",
    "sample_run_probs = sample_run_logits / sample_run_logits.sum(dim=1, keepdim=True)\n",
    "print(sample_run_probs.shape)\n",
    "print(sample_run_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's verify that the sum of the probabilities is 1.\n",
    "print(sample_run_probs.sum(dim=1, keepdim=True).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14393503963947296\n"
     ]
    }
   ],
   "source": [
    "# Let's say the target is 'c' which is represented as integer 3. So, we extract the probability of the next \n",
    "# character being 'c' from the 'sample_run_probs'.\n",
    "sample_run_target_prob = sample_run_probs[0][3]\n",
    "print(sample_run_target_prob.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.938393235206604\n"
     ]
    }
   ],
   "source": [
    "# Now, let's calculate the loss using negative log-likelihood as we explained in the previous notebook ('step_3_model_quality.ipynb').\n",
    "sample_run_loss = -torch.log(sample_run_target_prob)\n",
    "print(sample_run_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last step is to update the weights using the gradients. Let's do this in the actual training loop below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CREATION CONTINUED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell just combines all the code explained in SAMPLE RUN SECTION above.\n",
    "model_output = encoded_inputs @ weights\n",
    "logits = model_output.exp()\n",
    "probs = logits / logits.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's verify that the sum of the probabilities is 1 for a few of the rows.\n",
    "print(probs.sum(dim=1, keepdim=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0357, 0.0243, 0.0470, 0.0165, 0.0421, 0.0779, 0.0376, 0.0250, 0.0322,\n",
      "         0.0967, 0.0187, 0.0276, 0.0284, 0.0098, 0.0550, 0.0144, 0.0888, 0.0364,\n",
      "         0.0197, 0.0080, 0.0533, 0.0652, 0.0272, 0.0196, 0.0337, 0.0093, 0.0498],\n",
      "        [0.0342, 0.0071, 0.0536, 0.0138, 0.0901, 0.0060, 0.0645, 0.0082, 0.0105,\n",
      "         0.0151, 0.0202, 0.0030, 0.0386, 0.0683, 0.0269, 0.0195, 0.0084, 0.1684,\n",
      "         0.0097, 0.0904, 0.0314, 0.1321, 0.0255, 0.0058, 0.0036, 0.0059, 0.0394],\n",
      "        [0.0045, 0.0075, 0.0035, 0.0510, 0.0718, 0.0130, 0.0093, 0.0160, 0.0132,\n",
      "         0.0180, 0.0177, 0.0323, 0.0116, 0.0022, 0.0374, 0.0200, 0.0435, 0.0059,\n",
      "         0.0593, 0.0522, 0.0014, 0.0995, 0.0304, 0.0911, 0.1873, 0.0208, 0.0797],\n",
      "        [0.0072, 0.0466, 0.0878, 0.1439, 0.0462, 0.0483, 0.0242, 0.0045, 0.0036,\n",
      "         0.0489, 0.0590, 0.0590, 0.0281, 0.0217, 0.0320, 0.0083, 0.0289, 0.0432,\n",
      "         0.0249, 0.0168, 0.0039, 0.0560, 0.0504, 0.0209, 0.0091, 0.0497, 0.0269],\n",
      "        [0.0064, 0.0111, 0.0202, 0.0146, 0.0279, 0.0283, 0.0206, 0.0188, 0.0889,\n",
      "         0.0212, 0.0954, 0.0135, 0.0093, 0.0276, 0.0465, 0.0117, 0.0101, 0.1164,\n",
      "         0.0476, 0.0357, 0.0133, 0.1164, 0.0097, 0.0466, 0.0858, 0.0138, 0.0426]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([ 1, 12,  2, 15, 14])\n"
     ]
    }
   ],
   "source": [
    "# Each row of the 'probs' tensor contains the probabilities for the next character for the corresponding input.\n",
    "# probs[0][0] = The probability of the next character being '.' for the zeroth input.\n",
    "# probs[0][1] = The probability of the next character being 'a' for the zeroth input.\n",
    "# probs[0][2] = The probability of the next character being 'b' for the zeroth input.\n",
    "# probs[2][3] = The probability of the next character being 'c' for the second input.\n",
    "# ...\n",
    "# In general, \n",
    "# probs[i][j] = The probability of the next character being 'int_to_char[j]' for the ith input.\n",
    "print(probs[:5])\n",
    "# Targets tensor contains the indices of the target characters or the true next characters for each input.\n",
    "print(targets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0243, 0.0386, 0.0035, 0.0083, 0.0465], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's extract the probabilities assigned by the model to the target characters for each input.\n",
    "target_probs = probs[range(len(targets)), targets]\n",
    "# Let's understand what the printed values mean.\n",
    "# In the above cell, the targets for the first 5 inputs are [1, 12, 2, 15, 14].\n",
    "#\n",
    "# Let's consider the first target which is 1. This means that the target character (or the true output) is 'a'.\n",
    "# For the zeroth input, the model assigned a probability of probs[0][1] to the character 'a'. So, the target_probs[0]\n",
    "# should be equal to probs[0][1] = 0.0243 which is correct.\n",
    "# \n",
    "# Now, let's consider the second target which is 12. This means that the target character (or the true output) is 'k'.\n",
    "# For the second input, the model assigned a probability of probs[1][12] to the character 'k'. So, the target_probs[1]\n",
    "# should be equal to probs[1][12] = 0.0386 which is correct.\n",
    "#\n",
    "# Similarly, we can verify the rest of the values.\n",
    "print(target_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.79728102684021\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate the loss for the entire dataset using negative log-likelihood as explained in the previous \n",
    "# notebook (building_makemore_step_by_step/step_3_model_quality.ipynb).\n",
    "# We calculate the mean of the negative log-likelihoods for all the inputs.\n",
    "loss = -torch.log(target_probs).mean()\n",
    "# The loss is going to be a single positive number. Lower the loss, better the model.\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# The last step is to update the weights using the gradients. First, let's check what the gradients are.\n",
    "# It should be None since we haven't calculated the gradients yet. Let's verify.\n",
    "print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.9656e-03, -1.0170e-02,  1.3612e-03, -4.4705e-04, -1.3427e-03,\n",
      "          7.2652e-03,  3.8438e-03, -4.1995e-04,  3.5887e-04,  8.9341e-03,\n",
      "         -2.9020e-03, -5.7328e-03,  5.0600e-04, -7.0073e-03, -9.7385e-04,\n",
      "          1.1834e-03,  2.9324e-03,  4.0049e-03, -4.5081e-03, -1.0690e-02,\n",
      "          1.3846e-04,  5.9499e-03, -5.0418e-03,  1.9740e-03,  3.7370e-03,\n",
      "         -2.1661e-03,  5.2468e-03],\n",
      "        [-2.6616e-02, -6.9503e-03,  7.5360e-03,  2.0742e-03,  1.3182e-02,\n",
      "          2.4826e-04,  1.2590e-02, -1.6344e-03, -1.2588e-03, -1.1006e-03,\n",
      "         -1.6279e-04, -5.4089e-03, -1.0817e-03,  5.1614e-03, -4.4274e-02,\n",
      "          3.7938e-03, -3.6045e-04,  3.3272e-02, -1.9928e-02,  7.1790e-03,\n",
      "         -4.2244e-03,  2.5588e-02, -2.1162e-03,  7.9215e-04,  5.6764e-04,\n",
      "         -4.1262e-03,  7.2589e-03],\n",
      "        [-1.2791e-04, -3.2168e-03,  8.6052e-04,  1.5944e-03,  4.7394e-04,\n",
      "         -2.0195e-04,  2.6794e-04,  4.5694e-05, -2.6837e-03, -1.3577e-03,\n",
      "          6.3454e-04,  6.5036e-04,  2.8386e-04,  2.3916e-04,  3.2968e-04,\n",
      "         -3.5550e-04,  3.1455e-04,  4.7956e-04, -2.2260e-04,  1.7373e-04,\n",
      "          3.2009e-05,  1.4047e-04,  5.5859e-04,  2.2969e-04,  1.0105e-04,\n",
      "          4.5971e-04,  2.9679e-04],\n",
      "        [ 1.0927e-05, -3.2802e-04,  4.7698e-05,  1.2287e-04,  1.3172e-04,\n",
      "          1.8508e-05,  1.2132e-04,  2.3190e-04, -3.8124e-03, -3.3989e-05,\n",
      "          4.4373e-05, -1.0789e-04,  1.2687e-03,  1.8161e-05,  1.1762e-04,\n",
      "          1.5759e-04,  4.8530e-04,  8.1426e-05,  6.8538e-05,  3.6318e-04,\n",
      "          4.8253e-04,  1.8951e-05,  7.0445e-05,  5.0223e-05,  8.7215e-05,\n",
      "         -4.7527e-05,  3.3071e-04],\n",
      "        [-2.5699e-04, -3.7488e-03,  3.0520e-04,  7.5417e-04,  2.7312e-04,\n",
      "         -2.7221e-03,  4.7664e-03,  2.8316e-03, -5.8370e-03, -2.4092e-03,\n",
      "          1.3107e-03,  8.8607e-04,  2.9412e-04, -3.0342e-05,  1.7216e-04,\n",
      "          1.5275e-03,  6.6206e-04,  1.8567e-04, -8.0468e-04,  4.6572e-04,\n",
      "          2.2850e-04, -9.9490e-04, -3.8124e-05,  7.2457e-04,  1.2109e-03,\n",
      "          6.2076e-05,  1.8169e-04],\n",
      "        [-1.8299e-03, -1.7089e-04,  2.8168e-03,  7.7849e-04, -1.8935e-04,\n",
      "         -5.9631e-03,  9.0381e-04,  3.1484e-03,  4.3697e-03,  5.9967e-04,\n",
      "          1.7018e-03,  1.3123e-03, -1.9513e-03, -4.0824e-04, -5.4309e-03,\n",
      "          4.2452e-03,  1.3847e-03,  3.5187e-04, -1.4085e-03, -2.7073e-03,\n",
      "         -2.8742e-03,  7.2208e-04, -1.2397e-03,  1.2385e-03,  1.8403e-04,\n",
      "         -1.0516e-04,  5.2110e-04],\n",
      "        [-5.2484e-05, -2.8250e-04,  3.0410e-05,  2.8585e-05,  1.8612e-05,\n",
      "         -1.6816e-05, -1.8863e-05,  5.7529e-06,  6.9735e-05, -3.7323e-05,\n",
      "          1.9289e-05,  1.6189e-05,  7.6087e-06,  2.1061e-05,  2.0369e-05,\n",
      "         -3.5688e-06,  1.1508e-05,  1.0234e-05, -1.7431e-05, -3.8554e-07,\n",
      "         -3.7062e-06,  2.2678e-05,  4.4187e-05,  2.6809e-05,  1.6340e-05,\n",
      "          4.7165e-05,  1.6546e-05],\n",
      "        [ 1.9560e-03, -5.5704e-03,  1.0978e-03,  1.2030e-04,  4.6671e-04,\n",
      "         -9.1295e-04,  1.4162e-03,  1.0274e-03, -1.5369e-04, -9.9428e-04,\n",
      "          1.0386e-04,  5.1632e-04,  1.0596e-04,  1.2621e-03, -2.3270e-04,\n",
      "         -7.2469e-04,  9.3126e-05,  1.0572e-04, -1.9414e-04,  3.0639e-04,\n",
      "          8.8025e-05, -1.0392e-03,  9.3050e-05,  9.2367e-05,  6.1904e-04,\n",
      "          3.4451e-05,  3.1731e-04],\n",
      "        [-5.8747e-03, -3.4440e-02,  3.2505e-03,  7.4109e-04,  3.5267e-03,\n",
      "          2.1876e-03,  1.3027e-03,  1.0844e-03,  2.4715e-03, -1.4416e-02,\n",
      "          1.3398e-03,  2.9636e-02,  1.6401e-03,  2.6624e-04,  2.5246e-03,\n",
      "          4.8761e-04,  1.9607e-03,  1.3423e-04, -2.4187e-03,  3.2640e-03,\n",
      "          7.2512e-04, -3.3897e-03, -3.6123e-04,  8.1154e-04,  1.1718e-03,\n",
      "          8.5287e-04,  1.5210e-03],\n",
      "        [-1.6454e-02,  2.3858e-03,  4.0916e-03,  8.2096e-04, -9.6859e-04,\n",
      "          1.7918e-03,  2.3511e-03, -3.5977e-04,  1.5522e-03,  6.8669e-04,\n",
      "          9.9313e-04, -1.0292e-03, -2.7252e-03, -1.1415e-03, -8.0806e-03,\n",
      "          1.4981e-03,  7.0421e-04,  1.2841e-03, -4.2730e-03, -6.6343e-03,\n",
      "         -4.4652e-03,  8.2628e-04,  4.0325e-03,  2.3260e-03,  1.1096e-02,\n",
      "          6.8658e-03,  2.8243e-03],\n",
      "        [-5.0260e-04, -6.3937e-03,  1.0667e-04,  3.5483e-03,  2.9471e-05,\n",
      "         -2.3148e-03,  5.3814e-04,  1.2597e-03,  8.7696e-06, -1.5714e-03,\n",
      "          7.8782e-04,  1.1266e-04,  1.2879e-04,  8.4085e-04,  6.8662e-04,\n",
      "         -4.0772e-04,  2.1789e-04,  6.1369e-05,  4.6471e-06,  1.0752e-04,\n",
      "          6.6930e-04,  1.1698e-04,  6.4644e-05,  2.7041e-05,  1.0683e-03,\n",
      "          1.0568e-04,  6.9908e-04],\n",
      "        [-9.9728e-04, -1.2733e-02,  4.3896e-04,  1.0492e-03,  4.0718e-04,\n",
      "         -1.1102e-03,  3.3553e-04,  2.5189e-03, -8.1444e-04,  1.2987e-03,\n",
      "          1.2862e-03, -1.8117e-04,  8.2153e-04,  3.3062e-04,  5.4054e-04,\n",
      "         -4.8783e-04,  1.9342e-04,  2.0084e-03, -7.6539e-04, -2.2086e-03,\n",
      "         -6.9546e-05, -1.5987e-03,  2.8363e-04,  3.5776e-04,  8.6407e-03,\n",
      "          4.2905e-05,  4.1198e-04],\n",
      "        [-2.4564e-03, -9.8184e-03, -4.8057e-06,  1.1575e-03,  1.5845e-03,\n",
      "         -1.0433e-03,  2.0656e-04,  3.2684e-04,  2.6417e-04, -3.3222e-03,\n",
      "          3.7442e-04,  5.8247e-04, -1.4980e-03, -1.2795e-04,  7.5888e-04,\n",
      "         -3.7466e-04,  8.6913e-04,  1.3362e-04,  1.2933e-03,  1.0624e-03,\n",
      "         -2.1914e-05,  1.6462e-03,  4.3397e-05,  2.0898e-03,  4.3598e-03,\n",
      "          8.0307e-05,  1.8340e-03],\n",
      "        [-1.6629e-03, -1.1612e-02, -2.4594e-04,  2.9462e-04,  2.3994e-04,\n",
      "         -1.0003e-03,  1.4086e-04,  3.5397e-04,  1.8652e-03, -3.9721e-03,\n",
      "          8.0671e-04,  7.7073e-04,  6.3769e-05,  9.1283e-04,  3.8912e-04,\n",
      "          6.0506e-04,  3.3349e-04,  5.1544e-04, -7.2963e-05,  1.2695e-03,\n",
      "          2.2596e-03,  6.8897e-03,  3.4784e-04,  4.7246e-04,  2.8930e-04,\n",
      "         -3.7171e-04,  1.1775e-04],\n",
      "        [-1.1659e-02, -1.5335e-02,  5.3250e-03,  7.7671e-04, -2.1083e-03,\n",
      "          2.6050e-03,  9.7532e-04,  1.3213e-02,  3.5374e-04, -1.2766e-02,\n",
      "         -6.6731e-04, -1.7033e-04,  6.1233e-03, -2.6231e-04, -1.1014e-03,\n",
      "          2.4223e-03,  4.1253e-03,  4.2951e-04,  3.0286e-03,  1.7146e-03,\n",
      "         -3.1266e-03, -1.7357e-03,  1.0228e-04,  1.3229e-03,  4.4512e-04,\n",
      "          5.6361e-03,  3.3336e-04],\n",
      "        [-1.9260e-04,  1.1561e-04, -1.6782e-04,  1.1016e-04, -1.2777e-04,\n",
      "          3.6014e-04,  2.7050e-04, -4.3900e-04,  5.3589e-04,  2.1847e-04,\n",
      "          8.4228e-04, -3.5933e-04, -5.2932e-04, -3.4456e-04, -5.9979e-04,\n",
      "         -1.5862e-03, -3.1090e-04,  1.6096e-03, -3.1522e-04, -7.3830e-04,\n",
      "         -8.6920e-04,  1.0938e-03, -3.9816e-04,  1.6051e-04,  1.1750e-03,\n",
      "          2.4399e-05,  4.6179e-04],\n",
      "        [-4.2081e-04, -2.5323e-03,  3.2165e-04,  5.1698e-04,  3.7427e-03,\n",
      "         -3.4346e-04,  5.0988e-05,  9.9938e-04, -1.6161e-05, -3.0157e-04,\n",
      "          4.5169e-04,  3.8914e-04,  2.3490e-04,  4.5314e-04,  9.7723e-05,\n",
      "         -3.7182e-04,  1.1828e-05,  7.5460e-04, -3.9121e-03,  1.7822e-06,\n",
      "         -5.8591e-05, -1.2595e-03,  9.2178e-05,  1.5398e-04,  3.0341e-04,\n",
      "          2.6936e-04,  3.7086e-04],\n",
      "        [-3.4497e-05, -9.9194e-06, -3.1713e-06,  1.6400e-06,  1.0628e-06,\n",
      "          2.5328e-06,  3.7651e-06,  1.0243e-05,  7.6814e-06, -1.0343e-05,\n",
      "          3.0372e-06,  3.0823e-06,  1.3967e-06,  1.0302e-05,  1.5718e-06,\n",
      "          1.4547e-06, -5.7846e-07,  1.4488e-06,  2.5686e-05,  2.0500e-06,\n",
      "          1.0649e-05, -6.2632e-05,  5.4656e-06,  1.5359e-05,  1.2323e-06,\n",
      "          5.9233e-06,  5.5587e-06],\n",
      "        [-8.8603e-04, -1.9454e-02,  5.7673e-03,  7.2545e-04,  2.9903e-03,\n",
      "         -3.0180e-03,  1.2511e-03,  3.4955e-03,  1.8886e-03, -9.3448e-03,\n",
      "          2.5927e-03,  2.0642e-04,  1.8644e-03, -1.9847e-04,  9.0000e-04,\n",
      "         -1.0206e-03,  9.8029e-04,  3.8105e-04,  8.8358e-03, -2.0310e-03,\n",
      "         -1.8917e-03, -4.9169e-03, -3.3479e-04,  4.0613e-03,  8.4387e-04,\n",
      "          2.5095e-04,  6.0615e-03],\n",
      "        [ 4.8688e-03, -7.5734e-03,  8.8685e-04,  2.1996e-04,  3.0598e-04,\n",
      "         -2.5627e-04,  7.9654e-03,  8.0508e-03, -2.4617e-02, -2.1120e-03,\n",
      "          2.8387e-04,  6.0127e-04,  1.0472e-03,  1.6146e-04, -1.4569e-04,\n",
      "          1.6267e-04,  2.3175e-04,  1.9898e-03,  3.1936e-04,  5.4708e-04,\n",
      "          8.1855e-04, -2.6098e-03,  2.8045e-03,  1.2413e-03,  1.4920e-03,\n",
      "          2.6307e-03,  6.8524e-04],\n",
      "        [ 2.4431e-04, -3.5080e-03,  2.1476e-04,  1.2807e-03,  1.0896e-03,\n",
      "          5.4484e-04,  5.3484e-03,  1.1712e-04, -2.8538e-02, -2.0436e-03,\n",
      "          6.0659e-03,  1.7772e-03,  1.1051e-03,  1.3826e-03,  5.6390e-05,\n",
      "          6.6987e-04,  1.3712e-03,  1.7615e-03,  1.2814e-04,  1.9203e-03,\n",
      "          5.6914e-03,  8.1437e-04,  3.4216e-04,  9.9065e-04,  3.1420e-04,\n",
      "          1.2860e-04,  7.2998e-04],\n",
      "        [ 5.8227e-03,  1.1546e-03, -6.8968e-04,  1.4216e-04, -4.5282e-04,\n",
      "          1.4121e-03,  3.6433e-05,  9.5308e-05,  4.7753e-04,  9.2767e-04,\n",
      "          1.4822e-03, -5.3713e-04, -1.0497e-03,  4.1005e-04, -2.5799e-03,\n",
      "          4.3874e-04, -3.9616e-04,  3.1437e-04, -3.6167e-03, -3.4794e-03,\n",
      "         -2.1323e-03,  1.4598e-03, -1.2187e-03,  6.9816e-04,  4.4807e-04,\n",
      "          8.5911e-04, -2.6464e-05],\n",
      "        [-4.6816e-04, -1.1155e-02,  1.7233e-04,  1.0584e-03,  1.6879e-03,\n",
      "         -2.7633e-03,  1.9991e-03,  1.6321e-04,  4.0813e-04, -7.1311e-03,\n",
      "          5.6743e-04,  2.8241e-03,  9.0551e-04,  9.5529e-04, -5.6171e-05,\n",
      "          4.3679e-04,  2.1625e-03,  7.1798e-04,  9.0464e-04,  1.5224e-03,\n",
      "          1.9092e-03,  8.3981e-04,  9.5249e-04,  1.0223e-03,  3.2400e-04,\n",
      "         -2.8318e-04,  3.2315e-04],\n",
      "        [ 3.9573e-04, -2.4103e-03,  1.6595e-04,  8.2040e-05,  2.9652e-04,\n",
      "          1.1094e-04,  1.0944e-04,  5.5028e-05,  4.5826e-05, -3.4930e-04,\n",
      "          8.1270e-07,  1.4780e-04,  4.9211e-05,  4.5789e-05, -2.4866e-05,\n",
      "          2.2713e-04,  5.2709e-05,  3.9662e-04,  1.4566e-04, -1.0204e-04,\n",
      "          7.9061e-05,  8.4409e-05,  5.3622e-05,  2.9082e-05,  6.8125e-05,\n",
      "          2.0227e-04,  4.2779e-05],\n",
      "        [-8.7423e-06, -3.5407e-05,  9.9535e-06,  2.4123e-06,  7.9153e-06,\n",
      "          1.1879e-05,  2.1356e-05,  2.5236e-05,  1.3501e-06, -4.2470e-05,\n",
      "          1.1228e-05,  3.9849e-06,  9.5575e-06, -8.3593e-05,  7.0850e-06,\n",
      "          1.0196e-06,  6.1396e-06,  2.8061e-06,  5.9088e-06, -7.1090e-05,\n",
      "          3.3364e-06,  6.5470e-05,  5.0326e-06,  7.0878e-06,  2.1989e-05,\n",
      "          7.2233e-06,  3.3312e-06],\n",
      "        [-2.5034e-03, -1.4174e-02,  5.5079e-04,  1.6747e-03,  8.0969e-04,\n",
      "          3.0016e-05,  3.4722e-04,  1.6368e-04,  3.6946e-04,  1.3935e-04,\n",
      "          3.9017e-04,  2.0853e-03,  4.4891e-04,  2.2612e-04,  1.3032e-04,\n",
      "         -3.7953e-04,  1.7769e-03,  4.5266e-04,  2.6690e-03,  3.5311e-04,\n",
      "         -5.7839e-06,  1.5563e-04,  1.8902e-03,  6.3493e-04,  1.2494e-03,\n",
      "          2.6969e-04,  2.4504e-04],\n",
      "        [-8.2988e-05, -7.7988e-05,  4.9417e-05,  8.6957e-06,  1.9588e-05,\n",
      "          6.2889e-05,  1.2269e-04,  1.2428e-04, -6.8518e-04, -7.8426e-05,\n",
      "          3.3926e-05,  1.2898e-05,  3.4804e-05,  9.9408e-05,  9.5090e-08,\n",
      "         -2.4721e-05,  4.8759e-05,  6.5421e-05,  7.0416e-06,  2.6420e-05,\n",
      "          7.1712e-05, -3.0780e-05,  2.4838e-05,  1.0487e-05,  3.0277e-05,\n",
      "          8.3910e-05,  4.2523e-05]])\n"
     ]
    }
   ],
   "source": [
    "# Now, let's calculate the gradients. This can be done by calling the backward() function on the loss tensor.\n",
    "loss.backward()\n",
    "# Now, let's check the gradients. The gradients should be non-zero since we have calculated them.\n",
    "print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1113, -0.4976,  0.1632, -0.8817,  0.0538,  0.6691, -0.0593, -0.4675,\n",
      "         -0.2152,  0.8849, -0.7587, -0.3694, -0.3423, -1.4027,  0.3206, -1.0217,\n",
      "          0.7991, -0.0919, -0.7054, -1.6034,  0.2891,  0.4905, -0.3858, -0.7118,\n",
      "         -0.1702, -1.4597,  0.2212],\n",
      "        [ 0.2436, -1.3255,  0.6977, -0.6628,  1.2171, -1.4949,  0.8822, -1.1788,\n",
      "         -0.9341, -0.5677, -0.2772, -2.1839,  0.3667,  0.9385,  0.0034, -0.3135,\n",
      "         -1.1567,  1.8442, -1.0194,  1.2200,  0.1597,  1.6011, -0.0471, -1.5269,\n",
      "         -2.0142, -1.5177,  0.3885],\n",
      "        [-1.1849,  0.6894,  1.3233,  1.8171,  0.6808,  0.7243,  0.0323, -1.6593,\n",
      "         -1.8776,  0.7371,  0.9258,  0.9248,  0.1826, -0.0737,  0.3147, -1.0369,\n",
      "          0.2101,  0.6145,  0.0628, -0.3297, -1.7970,  0.8728,  0.7670, -0.1138,\n",
      "         -0.9428,  0.7541,  0.1408],\n",
      "        [-0.6937, -0.6159, -0.7295,  0.4308,  0.2862, -0.2481,  0.2040,  0.8519,\n",
      "         -1.4106, -0.1071, -0.8018,  0.2771,  2.5601, -1.6952,  0.1885,  0.7388,\n",
      "          1.5904, -0.1947, -0.2415,  1.3205,  1.5997, -1.0792, -0.3396, -0.6780,\n",
      "         -0.1261, -1.6770,  1.2068],\n",
      "        [ 0.5722,  0.0649, -0.0234,  0.8877,  0.9570, -0.5513,  2.6622,  2.1482,\n",
      "         -0.8561, -0.7210,  1.3756,  0.9989,  0.1469, -0.9053, -0.3587,  1.6375,\n",
      "          0.6898, -0.5844,  0.9077,  0.4849, -0.2632, -0.5433, -1.6406,  0.9295,\n",
      "          1.2908,  0.2612, -0.5861],\n",
      "        [-1.5107, -2.0155,  0.6966, -0.6675, -0.8425,  0.5283, -0.5446,  0.8100,\n",
      "          1.1230, -0.6129,  0.2231,  0.4595, -1.7033, -1.2777, -0.7434,  0.9715,\n",
      "          0.3553, -1.5174, -0.3567,  0.9452, -2.1719, -0.8038, -0.4531, -0.2216,\n",
      "         -2.0901, -0.1771, -0.8980],\n",
      "        [-0.9834,  0.7564,  0.3474,  0.2856, -0.0495,  0.6225, -1.9217, -1.3176,\n",
      "          1.2287,  0.7025, -0.0170, -0.2830, -0.6446, -0.0199,  0.1123, -0.6601,\n",
      "         -0.6243, -0.7416,  0.3422, -0.2873, -0.6726,  0.8843,  0.7211,  0.2214,\n",
      "         -0.2737,  0.8612, -0.0609],\n",
      "        [ 2.1075, -0.9936,  1.4081, -0.7945,  0.6144, -0.2731,  1.6562,  1.3941,\n",
      "          0.6059,  0.2208, -0.8245,  0.7289, -0.7336,  1.5625, -1.0208, -0.0176,\n",
      "         -0.9194, -0.9389, -1.9421,  0.2329, -1.1014, -1.2474, -0.7484, -0.9792,\n",
      "          0.8286, -0.2501,  0.1603],\n",
      "        [ 0.7289, -0.4475,  0.8217, -0.6014,  0.9073,  1.5693, -0.1107, -0.2572,\n",
      "          0.5634,  0.0615, -0.0638,  3.0321,  0.1859, -0.1703,  1.0273,  0.0705,\n",
      "          0.3588, -2.3593, -1.3957,  0.8678, -0.2251,  0.1190, -1.0772,  0.2517,\n",
      "         -0.2195,  0.6062,  0.0475],\n",
      "        [ 0.0108,  0.3777,  0.7337, -0.4914, -1.1050, -0.0389,  0.0418, -1.0468,\n",
      "          0.0337, -1.0544, -0.1972,  0.8521, -0.4068, -1.4114,  0.0192, -0.4114,\n",
      "         -0.3398, -0.5562, -0.6512, -0.2185,  0.7695, -0.9988,  1.1110,  0.0314,\n",
      "          1.5707,  1.5396,  0.2987],\n",
      "        [ 0.5560, -0.6875, -1.1066,  2.2693, -2.2510, -1.0118,  0.3829,  1.2364,\n",
      "         -1.0535, -0.9405,  0.8783, -0.8629, -0.9917,  0.8486,  0.7377, -1.3159,\n",
      "         -0.4561, -1.7884, -1.4166, -0.9863,  0.6092,  0.6668, -1.1206, -1.6680,\n",
      "          1.0686, -0.1831,  0.6445],\n",
      "        [-0.5593, -0.4493, -0.6475,  0.2084, -0.7377, -0.3219, -0.9491,  1.0706,\n",
      "         -1.0470,  1.3771,  0.3990, -1.2153, -0.0315, -0.8687, -0.3510, -0.2000,\n",
      "         -1.4447,  0.8404, -0.8669, -0.9730, -1.2452, -0.0116, -1.0545, -0.8748,\n",
      "          2.3002, -1.4453, -0.7350],\n",
      "        [-1.6075, -1.1048, -1.8680,  0.8120,  1.1540, -0.5541, -0.8932, -0.3464,\n",
      "         -0.5365, -0.2296, -0.2483,  0.3545, -0.6694, -2.3203,  0.5022, -0.1246,\n",
      "          0.6527, -1.3451,  0.9637,  0.8351, -2.7675,  1.4804,  0.2942,  1.3926,\n",
      "          2.1134, -0.0856,  1.2582],\n",
      "        [-1.8496, -1.2264, -0.6666, -0.5089, -0.5193,  0.4295, -1.3418, -0.3649,\n",
      "          1.2699,  0.5686,  0.4716,  0.4617, -1.3822,  0.8790, -0.0989,  1.2044,\n",
      "          0.1998, -0.0445, -0.2705,  1.1503,  1.4568,  2.7647, -0.3187, -0.1316,\n",
      "         -0.6221, -1.4301, -1.4317],\n",
      "        [ 2.1143, -1.1868,  1.2047, -0.3534,  0.3413,  1.3210, -0.5344,  2.2025,\n",
      "         -1.2146, -0.8655, -1.0132, -0.3668,  1.3070, -2.1880, -2.0553,  0.7387,\n",
      "          0.9458, -1.3522,  0.6632,  0.5437,  0.5325, -0.7293, -1.3583, -0.1867,\n",
      "         -1.3207,  1.4492, -1.5154],\n",
      "        [-1.2364, -0.6758, -0.0800, -0.4060,  0.2415,  0.2560, -0.0605, -0.1540,\n",
      "          1.4020, -0.0311,  1.4726, -0.4815, -0.8534,  0.2309,  0.7534, -0.6288,\n",
      "         -0.7702,  1.6714,  0.7765,  0.4891, -0.5016,  1.6711, -0.8182,  0.7552,\n",
      "          1.3664, -0.4598,  0.6669],\n",
      "        [-0.7839,  2.1908,  0.3532,  0.8087,  2.7851, -0.5777, -1.5113,  1.4644,\n",
      "         -0.5434,  1.1363,  0.7061,  0.5938,  0.1270,  0.6934, -0.5838,  0.6027,\n",
      "          0.1503,  1.1834, -0.7286, -1.2537, -0.6925, -2.8564, -0.8612, -0.4060,\n",
      "          0.2722,  0.2987,  0.4730],\n",
      "        [-0.4718, -1.1712, -1.9521, -0.7573, -0.1884,  0.2220,  0.0738,  1.0746,\n",
      "          0.7868, -0.3379, -0.1411, -0.1263, -0.9179,  1.0803, -0.7998, -0.8772,\n",
      "         -1.0245, -0.8813,  1.9939,  0.1048,  1.1134, -0.0314,  0.4465,  1.4797,\n",
      "         -1.0431,  0.7966,  0.4633],\n",
      "        [ 0.5286,  0.0911,  1.2825, -0.6289,  0.7275, -1.4302, -0.2679,  0.8160,\n",
      "          0.2063, -0.7039,  0.5771, -1.1972,  0.2661, -0.7091,  0.1463, -1.7972,\n",
      "         -0.3574, -1.4564,  1.7182, -1.0186, -2.1021, -1.6986, -1.0374,  0.9234,\n",
      "         -0.6661, -0.6180,  1.3113],\n",
      "        [ 1.4741, -0.2859, -0.3725, -1.6563, -1.4151, -0.1897,  1.7868,  1.8009,\n",
      "          0.9765, -0.4629, -1.4979, -0.4023, -0.1687, -0.7294, -2.3340, -0.4907,\n",
      "         -1.4583,  0.3992,  0.3334, -0.4138,  0.0513, -1.3814,  0.8961,  0.5973,\n",
      "          0.1112,  0.7747, -0.6643],\n",
      "        [ 0.6153, -0.5209, -1.7862,  0.0208, -0.2502, -0.1451,  1.3396, -2.4515,\n",
      "         -1.4938, -0.7996,  1.4667,  0.2994, -0.1398,  0.0666, -2.0364, -0.4900,\n",
      "          0.0225,  0.2285, -0.1856,  0.4139,  1.5253, -0.0229, -1.1519, -0.3089,\n",
      "         -1.4955, -0.7526, -0.6524],\n",
      "        [ 2.4230,  0.5198, -0.3199, -0.6821,  0.2706,  0.6930, -2.3074,  0.4205,\n",
      "          0.2501,  0.2797,  1.2182,  0.1727,  0.2068,  1.4007, -1.0218, -0.5113,\n",
      "         -0.5124, -0.8314, -1.9171, -0.3381, -0.8734,  0.6993, -1.7235,  0.0585,\n",
      "         -0.2682,  0.3430, -2.3405],\n",
      "        [-0.8842, -0.3792, -1.6314,  0.1558,  0.6459, -0.8069,  0.7884, -1.6625,\n",
      "         -0.7064,  0.4471, -0.4299,  1.1494,  0.0164,  0.0613, -2.6981, -0.5081,\n",
      "          0.8745, -0.2358,  0.2436,  0.5432,  0.7500,  0.1239,  0.1318,  0.1212,\n",
      "         -1.0315, -0.7186, -1.0172],\n",
      "        [ 1.2741,  0.1501,  0.3641, -0.3623,  0.9709,  0.8835, -0.0742, -0.6366,\n",
      "         -0.6977,  0.6453, -2.9267,  0.2748, -0.6125, -0.2925, -1.0978,  0.6956,\n",
      "         -0.7705,  1.2135,  0.8028, -0.4904,  0.2571, -0.2913, -0.6593, -1.3994,\n",
      "         -0.5482,  0.6185, -1.0135],\n",
      "        [-0.6152, -0.4278,  0.2729, -0.5790,  0.0437,  1.0218,  1.0363,  1.2032,\n",
      "         -0.1005, -0.8075,  0.3934, -0.6425,  0.2323,  0.7118,  0.1631, -0.9765,\n",
      "         -0.2103, -0.9933, -0.2486, -0.6137, -0.8202,  2.1842, -0.4091, -0.0667,\n",
      "          1.0655,  0.5185, -0.8217],\n",
      "        [-1.6378, -0.5376, -0.1287,  0.9669,  0.2972,  0.2010, -0.6132, -1.2794,\n",
      "         -0.4037,  0.0097, -0.4733,  1.2126, -0.1738, -0.7859, -0.6041, -0.2325,\n",
      "          1.0330, -0.3480,  1.4597, -0.0559, -1.2836,  0.7126,  1.1083,  0.0048,\n",
      "          0.6674, -0.5581, -0.9543],\n",
      "        [ 0.2181,  2.0906,  0.4160, -1.3930, -0.5809,  1.4967,  1.2538,  1.2667,\n",
      "         -0.0937,  0.1063, -0.0317, -0.9988,  0.0941,  1.1970, -0.9961, -1.6909,\n",
      "          0.3310,  0.6250, -0.7701, -0.2146,  0.7421,  0.0571,  0.0732, -0.4882,\n",
      "         -0.1455,  0.9776,  0.3895]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Just a point to remember - Apparently, there is a difference when you use weights.data and weights in the below\n",
    "# equation. weights.data is the actual tensor and weights is a wrapper around the tensor. Apparently, if you use\n",
    "# weights, this changes something in the computation graph and gradient calculation is impacted -- I didn't really\n",
    "# understand the details. But, it is better to use weights.data to update the weights.\n",
    "#\n",
    "# Now, let's just update the weights using the gradients. We will use a learning rate of 0.1 (randomly chosen).\n",
    "weights.data += 0.1 * weights.grad\n",
    "# Let's check the updated weights.\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7987, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now, let's compute the loss again. It should be lesser than what we have computed above since we did\n",
    "# one loop of optimization using gradient descent.\n",
    "model_output_iter2 = encoded_inputs @ weights\n",
    "logits_iter2 = model_output_iter2.exp()\n",
    "probs_iter2 = logits_iter2 / logits_iter2.sum(dim=1, keepdim=True)\n",
    "target_probs_iter2 = probs_iter2[range(len(targets)), targets]\n",
    "loss_iter2 = -torch.log(target_probs_iter2).mean()\n",
    "# Note that the loss before was '3.9736' and the loss now is '3.9685'. Looking at the tiny change, I \n",
    "# think we can use slightly higher learning rate.\n",
    "print(loss_iter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUTTING TRAINING LOOP TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the above steps need to be repeated multiple times to train the model. This is called a training loop.\n",
    "# Let's put all the above steps in a function and run the training loop and observe how the loss changes over time.\n",
    "def training_loop(inputs: Tensor, targets: Tensor, weights: Tensor, num_loops: int, learning_rate: float):\n",
    "    for iteration in range(num_loops):\n",
    "        # Forward Propagation\n",
    "        model_output = inputs @ weights\n",
    "        logits = model_output.exp()\n",
    "        probs = logits / logits.sum(dim=1, keepdim=True)\n",
    "        target_probs = probs[torch.arange(start=0, end=len(targets)), targets]\n",
    "        loss = -torch.log(target_probs).mean()\n",
    "        print(f\"Loss after iteration {iteration} is {loss.item()}\")\n",
    "        # Back Propagation\n",
    "        # Always, zero the weights from the previous loop. Other it will update the gradients instead of over-writing.\n",
    "        weights.grad = None\n",
    "        loss.backward()\n",
    "        weights.data += -learning_rate * weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 is 3.798703670501709\n",
      "Loss after iteration 1 is 3.661177158355713\n",
      "Loss after iteration 2 is 3.541262626647949\n",
      "Loss after iteration 3 is 3.4366979598999023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 4 is 3.3456366062164307\n",
      "Loss after iteration 5 is 3.2659871578216553\n",
      "Loss after iteration 6 is 3.195650339126587\n",
      "Loss after iteration 7 is 3.1329286098480225\n",
      "Loss after iteration 8 is 3.0766279697418213\n",
      "Loss after iteration 9 is 3.025914430618286\n",
      "Loss after iteration 10 is 2.9801418781280518\n",
      "Loss after iteration 11 is 2.9387569427490234\n",
      "Loss after iteration 12 is 2.90126371383667\n",
      "Loss after iteration 13 is 2.867218255996704\n",
      "Loss after iteration 14 is 2.8362231254577637\n",
      "Loss after iteration 15 is 2.80792498588562\n",
      "Loss after iteration 16 is 2.7820098400115967\n",
      "Loss after iteration 17 is 2.7582008838653564\n",
      "Loss after iteration 18 is 2.7362561225891113\n",
      "Loss after iteration 19 is 2.715965509414673\n",
      "Loss after iteration 20 is 2.6971476078033447\n",
      "Loss after iteration 21 is 2.6796483993530273\n",
      "Loss after iteration 22 is 2.6633341312408447\n",
      "Loss after iteration 23 is 2.6480886936187744\n",
      "Loss after iteration 24 is 2.6338131427764893\n",
      "Loss after iteration 25 is 2.62041974067688\n",
      "Loss after iteration 26 is 2.6078312397003174\n",
      "Loss after iteration 27 is 2.5959794521331787\n",
      "Loss after iteration 28 is 2.584803581237793\n",
      "Loss after iteration 29 is 2.574248790740967\n",
      "Loss after iteration 30 is 2.5642662048339844\n",
      "Loss after iteration 31 is 2.554811477661133\n",
      "Loss after iteration 32 is 2.545844554901123\n",
      "Loss after iteration 33 is 2.5373287200927734\n",
      "Loss after iteration 34 is 2.529230833053589\n",
      "Loss after iteration 35 is 2.5215208530426025\n",
      "Loss after iteration 36 is 2.5141704082489014\n",
      "Loss after iteration 37 is 2.507154941558838\n",
      "Loss after iteration 38 is 2.5004501342773438\n",
      "Loss after iteration 39 is 2.4940359592437744\n",
      "Loss after iteration 40 is 2.4878921508789062\n",
      "Loss after iteration 41 is 2.4820001125335693\n",
      "Loss after iteration 42 is 2.476344585418701\n",
      "Loss after iteration 43 is 2.4709091186523438\n",
      "Loss after iteration 44 is 2.4656808376312256\n",
      "Loss after iteration 45 is 2.460646390914917\n",
      "Loss after iteration 46 is 2.455794095993042\n",
      "Loss after iteration 47 is 2.451113224029541\n",
      "Loss after iteration 48 is 2.44659423828125\n",
      "Loss after iteration 49 is 2.4422271251678467\n",
      "Loss after iteration 50 is 2.4380037784576416\n",
      "Loss after iteration 51 is 2.4339170455932617\n",
      "Loss after iteration 52 is 2.4299585819244385\n",
      "Loss after iteration 53 is 2.4261229038238525\n",
      "Loss after iteration 54 is 2.422403335571289\n",
      "Loss after iteration 55 is 2.418794631958008\n",
      "Loss after iteration 56 is 2.4152910709381104\n",
      "Loss after iteration 57 is 2.4118878841400146\n",
      "Loss after iteration 58 is 2.408581018447876\n",
      "Loss after iteration 59 is 2.405365467071533\n",
      "Loss after iteration 60 is 2.402237892150879\n",
      "Loss after iteration 61 is 2.3991940021514893\n",
      "Loss after iteration 62 is 2.396230459213257\n",
      "Loss after iteration 63 is 2.3933451175689697\n",
      "Loss after iteration 64 is 2.390532970428467\n",
      "Loss after iteration 65 is 2.3877928256988525\n",
      "Loss after iteration 66 is 2.3851211071014404\n",
      "Loss after iteration 67 is 2.3825151920318604\n",
      "Loss after iteration 68 is 2.3799734115600586\n",
      "Loss after iteration 69 is 2.377493143081665\n",
      "Loss after iteration 70 is 2.3750712871551514\n",
      "Loss after iteration 71 is 2.372706890106201\n",
      "Loss after iteration 72 is 2.3703975677490234\n",
      "Loss after iteration 73 is 2.3681418895721436\n",
      "Loss after iteration 74 is 2.3659369945526123\n",
      "Loss after iteration 75 is 2.3637821674346924\n",
      "Loss after iteration 76 is 2.361675262451172\n",
      "Loss after iteration 77 is 2.3596155643463135\n",
      "Loss after iteration 78 is 2.357600688934326\n",
      "Loss after iteration 79 is 2.3556294441223145\n",
      "Loss after iteration 80 is 2.353700637817383\n",
      "Loss after iteration 81 is 2.351813316345215\n",
      "Loss after iteration 82 is 2.3499653339385986\n",
      "Loss after iteration 83 is 2.348156452178955\n",
      "Loss after iteration 84 is 2.3463852405548096\n",
      "Loss after iteration 85 is 2.3446500301361084\n",
      "Loss after iteration 86 is 2.3429503440856934\n",
      "Loss after iteration 87 is 2.341285228729248\n",
      "Loss after iteration 88 is 2.339653730392456\n",
      "Loss after iteration 89 is 2.3380544185638428\n",
      "Loss after iteration 90 is 2.336487054824829\n",
      "Loss after iteration 91 is 2.3349506855010986\n",
      "Loss after iteration 92 is 2.333444118499756\n",
      "Loss after iteration 93 is 2.3319671154022217\n",
      "Loss after iteration 94 is 2.3305184841156006\n",
      "Loss after iteration 95 is 2.3290975093841553\n",
      "Loss after iteration 96 is 2.3277032375335693\n",
      "Loss after iteration 97 is 2.326336145401001\n",
      "Loss after iteration 98 is 2.3249943256378174\n",
      "Loss after iteration 99 is 2.3236773014068604\n",
      "Loss after iteration 100 is 2.322385549545288\n",
      "Loss after iteration 101 is 2.3211171627044678\n",
      "Loss after iteration 102 is 2.3198723793029785\n",
      "Loss after iteration 103 is 2.318650484085083\n",
      "Loss after iteration 104 is 2.3174502849578857\n",
      "Loss after iteration 105 is 2.316272258758545\n",
      "Loss after iteration 106 is 2.315115213394165\n",
      "Loss after iteration 107 is 2.313979148864746\n",
      "Loss after iteration 108 is 2.312863349914551\n",
      "Loss after iteration 109 is 2.311767101287842\n",
      "Loss after iteration 110 is 2.310690402984619\n",
      "Loss after iteration 111 is 2.3096325397491455\n",
      "Loss after iteration 112 is 2.308593511581421\n",
      "Loss after iteration 113 is 2.307572364807129\n",
      "Loss after iteration 114 is 2.3065688610076904\n",
      "Loss after iteration 115 is 2.3055830001831055\n",
      "Loss after iteration 116 is 2.3046138286590576\n",
      "Loss after iteration 117 is 2.303661584854126\n",
      "Loss after iteration 118 is 2.302725315093994\n",
      "Loss after iteration 119 is 2.301805257797241\n",
      "Loss after iteration 120 is 2.300900459289551\n",
      "Loss after iteration 121 is 2.300010919570923\n",
      "Loss after iteration 122 is 2.2991366386413574\n",
      "Loss after iteration 123 is 2.298276662826538\n",
      "Loss after iteration 124 is 2.297430992126465\n",
      "Loss after iteration 125 is 2.2965993881225586\n",
      "Loss after iteration 126 is 2.2957818508148193\n",
      "Loss after iteration 127 is 2.2949771881103516\n",
      "Loss after iteration 128 is 2.2941861152648926\n",
      "Loss after iteration 129 is 2.293407440185547\n",
      "Loss after iteration 130 is 2.2926416397094727\n",
      "Loss after iteration 131 is 2.29188871383667\n",
      "Loss after iteration 132 is 2.291147232055664\n",
      "Loss after iteration 133 is 2.2904179096221924\n",
      "Loss after iteration 134 is 2.2897000312805176\n",
      "Loss after iteration 135 is 2.288994312286377\n",
      "Loss after iteration 136 is 2.2882986068725586\n",
      "Loss after iteration 137 is 2.287614583969116\n",
      "Loss after iteration 138 is 2.2869412899017334\n",
      "Loss after iteration 139 is 2.286278247833252\n",
      "Loss after iteration 140 is 2.285625696182251\n",
      "Loss after iteration 141 is 2.2849831581115723\n",
      "Loss after iteration 142 is 2.284350633621216\n",
      "Loss after iteration 143 is 2.2837278842926025\n",
      "Loss after iteration 144 is 2.283114433288574\n",
      "Loss after iteration 145 is 2.28251051902771\n",
      "Loss after iteration 146 is 2.2819156646728516\n",
      "Loss after iteration 147 is 2.281330108642578\n",
      "Loss after iteration 148 is 2.2807528972625732\n",
      "Loss after iteration 149 is 2.280184268951416\n",
      "Loss after iteration 150 is 2.2796244621276855\n",
      "Loss after iteration 151 is 2.2790727615356445\n",
      "Loss after iteration 152 is 2.278529167175293\n",
      "Loss after iteration 153 is 2.277993679046631\n",
      "Loss after iteration 154 is 2.2774658203125\n",
      "Loss after iteration 155 is 2.2769458293914795\n",
      "Loss after iteration 156 is 2.2764334678649902\n",
      "Loss after iteration 157 is 2.275928497314453\n",
      "Loss after iteration 158 is 2.275430202484131\n",
      "Loss after iteration 159 is 2.2749392986297607\n",
      "Loss after iteration 160 is 2.2744553089141846\n",
      "Loss after iteration 161 is 2.2739782333374023\n",
      "Loss after iteration 162 is 2.273508310317993\n",
      "Loss after iteration 163 is 2.2730445861816406\n",
      "Loss after iteration 164 is 2.2725868225097656\n",
      "Loss after iteration 165 is 2.2721359729766846\n",
      "Loss after iteration 166 is 2.271691083908081\n",
      "Loss after iteration 167 is 2.2712526321411133\n",
      "Loss after iteration 168 is 2.270819664001465\n",
      "Loss after iteration 169 is 2.270392656326294\n",
      "Loss after iteration 170 is 2.2699718475341797\n",
      "Loss after iteration 171 is 2.2695562839508057\n",
      "Loss after iteration 172 is 2.269146203994751\n",
      "Loss after iteration 173 is 2.2687416076660156\n",
      "Loss after iteration 174 is 2.2683427333831787\n",
      "Loss after iteration 175 is 2.267948627471924\n",
      "Loss after iteration 176 is 2.2675600051879883\n",
      "Loss after iteration 177 is 2.2671759128570557\n",
      "Loss after iteration 178 is 2.2667970657348633\n",
      "Loss after iteration 179 is 2.266423225402832\n",
      "Loss after iteration 180 is 2.2660539150238037\n",
      "Loss after iteration 181 is 2.2656898498535156\n",
      "Loss after iteration 182 is 2.2653298377990723\n",
      "Loss after iteration 183 is 2.264974355697632\n",
      "Loss after iteration 184 is 2.2646234035491943\n",
      "Loss after iteration 185 is 2.2642767429351807\n",
      "Loss after iteration 186 is 2.263934373855591\n",
      "Loss after iteration 187 is 2.263596296310425\n",
      "Loss after iteration 188 is 2.2632620334625244\n",
      "Loss after iteration 189 is 2.262932538986206\n",
      "Loss after iteration 190 is 2.262606382369995\n",
      "Loss after iteration 191 is 2.262284517288208\n",
      "Loss after iteration 192 is 2.2619662284851074\n",
      "Loss after iteration 193 is 2.2616519927978516\n",
      "Loss after iteration 194 is 2.261341094970703\n",
      "Loss after iteration 195 is 2.261033773422241\n",
      "Loss after iteration 196 is 2.260730266571045\n",
      "Loss after iteration 197 is 2.2604305744171143\n",
      "Loss after iteration 198 is 2.260133743286133\n",
      "Loss after iteration 199 is 2.259840488433838\n",
      "Loss after iteration 200 is 2.2595508098602295\n",
      "Loss after iteration 201 is 2.2592639923095703\n",
      "Loss after iteration 202 is 2.2589807510375977\n",
      "Loss after iteration 203 is 2.2587006092071533\n",
      "Loss after iteration 204 is 2.2584238052368164\n",
      "Loss after iteration 205 is 2.2581496238708496\n",
      "Loss after iteration 206 is 2.257878541946411\n",
      "Loss after iteration 207 is 2.257610321044922\n",
      "Loss after iteration 208 is 2.25734543800354\n",
      "Loss after iteration 209 is 2.257082939147949\n",
      "Loss after iteration 210 is 2.2568235397338867\n",
      "Loss after iteration 211 is 2.2565667629241943\n",
      "Loss after iteration 212 is 2.256312847137451\n",
      "Loss after iteration 213 is 2.256061553955078\n",
      "Loss after iteration 214 is 2.255812644958496\n",
      "Loss after iteration 215 is 2.2555668354034424\n",
      "Loss after iteration 216 is 2.2553229331970215\n",
      "Loss after iteration 217 is 2.255082130432129\n",
      "Loss after iteration 218 is 2.2548437118530273\n",
      "Loss after iteration 219 is 2.2546074390411377\n",
      "Loss after iteration 220 is 2.2543740272521973\n",
      "Loss after iteration 221 is 2.2541422843933105\n",
      "Loss after iteration 222 is 2.253913164138794\n",
      "Loss after iteration 223 is 2.2536869049072266\n",
      "Loss after iteration 224 is 2.253462076187134\n",
      "Loss after iteration 225 is 2.253239631652832\n",
      "Loss after iteration 226 is 2.2530195713043213\n",
      "Loss after iteration 227 is 2.2528016567230225\n",
      "Loss after iteration 228 is 2.2525856494903564\n",
      "Loss after iteration 229 is 2.2523717880249023\n",
      "Loss after iteration 230 is 2.252159833908081\n",
      "Loss after iteration 231 is 2.25195050239563\n",
      "Loss after iteration 232 is 2.251742362976074\n",
      "Loss after iteration 233 is 2.2515366077423096\n",
      "Loss after iteration 234 is 2.2513325214385986\n",
      "Loss after iteration 235 is 2.2511308193206787\n",
      "Loss after iteration 236 is 2.2509305477142334\n",
      "Loss after iteration 237 is 2.250732183456421\n",
      "Loss after iteration 238 is 2.250535726547241\n",
      "Loss after iteration 239 is 2.2503411769866943\n",
      "Loss after iteration 240 is 2.250148057937622\n",
      "Loss after iteration 241 is 2.2499570846557617\n",
      "Loss after iteration 242 is 2.249767541885376\n",
      "Loss after iteration 243 is 2.249580144882202\n",
      "Loss after iteration 244 is 2.2493937015533447\n",
      "Loss after iteration 245 is 2.249209403991699\n",
      "Loss after iteration 246 is 2.249026298522949\n",
      "Loss after iteration 247 is 2.248845338821411\n",
      "Loss after iteration 248 is 2.2486655712127686\n",
      "Loss after iteration 249 is 2.2484874725341797\n",
      "Loss after iteration 250 is 2.2483108043670654\n",
      "Loss after iteration 251 is 2.248135805130005\n",
      "Loss after iteration 252 is 2.247962236404419\n",
      "Loss after iteration 253 is 2.2477900981903076\n",
      "Loss after iteration 254 is 2.247619867324829\n",
      "Loss after iteration 255 is 2.247450590133667\n",
      "Loss after iteration 256 is 2.2472825050354004\n",
      "Loss after iteration 257 is 2.2471165657043457\n",
      "Loss after iteration 258 is 2.2469513416290283\n",
      "Loss after iteration 259 is 2.2467877864837646\n",
      "Loss after iteration 260 is 2.2466256618499756\n",
      "Loss after iteration 261 is 2.246464729309082\n",
      "Loss after iteration 262 is 2.246304988861084\n",
      "Loss after iteration 263 is 2.2461466789245605\n",
      "Loss after iteration 264 is 2.2459897994995117\n",
      "Loss after iteration 265 is 2.2458338737487793\n",
      "Loss after iteration 266 is 2.2456793785095215\n",
      "Loss after iteration 267 is 2.24552583694458\n",
      "Loss after iteration 268 is 2.2453739643096924\n",
      "Loss after iteration 269 is 2.245223045349121\n",
      "Loss after iteration 270 is 2.245073080062866\n",
      "Loss after iteration 271 is 2.244924545288086\n",
      "Loss after iteration 272 is 2.2447774410247803\n",
      "Loss after iteration 273 is 2.244630813598633\n",
      "Loss after iteration 274 is 2.244486093521118\n",
      "Loss after iteration 275 is 2.2443418502807617\n",
      "Loss after iteration 276 is 2.244199275970459\n",
      "Loss after iteration 277 is 2.2440574169158936\n",
      "Loss after iteration 278 is 2.2439167499542236\n",
      "Loss after iteration 279 is 2.243777275085449\n",
      "Loss after iteration 280 is 2.243638515472412\n",
      "Loss after iteration 281 is 2.2435009479522705\n",
      "Loss after iteration 282 is 2.2433643341064453\n",
      "Loss after iteration 283 is 2.2432291507720947\n",
      "Loss after iteration 284 is 2.2430944442749023\n",
      "Loss after iteration 285 is 2.2429611682891846\n",
      "Loss after iteration 286 is 2.242828607559204\n",
      "Loss after iteration 287 is 2.24269700050354\n",
      "Loss after iteration 288 is 2.2425665855407715\n",
      "Loss after iteration 289 is 2.2424368858337402\n",
      "Loss after iteration 290 is 2.2423081398010254\n",
      "Loss after iteration 291 is 2.242180824279785\n",
      "Loss after iteration 292 is 2.242053985595703\n",
      "Loss after iteration 293 is 2.2419278621673584\n",
      "Loss after iteration 294 is 2.241802930831909\n",
      "Loss after iteration 295 is 2.2416789531707764\n",
      "Loss after iteration 296 is 2.24155592918396\n",
      "Loss after iteration 297 is 2.2414333820343018\n",
      "Loss after iteration 298 is 2.241312026977539\n",
      "Loss after iteration 299 is 2.2411913871765137\n"
     ]
    }
   ],
   "source": [
    "# Note that the loss we obtained here is very close to the loss we obtained in the rule-based model. This is because\n",
    "# the neural network we built is equivalent to the rule-based model as explained above. Ofcourse, this model seems\n",
    "# slightly better than the rule-based model.\n",
    "# Rule-based model loss: 2.483\n",
    "# Neural network model loss: 2.241\n",
    "training_loop(inputs=encoded_inputs, targets=targets, weights=weights, num_loops=300, learning_rate=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have seen how to train a simple neural network to generate names. Now, let's see how to generate names using this\n",
    "# trained model. The idea is to take a character as input, pass it through the neural network, get the probabilities\n",
    "def generate_names(weights: Tensor, char_to_int: dict, int_to_char: dict, num_names: int):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    for _ in range(num_names):\n",
    "        name = BOUND_CHARACTER\n",
    "        while True:\n",
    "            prev_char_idx = char_to_int[name[-1]]\n",
    "            input_tensor = torch.zeros(size=(1, 27), dtype=torch.float32)\n",
    "            input_tensor[0][prev_char_idx] = 1.0\n",
    "            output_tensor = input_tensor @ weights\n",
    "            logits = output_tensor.exp()\n",
    "            output_probs = logits / logits.sum(dim=1, keepdim=True)\n",
    "            output_char = int_to_char[torch.multinomial(output_probs, num_samples=1).item()]\n",
    "            if output_char == BOUND_CHARACTER:\n",
    "                break\n",
    "            name += output_char\n",
    "        print(name[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n",
      "velivadanilfithivarar\n",
      "yaakumain\n",
      "theuthinove\n",
      "gasubhumath\n",
      "thewarumady\n",
      "inthinurath\n",
      "arakuja\n",
      "thidufukokanusharina\n",
      "ven\n",
      "vanuksith\n",
      "ntheveg\n",
      "satithesila\n",
      "jahagenoraja\n",
      "asanthran\n",
      "ati\n",
      "ueleva\n",
      "nishina\n",
      "h\n",
      "hthavathaila\n"
     ]
    }
   ],
   "source": [
    "# Note the names are also very similar to the names generated by the rule-based model.\n",
    "generate_names(weights=weights, char_to_int=char_to_int, int_to_char=int_to_char, num_names=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".makemore_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
