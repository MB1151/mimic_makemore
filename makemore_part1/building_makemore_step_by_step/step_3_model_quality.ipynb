{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) What is likelihood?\n",
    "# 2) What is Maximum Likelihood Estimation (MLE)? \n",
    "# 3) How to use likelihood in the context of our name generation problem?\n",
    "# 4) How to evaluate the quality of the model using the concept of likelihood?\n",
    "#\n",
    "# Resources:\n",
    "#\n",
    "# 1) https://www.youtube.com/watch?v=ScduwntrMzc&t=41s\n",
    "#       -- This video explains the concept of likelihood and several other related concepts.\n",
    "# 2) https://www.youtube.com/watch?v=7kLHJ-F33GI&t=40s\n",
    "#       -- This video explains the concept of Maximum Likelihood Estimation (MLE) and several other related concepts.\n",
    "#       -- First 20 minutes of this video is enough to understand the concept of MLE.\n",
    "#\n",
    "# Please watch these videos before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with understanding Likelihood first.\n",
    "#\n",
    "# Let's say we have a coin that was flipped 'n' times and we observed heads 'h' times. Now, we want to estimate the \n",
    "# probability 'p' of getting heads when we flip the coin. We can use the likelihood function to estimate the probability.\n",
    "#\n",
    "# Likelihood shows how likely a particular probability 'p' (0.5 for example) is given the data we have. Higher the\n",
    "# likelihood, more likely the particular probability value 'p' is. However, likelihood by itself doesn't give us very \n",
    "# useful information. It is the relative likelihood that is generally more useful. Relative likelihood is the ratio of \n",
    "# likelihood for two probability values (p). For example, if the likelihood of getting heads with probability 0.5 is \n",
    "# 0.1 and the likelihood of getting heads with probability 0.6 is 0.2, then the relative likelihood of getting heads\n",
    "# with probability 0.6 is (0.2/0.1) = 2.0.\n",
    "#\n",
    "# For the case of coin flip, likelihood is defined as:\n",
    "# Likelihood(p|data) = Probabability of observing the data given the heads probability is 'p'\n",
    "#\n",
    "# In general, let 'theta' be the population parameter (equivalent to 'p' in the coin flip example) and 'Y' be the data. \n",
    "# Then, the likelihood function is defined as:\n",
    "# \n",
    "# Likelihood(theta|Y) = Probabaility of observing the data 'Y' given that the population parameter is 'theta'.\n",
    "#\n",
    "# Likelihood describes the extent to which the data supports any particular value of the population parameter. \n",
    "# Higher support corresponds to higher value of likelihood.\n",
    "#\n",
    "# Extending this concept to the case of multiple data points, the likelihood function is a function that relates the \n",
    "# population parameter to the data. Basically, likelihood function is just a mathematical function that takes in data,\n",
    "# population parameter as input and returns the likelihood of the population parameter.\n",
    "#\n",
    "# NOTE: In most places (textbooks, blogs, ... etc), data is referred to as sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, there are usually multiple data points for the same population parameter. In such case, we need to use \n",
    "# all the data points to calculate the likelihood. In this case, the likelihood is the product of the likelihoods of \n",
    "# each data point. This turns out to be the product of the probabilities of observing each data point given the \n",
    "# population parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, likelihood is defined as:\n",
    "\n",
    "$$L(\\theta_{0};y) = Prob(Y=y;\\theta=\\theta_{0}) = f_{Y}(y;\\theta_{0})$$\n",
    "\n",
    "If it is a discrete probability distribution, then we use the probability mass function. <br>\n",
    "If it is a continuous probability distribution, then we the probability density function. <br>\n",
    "\n",
    "Generalizing this, likelihood function is defined as:\n",
    "\n",
    "$$L(\\theta) = L(\\theta;y) = f_{Y}(y;\\theta)$$\n",
    "\n",
    "In general, Likelihood function is calculated as follows when there are multiple data points:\n",
    "\n",
    "$$L(\\theta;y) = \\prod_{i=1}^{n} f_{Y}(y_{i};\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's extend the concept of likelihood to Maximum Likelihood Estimation (MLE).\n",
    "# MLE provides the parameter values that make the observed sample most likely among all possible samples. Basically, \n",
    "# it says that the when the parameters are set to MLE values, the observed data is most the likely result to be \n",
    "# observed if an experiment is conducted. \n",
    "#\n",
    "# MLE is referred as any of the following:\n",
    "# 1) Maximum Likelihood Estimation\n",
    "# 2) Maximum Likelihood Estimate\n",
    "# 3) Maximum Likelihood Estimator \n",
    "#\n",
    "# The MLE is calculated by maximizing the likelihood function with respect to the parameter. In other words, we find\n",
    "# the parameter value that maximizes the likelihood function. This is done by taking the derivative of the likelihood\n",
    "# function with respect to the parameter, setting it to zero and solving for the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, $\\hat{\\theta}$ is the maximum likelihood estimate if\n",
    "$$L(\\hat{\\theta}) > L(\\theta_{0})$$\n",
    "for all values of $\\theta_{0}$ in the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have understood the concept of likelihood and MLE, let's see how can we use it in the context of name\n",
    "# generator model we built in the previous notebook (building_makemore_step_by_step/step_2_rule_based_name_generator.ipynb).\n",
    "#\n",
    "# First, let's try to understand what does likelihood even mean in the context of name generator model. Please note\n",
    "# that we are dealing with individual character predictions in the name generator model. So, likelihood also relates \n",
    "# to individual character predictions.\n",
    "#\n",
    "# Let's say we have the name \"Virat\" in the training data. That means \n",
    "# 'V' occured at the first position with 100% certainty.\n",
    "# 'i' occured at the second position with 100% certainty.\n",
    "# 'r' occured at the third position with 100% certainty.\n",
    "# 'a' occured at the fourth position with 100% certainty.\n",
    "# 't' occured at the end position with 100% certainty.\n",
    "#\n",
    "# However, in the name generation model (rule based), the model assigns some probability for each character to occur\n",
    "# at that specific position depending on the previous character. Ideally, all of these probabilities should be 1.0. \n",
    "# But, in practice, they are not. So, the question we need to ask is \"How likely are the model parameters given the \n",
    "# name 'Virat'?\" This is where likelihood comes into picture. Likelihood is the probability of observing the \n",
    "# character-pairs in the name 'Virat' given the model parameters. Higher the likelihood, more likely the model \n",
    "# parameters are.\n",
    "#\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# NOTE: The model parameters are just the pre-computed probabilities in the rule based name generator model.\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# \n",
    "# character-pair -- Some specific consecutive character pair from the name\n",
    "# model_parameters -- The pre-computed probabilities in the model\n",
    "#\n",
    "# Likelihood(model_parameters|character-pair) = Probability of observing the character-pair given the model parameters.\n",
    "#\n",
    "# Now, we have multiple character-pairs in the name 'Virat' and there are 60k such names in the dataset. So, we need to\n",
    "# use all the character-pairs to calculate the likelihood.\n",
    "#\n",
    "# data -- All the character-pairs in the entire dataset\n",
    "#\n",
    "# Likelihood(model_parameters|data) = Probability of observing the data given the model parameters.\n",
    "#\n",
    "# Higher the likelihood, more likely the model parameters are.\n",
    "#\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Now, let's think about how high can likelihood be for our model?\n",
    "# Likelihood is the product of probabilities where each probability is between 0 and 1. So, the maximum value \n",
    "# likelihood can take is 1.0. The closer the likelihood is to 1.0, the better the model parameters are i.e., the \n",
    "# better the model is at predicting the character-pairs in the training dataset.\n",
    "#\n",
    "# Thinking about this a bit, we can see that the likelihood is a good measure of the quality of the model. If the\n",
    "# likelihood is high, then the model is good at predicting the character-pairs in the dataset. If the likelihood is \n",
    "# low, then the model is not good at predicting the character-pairs in the dataset.\n",
    "#\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Since, we have thousands of data points, the likelihood value can be very small. To avoid the floating point issues \n",
    "# with small likelihood values, we generally take the log of the likelihood. This is called log-likelihood. \n",
    "# Log-likelihood is the log of the likelihood value. The log-likelihood value is generally more interpretable than \n",
    "# the likelihood value.\n",
    "#\n",
    "# Log-likelihood(model_parameters|data) = log(Probability of observing the data given the model parameters)\n",
    "#\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#\n",
    "# In general, loss functions are used to train the model. However, loss functions traditionally follow the standard\n",
    "# that lower the loss, better the model. But, likelihood has the opposite behavior. Higher the likelihood, better \n",
    "# the model. So, we generally convert the likelihood to loss by taking the negative of the log-likelihood. This is \n",
    "# called negative log-likelihood.\n",
    "#\n",
    "# Negative Log-likelihood(model_parameters|data) = -log(Probability of observing the data given the model parameters)\n",
    "#\n",
    "# ---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, Likelihood is given by\n",
    "\n",
    "$$L(\\theta;y) = \\prod_{i=1}^{n} f_{Y}(y_{i};\\theta)$$\n",
    "\n",
    "Let's calculate log-likelihood from likelihood.\n",
    "\n",
    "$$ln(L(\\theta;y)) = \\ln(\\prod_{i=1}^{n} f_{Y}(y_{i};\\theta))$$\n",
    "$$\\implies \\ln(L(\\theta;y)) = \\ln(f_{Y}(y_{1};\\theta) * f_{Y}(y_{2};\\theta) * ... * f_{Y}(y_{n};\\theta))$$\n",
    "\n",
    "In general, $$\\ln(a * b * c) = \\ln(a) + \\ln(b) + \\ln(c)$$\n",
    "\n",
    "$$\\implies \\ln(L(\\theta;y)) = \\ln(f_{Y}(y_{1};\\theta)) + \\ln(f_{Y}(y_{2};\\theta)) + ... + \\ln(f_{Y}(y_{n};\\theta))$$\n",
    "$$\\implies LogLikelihood = \\sum_{i=1}^{n} \\ln(f_{Y}(y_i;\\theta))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27])\n",
      "tensor([[0.8199, 0.0169, 0.6668, 0.9757, 0.0602, 0.8629, 0.8615, 0.1943, 0.2582,\n",
      "         0.2903, 0.5666, 0.0518, 0.5317, 0.2576, 0.0476, 0.9891, 0.6111, 0.4120,\n",
      "         0.5988, 0.0928, 0.2383, 0.6663, 0.6120, 0.1714, 0.8228, 0.6810, 0.8106],\n",
      "        [0.9953, 0.8853, 0.0206, 0.3547, 1.0000, 0.6732, 0.3945, 0.7243, 0.1043,\n",
      "         0.4440, 0.8964, 0.5391, 0.7529, 0.4843, 0.5598, 0.1409, 0.4468, 0.1883,\n",
      "         0.5248, 0.5410, 0.9613, 0.2522, 0.6913, 0.4342, 0.2568, 0.9773, 0.2200],\n",
      "        [0.8267, 0.8841, 0.5229, 0.7684, 0.2407, 0.0153, 0.0343, 0.0102, 0.2351,\n",
      "         0.4898, 0.8461, 0.1143, 0.3846, 0.1188, 0.9163, 0.9980, 0.4954, 0.4438,\n",
      "         0.9770, 0.5059, 0.1605, 0.3559, 0.1445, 0.1512, 0.6114, 0.9607, 0.2002],\n",
      "        [0.2143, 0.7254, 0.5864, 0.9842, 0.7713, 0.7198, 0.3779, 0.5235, 0.8627,\n",
      "         0.7900, 0.5658, 0.2948, 0.7811, 0.1567, 0.0765, 0.8027, 0.2379, 0.9157,\n",
      "         0.6232, 0.1776, 0.4618, 0.5658, 0.0497, 0.4491, 0.1966, 0.0996, 0.9648],\n",
      "        [0.2598, 0.5122, 0.5263, 0.5845, 0.5400, 0.7646, 0.1234, 0.9778, 0.2689,\n",
      "         0.5527, 0.7338, 0.9168, 0.0708, 0.5968, 0.5794, 0.2931, 0.0142, 0.8082,\n",
      "         0.5626, 0.3887, 0.0372, 0.9150, 0.7575, 0.6147, 0.5715, 0.4275, 0.3278],\n",
      "        [0.9536, 0.7041, 0.1329, 0.6224, 0.3543, 0.4354, 0.1294, 0.8540, 0.6541,\n",
      "         0.8528, 0.9828, 0.1895, 0.8927, 0.9156, 0.4274, 0.5466, 0.7831, 0.6017,\n",
      "         0.7164, 0.0021, 0.4735, 0.5531, 0.0617, 0.0428, 0.5177, 0.1592, 0.8262],\n",
      "        [0.8354, 0.2008, 0.7353, 0.8194, 0.6964, 0.2329, 0.5489, 0.4999, 0.6622,\n",
      "         0.0174, 0.2723, 0.9620, 0.6347, 0.6645, 0.8849, 0.3310, 0.5592, 0.1549,\n",
      "         0.4565, 0.2050, 0.5624, 0.6333, 0.6783, 0.0775, 0.6861, 0.9191, 0.1306],\n",
      "        [0.6074, 0.8395, 0.0860, 0.4476, 0.7715, 0.2819, 0.7738, 0.6728, 0.9454,\n",
      "         0.2788, 0.3448, 0.8891, 0.0296, 0.0960, 0.3171, 0.7448, 0.8816, 0.0764,\n",
      "         0.5528, 0.1034, 0.4240, 0.8737, 0.8385, 0.8070, 0.7008, 0.2727, 0.3962],\n",
      "        [0.7268, 0.6987, 0.7684, 0.7339, 0.3783, 0.9009, 0.6521, 0.9033, 0.1643,\n",
      "         0.4706, 0.0634, 0.6326, 0.6781, 0.5580, 0.8814, 0.0913, 0.0635, 0.0123,\n",
      "         0.6586, 0.7995, 0.4729, 0.9864, 0.3349, 0.9035, 0.8151, 0.2071, 0.3020],\n",
      "        [0.6770, 0.9320, 0.5660, 0.4276, 0.9372, 0.1021, 0.5021, 0.0170, 0.4235,\n",
      "         0.9758, 0.9299, 0.9557, 0.4173, 0.3045, 0.2417, 0.2164, 0.7569, 0.6562,\n",
      "         0.1655, 0.7543, 0.2318, 0.1467, 0.9177, 0.4600, 0.4674, 0.2321, 0.0432],\n",
      "        [0.7622, 0.1154, 0.0426, 0.9866, 0.0925, 0.6541, 0.8615, 0.7347, 0.0685,\n",
      "         0.4968, 0.3355, 0.4286, 0.3643, 0.4461, 0.2493, 0.8612, 0.1956, 0.4335,\n",
      "         0.4801, 0.5060, 0.7525, 0.2626, 0.9127, 0.7401, 0.8884, 0.2317, 0.4897],\n",
      "        [0.1414, 0.7768, 0.7006, 0.2063, 0.2760, 0.3825, 0.0724, 0.8504, 0.0912,\n",
      "         0.2599, 0.0019, 0.9978, 0.8716, 0.6551, 0.6526, 0.1977, 0.9146, 0.8736,\n",
      "         0.4837, 0.4056, 0.7605, 0.3200, 0.4166, 0.7103, 0.1096, 0.3089, 0.5317],\n",
      "        [0.0163, 0.8105, 0.5295, 0.0166, 0.5733, 0.4801, 0.4702, 0.4398, 0.6475,\n",
      "         0.7377, 0.2874, 0.3532, 0.7616, 0.1481, 0.1745, 0.0808, 0.3639, 0.1365,\n",
      "         0.8538, 0.3135, 0.3211, 0.0768, 0.8481, 0.6784, 0.6763, 0.6331, 0.2929],\n",
      "        [0.5159, 0.0599, 0.3999, 0.2780, 0.4522, 0.4081, 0.1829, 0.2672, 0.9172,\n",
      "         0.5921, 0.8718, 0.8980, 0.4213, 0.0416, 0.1629, 0.3035, 0.6511, 0.9633,\n",
      "         0.3959, 0.0156, 0.1844, 0.2788, 0.0800, 0.9335, 0.4854, 0.2559, 0.8674],\n",
      "        [0.1437, 0.8146, 0.2770, 0.8145, 0.0647, 0.1398, 0.7615, 0.1173, 0.4425,\n",
      "         0.5828, 0.3506, 0.5805, 0.8136, 0.0087, 0.8328, 0.5313, 0.3354, 0.3823,\n",
      "         0.4623, 0.4585, 0.4290, 0.8692, 0.4929, 0.6815, 0.3384, 0.3102, 0.1419],\n",
      "        [0.1752, 0.4629, 0.8557, 0.5768, 0.8836, 0.9325, 0.2659, 0.1532, 0.8131,\n",
      "         0.8667, 0.2884, 0.0028, 0.5436, 0.3936, 0.7230, 0.8943, 0.8403, 0.6202,\n",
      "         0.5324, 0.2801, 0.4880, 0.7330, 0.5999, 0.3353, 0.5459, 0.8606, 0.5028],\n",
      "        [0.6606, 0.1662, 0.9738, 0.1789, 0.3603, 0.1453, 0.6466, 0.6946, 0.5310,\n",
      "         0.3019, 0.2886, 0.1949, 0.9536, 0.7146, 0.1964, 0.1829, 0.2841, 0.2175,\n",
      "         0.3240, 0.8307, 0.7171, 0.8810, 0.7232, 0.2662, 0.3333, 0.5192, 0.8052],\n",
      "        [0.7402, 0.0084, 0.6982, 0.3819, 0.9728, 0.3523, 0.1835, 0.4869, 0.4104,\n",
      "         0.3350, 0.8458, 0.8663, 0.2871, 0.5381, 0.0331, 0.5339, 0.7224, 0.3559,\n",
      "         0.9508, 0.7287, 0.5829, 0.3939, 0.1844, 0.6350, 0.3292, 0.9793, 0.5895],\n",
      "        [0.5018, 0.0436, 0.4142, 0.2624, 0.1961, 0.0880, 0.1177, 0.0770, 0.2570,\n",
      "         0.6760, 0.5523, 0.2439, 0.7463, 0.4604, 0.7519, 0.6478, 0.8983, 0.6485,\n",
      "         0.4611, 0.2649, 0.1418, 0.6881, 0.6938, 0.6634, 0.9691, 0.6176, 0.0315],\n",
      "        [0.5306, 0.9244, 0.0474, 0.5187, 0.5857, 0.9966, 0.1649, 0.7947, 0.8158,\n",
      "         0.4582, 0.6208, 0.1841, 0.4508, 0.1974, 0.0905, 0.7049, 0.6791, 0.6031,\n",
      "         0.2318, 0.4856, 0.1440, 0.6608, 0.9855, 0.2960, 0.4210, 0.2256, 0.1180],\n",
      "        [0.6860, 0.4318, 0.0827, 0.1804, 0.7282, 0.3439, 0.9008, 0.4031, 0.2740,\n",
      "         0.5896, 0.8425, 0.3887, 0.8967, 0.0650, 0.9938, 0.9013, 0.7003, 0.1829,\n",
      "         0.3320, 0.3405, 0.5374, 0.5562, 0.2993, 0.0257, 0.2970, 0.3096, 0.5620],\n",
      "        [0.6624, 0.3504, 0.7626, 0.7237, 0.5227, 0.2780, 0.6611, 0.5215, 0.1266,\n",
      "         0.3499, 0.3072, 0.0908, 0.2001, 0.5753, 0.6981, 0.8798, 0.5922, 0.1846,\n",
      "         0.2248, 0.7281, 0.9060, 0.3118, 0.4945, 0.9056, 0.8322, 0.6099, 0.6568],\n",
      "        [0.6240, 0.3287, 0.3525, 0.2832, 0.5781, 0.9629, 0.3467, 0.9064, 0.5093,\n",
      "         0.2521, 0.7361, 0.2186, 0.1290, 0.8464, 0.8545, 0.4385, 0.7367, 0.7945,\n",
      "         0.3939, 0.1981, 0.9901, 0.0693, 0.1868, 0.6468, 0.0873, 0.6300, 0.3799],\n",
      "        [0.7176, 0.4803, 0.1657, 0.3610, 0.1297, 0.3336, 0.0594, 0.7276, 0.6883,\n",
      "         0.6641, 0.8702, 0.5136, 0.6459, 0.7934, 0.9154, 0.5018, 0.0827, 0.2007,\n",
      "         0.3423, 0.4892, 0.0841, 0.7671, 0.4610, 0.5721, 0.3800, 0.8761, 0.9933],\n",
      "        [0.0879, 0.8889, 0.7281, 0.5487, 0.2563, 0.2020, 0.4953, 0.9489, 0.5427,\n",
      "         0.2002, 0.1918, 0.4686, 0.1201, 0.4101, 0.9438, 0.5031, 0.6195, 0.3446,\n",
      "         0.2299, 0.6849, 0.5916, 0.2645, 0.8820, 0.2463, 0.8441, 0.9815, 0.7772],\n",
      "        [0.6242, 0.1045, 0.8835, 0.9647, 0.1450, 0.1985, 0.9022, 0.7439, 0.8795,\n",
      "         0.7888, 0.7173, 0.9651, 0.8075, 0.7357, 0.7467, 0.0074, 0.9904, 0.7329,\n",
      "         0.9468, 0.7580, 0.4451, 0.7227, 0.1360, 0.3686, 0.1910, 0.4876, 0.1175],\n",
      "        [0.4989, 0.9330, 0.2050, 0.0643, 0.4623, 0.0674, 0.0083, 0.2221, 0.8882,\n",
      "         0.6666, 0.9556, 0.9947, 0.4870, 0.2924, 0.8535, 0.6698, 0.2099, 0.6133,\n",
      "         0.4568, 0.1010, 0.2067, 0.4201, 0.3168, 0.9487, 0.6800, 0.3780, 0.8536]])\n"
     ]
    }
   ],
   "source": [
    "# Let me create a random probabilities tensor similar to the one we have in the rule based name generator model.\n",
    "# We will use this tensor to calculate the log-likelihood.\n",
    "probs = torch.rand(size=(27, 27), dtype=torch.float32)\n",
    "print(probs.shape)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n"
     ]
    }
   ],
   "source": [
    "char_to_int = {char: idx + 1 for idx, char in enumerate(string.ascii_lowercase)}\n",
    "char_to_int['.'] = 0\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log-likelihood or log_loss: 18.903888702392578\n",
      "Normalized Negative Log-likelihood or Normalized log_loss: 1.5753240585327148\n"
     ]
    }
   ],
   "source": [
    "# Consider the following two names in the dataset.add\n",
    "# 1) 'virat'\n",
    "# 2) 'prabhas'\n",
    "#\n",
    "# Let's calculate the log-likelihood for these two names using the model parameters (random probs created above) we have.\n",
    "# The log-likelihood is the sum of the log of the probabilities of the character-pairs in the name as explained above.\n",
    "log_loss = 0.0\n",
    "num_char_pairs = 0\n",
    "for name in ['virat', 'prabhas']:\n",
    "    num_char_pairs += len(name)\n",
    "    name = '.' + name + '.'\n",
    "    for first_char, second_char in zip(name, name[1:]):\n",
    "        first_char_idx = char_to_int[first_char]\n",
    "        second_char_idx = char_to_int[second_char]\n",
    "        # Calculate the log-likelihood for the character-pair.\n",
    "        log_loss = log_loss + torch.log(probs[first_char_idx, second_char_idx])\n",
    "\n",
    "# Take the negative of the log-likelihood to get the negative log-likelihood.\n",
    "log_loss = -log_loss\n",
    "print(f\"Negative Log-likelihood or log_loss: {log_loss}\")\n",
    "# It is good to calculate the normalized log loss. This sometimes can help in interpreting the log loss value.\n",
    "normalized_log_loss = log_loss / num_char_pairs\n",
    "print(f\"Normalized Negative Log-likelihood or Normalized log_loss: {normalized_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".makemore_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
