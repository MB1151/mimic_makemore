{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) What is likelihood?\n",
    "# 2) What is Maximum Likelihood Estimation (MLE)? \n",
    "# 3) How to use likelihood in the context of our name generation problem?\n",
    "# 4) How to evaluate the quality of the model using the concept of likelihood?\n",
    "#\n",
    "# Resources:\n",
    "#\n",
    "# 1) https://www.youtube.com/watch?v=ScduwntrMzc&t=41s\n",
    "#       -- This video explains the concept of likelihood and several other related concepts.\n",
    "# 2) https://www.youtube.com/watch?v=7kLHJ-F33GI&t=40s\n",
    "#       -- This video explains the concept of Maximum Likelihood Estimation (MLE) and several other related concepts.\n",
    "#       -- First 20 minutes of this video is enough to understand the concept of MLE.\n",
    "#\n",
    "# Please watch these videos before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with understanding Likelihood first.\n",
    "#\n",
    "# Let's say we have a coin that was flipped 'n' times and we observed heads 'h' times. Now, we want to estimate the \n",
    "# probability 'p' of getting heads when we flip the coin. We can use the likelihood function to estimate the probability.\n",
    "#\n",
    "# Likelihood shows how likely a particular probability 'p' (0.5 for example) is given the data we have. Higher the\n",
    "# likelihood, more likely the particular probability value 'p' is. However, likelihood by itself doesn't give us very \n",
    "# useful information. It is the relative likelihood that is generally more useful. Relative likelihood is the ratio of \n",
    "# likelihood for two probability values (p). For example, if the likelihood of getting heads with probability 0.5 is \n",
    "# 0.1 and the likelihood of getting heads with probability 0.6 is 0.2, then the relative likelihood of getting heads\n",
    "# with probability 0.6 is (0.2/0.1) = 2.0.\n",
    "#\n",
    "# For the case of coin flip, likelihood is defined as:\n",
    "# Likelihood(p|data) = Probabability of observing the data given the heads probability is 'p'\n",
    "#\n",
    "# In general, let 'theta' be the population parameter (equivalent to 'p' in the coin flip example) and 'Y' be the data. \n",
    "# Then, the likelihood function is defined as:\n",
    "# \n",
    "# Likelihood(theta|Y) = Probabaility of observing the data 'Y' given that the population parameter is 'theta'.\n",
    "#\n",
    "# Likelihood describes the extent to which the data supports any particular value of the population parameter. \n",
    "# Higher support corresponds to higher value of likelihood.\n",
    "#\n",
    "# Extending this concept to the case of multiple data points, the likelihood function is a function that relates the \n",
    "# population parameter to the data. Basically, likelihood function is just a mathematical function that takes in data,\n",
    "# population parameter as input and returns the likelihood of the population parameter.\n",
    "#\n",
    "# NOTE: In most places (textbooks, blogs, ... etc), data is referred to as sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, there are usually multiple data points for the same population parameter. In such case, we need to use \n",
    "# all the data points to calculate the likelihood. In this case, the likelihood is the product of the likelihoods of \n",
    "# each data point. This turns out to be the product of the probabilities of observing each data point given the \n",
    "# population parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, likelihood is defined as:\n",
    "\n",
    "$$L(\\theta_{0};y) = Prob(Y=y;\\theta=\\theta_{0}) = f_{Y}(y;\\theta_{0})$$\n",
    "\n",
    "If it is a discrete probability distribution, then we use the probability mass function. <br>\n",
    "If it is a continuous probability distribution, then we the probability density function. <br>\n",
    "\n",
    "Generalizing this, likelihood function is defined as:\n",
    "\n",
    "$$L(\\theta) = L(\\theta;y) = f_{Y}(y;\\theta)$$\n",
    "\n",
    "In general, Likelihood function is calculated as follows when there are multiple data points:\n",
    "\n",
    "$$L(\\theta;y) = \\prod_{i=1}^{n} f_{Y}(y_{i};\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's extend the concept of likelihood to Maximum Likelihood Estimation (MLE).\n",
    "# MLE provides the parameter values that make the observed sample most likely among all possible samples. Basically, \n",
    "# it says that the when the parameters are set to MLE values, the observed data is most the likely result to be \n",
    "# observed if an experiment is conducted. \n",
    "#\n",
    "# MLE is referred as any of the following:\n",
    "# 1) Maximum Likelihood Estimation\n",
    "# 2) Maximum Likelihood Estimate\n",
    "# 3) Maximum Likelihood Estimator \n",
    "#\n",
    "# The MLE is calculated by maximizing the likelihood function with respect to the parameter. In other words, we find\n",
    "# the parameter value that maximizes the likelihood function. This is done by taking the derivative of the likelihood\n",
    "# function with respect to the parameter, setting it to zero and solving for the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, $\\hat{\\theta}$ is the maximum likelihood estimate if\n",
    "$$L(\\hat{\\theta}) > L(\\theta_{0})$$\n",
    "for all values of $\\theta_{0}$ in the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have understood the concept of likelihood and MLE, let's see how can we use it in the context of name\n",
    "# generator model we built in the previous notebook (building_makemore_step_by_step/step_2_rule_based_name_generator.ipynb).\n",
    "#\n",
    "# First, let's try to understand what does likelihood even mean in the context of name generator model. Please note\n",
    "# that we are dealing with individual character predictions in the name generator model. So, likelihood also relates \n",
    "# to individual character predictions.\n",
    "#\n",
    "# Let's say we have the name \"Virat\" in the training data. That means \n",
    "# 'V' occured at the first position with 100% certainty.\n",
    "# 'i' occured at the second position with 100% certainty.\n",
    "# 'r' occured at the third position with 100% certainty.\n",
    "# 'a' occured at the fourth position with 100% certainty.\n",
    "# 't' occured at the end position with 100% certainty.\n",
    "#\n",
    "# However, in the name generation model (rule based), the model assigns some probability for each character to occur\n",
    "# at that specific position depending on the previous character. Ideally, all of these probabilities should be 1.0. \n",
    "# But, in practice, they are not. So, the question we need to ask is \"How likely are the model parameters given the \n",
    "# name 'Virat'?\" This is where likelihood comes into picture. Likelihood is the probability of observing the \n",
    "# character-pairs in the name 'Virat' given the model parameters. Higher the likelihood, more likely the model \n",
    "# parameters are.\n",
    "#\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# NOTE: The model parameters are just the pre-computed probabilities in the rule based name generator model.\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# \n",
    "# character-pair -- Some specific consecutive character pair from the name.\n",
    "# model_parameters -- The pre-computed probabilities in the model.\n",
    "#\n",
    "# Likelihood(model_parameters|character-pair) = Probability of observing the character-pair given the model parameters.\n",
    "#\n",
    "# Now, we have multiple character-pairs in the name 'Virat' and there are 60k such names in the dataset. So, we need to\n",
    "# use all the character-pairs to calculate the likelihood.\n",
    "#\n",
    "# data -- All the character-pairs in the entire dataset.\n",
    "#\n",
    "# Likelihood(model_parameters|data) = Probability of observing the data given the model parameters.\n",
    "#\n",
    "# Higher the likelihood, more likely the model parameters are.\n",
    "#\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Now, let's think about how high can likelihood be used for our model?\n",
    "# Likelihood is the product of probabilities where each probability is between 0 and 1. So, the maximum value \n",
    "# likelihood can take is 1.0. The closer the likelihood is to 1.0, the better the model parameters are i.e., the \n",
    "# better the model is at predicting the character-pairs in the training dataset.\n",
    "#\n",
    "# Thinking about this a bit, we can see that the likelihood is a good measure of the quality of the model. If the\n",
    "# likelihood is high, then the model is good at predicting the character-pairs in the dataset. If the likelihood is \n",
    "# low, then the model is not good at predicting the character-pairs in the dataset.\n",
    "#\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Since, we have thousands of data points, the likelihood value can be very small. To avoid the floating point issues \n",
    "# with small likelihood values, we generally take the log of the likelihood. This is called log-likelihood. \n",
    "# Log-likelihood is the log of the likelihood value. The log-likelihood value is generally more interpretable than \n",
    "# the likelihood value.\n",
    "#\n",
    "# Log-likelihood(model_parameters|data) = log(Probability of observing the data given the model parameters)\n",
    "#\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#\n",
    "# In general, loss functions are used to train the model. However, loss functions traditionally follow the standard\n",
    "# that lower the loss, better the model. But, likelihood has the opposite behavior. Higher the likelihood, better \n",
    "# the model. So, we generally convert the likelihood to loss by taking the negative of the log-likelihood. This is \n",
    "# called negative log-likelihood.\n",
    "#\n",
    "# Negative Log-likelihood(model_parameters|data) = -log(Probability of observing the data given the model parameters)\n",
    "#\n",
    "# ---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, Likelihood is given by\n",
    "\n",
    "$$L(\\theta;y) = \\prod_{i=1}^{n} f_{Y}(y_{i};\\theta)$$\n",
    "\n",
    "Let's calculate log-likelihood from likelihood.\n",
    "\n",
    "$$ln(L(\\theta;y)) = \\ln(\\prod_{i=1}^{n} f_{Y}(y_{i};\\theta))$$\n",
    "$$\\implies \\ln(L(\\theta;y)) = \\ln(f_{Y}(y_{1};\\theta) * f_{Y}(y_{2};\\theta) * ... * f_{Y}(y_{n};\\theta))$$\n",
    "\n",
    "In general, $$\\ln(a * b * c) = \\ln(a) + \\ln(b) + \\ln(c)$$\n",
    "\n",
    "$$\\implies \\ln(L(\\theta;y)) = \\ln(f_{Y}(y_{1};\\theta)) + \\ln(f_{Y}(y_{2};\\theta)) + ... + \\ln(f_{Y}(y_{n};\\theta))$$\n",
    "$$\\implies LogLikelihood = \\sum_{i=1}^{n} \\ln(f_{Y}(y_i;\\theta))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27])\n",
      "tensor([[0.0606, 0.3155, 0.3703, 0.0925, 0.3342, 0.9187, 0.0161, 0.2763, 0.4431,\n",
      "         0.3939, 0.5886, 0.9199, 0.4528, 0.8423, 0.6368, 0.4088, 0.8076, 0.0119,\n",
      "         0.1013, 0.1469, 0.8517, 0.3040, 0.2748, 0.5576, 0.6734, 0.6187, 0.5335],\n",
      "        [0.8193, 0.1403, 0.8351, 0.8965, 0.8788, 0.7559, 0.1769, 0.4117, 0.4928,\n",
      "         0.5601, 0.9379, 0.1382, 0.7537, 0.1907, 0.4578, 0.9954, 0.6802, 0.0898,\n",
      "         0.8701, 0.6965, 0.2352, 0.5502, 0.3446, 0.2392, 0.9965, 0.0893, 0.5960],\n",
      "        [0.5915, 0.9247, 0.7120, 0.5068, 0.9814, 0.7232, 0.9259, 0.1437, 0.1048,\n",
      "         0.5992, 0.3271, 0.0415, 0.5631, 0.9979, 0.9045, 0.1884, 0.5286, 0.7172,\n",
      "         0.4548, 0.8255, 0.8548, 0.3602, 0.6327, 0.7611, 0.5000, 0.2107, 0.6601],\n",
      "        [0.5832, 0.6810, 0.9763, 0.7383, 0.9999, 0.5205, 0.9395, 0.6658, 0.4594,\n",
      "         0.6697, 0.7130, 0.3808, 0.5926, 0.4847, 0.3609, 0.0166, 0.3119, 0.4930,\n",
      "         0.5470, 0.8848, 0.4311, 0.4799, 0.6165, 0.9476, 0.5108, 0.3714, 0.7660],\n",
      "        [0.9806, 0.0237, 0.9333, 0.2030, 0.8858, 0.5464, 0.2077, 0.1851, 0.7819,\n",
      "         0.5707, 0.2549, 0.5478, 0.1769, 0.1430, 0.3409, 0.8087, 0.9291, 0.9605,\n",
      "         0.5409, 0.3105, 0.0287, 0.0763, 0.6158, 0.7769, 0.3002, 0.7899, 0.7371],\n",
      "        [0.8856, 0.4974, 0.0074, 0.1232, 0.8600, 0.8483, 0.6857, 0.1748, 0.0945,\n",
      "         0.3569, 0.9753, 0.6566, 0.0483, 0.2536, 0.8206, 0.0246, 0.3595, 0.9455,\n",
      "         0.1816, 0.4402, 0.6831, 0.4144, 0.7129, 0.8078, 0.5679, 0.9953, 0.5990],\n",
      "        [0.1130, 0.4159, 0.7486, 0.5126, 0.9231, 0.1730, 0.0960, 0.7434, 0.3504,\n",
      "         0.7894, 0.9878, 0.6191, 0.1117, 0.1321, 0.9177, 0.2177, 0.4561, 0.4573,\n",
      "         0.7755, 0.0650, 0.9086, 0.5811, 0.0647, 0.1892, 0.3041, 0.3390, 0.8597],\n",
      "        [0.9240, 0.3939, 0.5744, 0.1418, 0.9109, 0.3413, 0.4828, 0.9945, 0.5033,\n",
      "         0.9188, 0.3344, 0.4464, 0.0096, 0.2306, 0.5053, 0.8455, 0.2223, 0.8514,\n",
      "         0.3887, 0.3211, 0.0464, 0.3595, 0.1363, 0.8455, 0.3230, 0.5746, 0.7486],\n",
      "        [0.5507, 0.7581, 0.8469, 0.3473, 0.2513, 0.6423, 0.3767, 0.0477, 0.1603,\n",
      "         0.9522, 0.8364, 0.2587, 0.9623, 0.3080, 0.7220, 0.4380, 0.2661, 0.8167,\n",
      "         0.7617, 0.5140, 0.9233, 0.2270, 0.4092, 0.0561, 0.6414, 0.1174, 0.4999],\n",
      "        [0.3595, 0.8247, 0.3455, 0.9732, 0.7673, 0.8405, 0.5744, 0.5834, 0.5200,\n",
      "         0.6420, 0.2952, 0.0114, 0.3541, 0.2240, 0.6998, 0.5477, 0.6605, 0.1267,\n",
      "         0.8200, 0.9968, 0.2528, 0.0158, 0.5549, 0.0620, 0.9500, 0.6562, 0.6636],\n",
      "        [0.5326, 0.3072, 0.8044, 0.3370, 0.3590, 0.7469, 0.8962, 0.1899, 0.2927,\n",
      "         0.2093, 0.7161, 0.5822, 0.9289, 0.5064, 0.1921, 0.1690, 0.0134, 0.7777,\n",
      "         0.9711, 0.0045, 0.2537, 0.4578, 0.5271, 0.3243, 0.3830, 0.2090, 0.8262],\n",
      "        [0.1627, 0.9112, 0.1146, 0.7092, 0.2834, 0.9176, 0.7985, 0.9429, 0.6534,\n",
      "         0.0387, 0.2569, 0.3534, 0.1159, 0.1863, 0.1019, 0.9510, 0.6149, 0.7131,\n",
      "         0.8177, 0.7004, 0.5030, 0.6573, 0.9750, 0.7474, 0.3570, 0.6028, 0.6614],\n",
      "        [0.7004, 0.4988, 0.9787, 0.5260, 0.3141, 0.5180, 0.1758, 0.3231, 0.5254,\n",
      "         0.8402, 0.9419, 0.7960, 0.3906, 0.3789, 0.8559, 0.7415, 0.2674, 0.8342,\n",
      "         0.3944, 0.3822, 0.1966, 0.5473, 0.9676, 0.2127, 0.2123, 0.3800, 0.1192],\n",
      "        [0.1968, 0.8852, 0.3070, 0.9965, 0.0305, 0.6778, 0.4930, 0.7736, 0.2303,\n",
      "         0.4477, 0.7813, 0.2032, 0.2053, 0.5828, 0.4487, 0.0309, 0.9299, 0.6695,\n",
      "         0.1803, 0.3561, 0.1981, 0.3133, 0.3837, 0.1451, 0.1960, 0.6480, 0.3970],\n",
      "        [0.4823, 0.0868, 0.3010, 0.8256, 0.9283, 0.4915, 0.3930, 0.4099, 0.4739,\n",
      "         0.9703, 0.7515, 0.8926, 0.8084, 0.8530, 0.4377, 0.9339, 0.2172, 0.2347,\n",
      "         0.4062, 0.1866, 0.8231, 0.3685, 0.8685, 0.5410, 0.1058, 0.5134, 0.8964],\n",
      "        [0.4306, 0.7319, 0.9520, 0.5532, 0.9511, 0.7707, 0.3148, 0.5920, 0.1192,\n",
      "         0.6969, 0.2073, 0.2700, 0.9746, 0.2534, 0.2571, 0.2719, 0.5346, 0.3982,\n",
      "         0.4383, 0.0543, 0.5501, 0.9534, 0.1278, 0.8791, 0.1443, 0.6067, 0.3040],\n",
      "        [0.6192, 0.0534, 0.1795, 0.4176, 0.2362, 0.2530, 0.8925, 0.5985, 0.8691,\n",
      "         0.4932, 0.8112, 0.9856, 0.2916, 0.1427, 0.5497, 0.2017, 0.5496, 0.7850,\n",
      "         0.8224, 0.5299, 0.5466, 0.6610, 0.5003, 0.3872, 0.2861, 0.1156, 0.5412],\n",
      "        [0.7334, 0.5343, 0.4907, 0.4019, 0.3583, 0.2537, 0.8702, 0.2766, 0.0212,\n",
      "         0.8603, 0.8386, 0.2526, 0.4739, 0.0265, 0.4372, 0.7176, 0.8496, 0.9308,\n",
      "         0.1698, 0.5009, 0.5561, 0.5281, 0.8610, 0.6621, 0.2482, 0.4788, 0.1156],\n",
      "        [0.5647, 0.2413, 0.8148, 0.4629, 0.0507, 0.5355, 0.4362, 0.5373, 0.6029,\n",
      "         0.2765, 0.7686, 0.4365, 0.3432, 0.2268, 0.8523, 0.6200, 0.1239, 0.9427,\n",
      "         0.7297, 0.9611, 0.9809, 0.5244, 0.9089, 0.2980, 0.2840, 0.3132, 0.8155],\n",
      "        [0.2060, 0.0193, 0.7340, 0.3550, 0.5142, 0.2325, 0.2224, 0.9671, 0.8763,\n",
      "         0.2772, 0.8809, 0.9203, 0.7467, 0.4505, 0.5383, 0.4268, 0.3338, 0.9215,\n",
      "         0.3753, 0.6457, 0.6547, 0.9619, 0.6644, 0.5965, 0.0713, 0.4228, 0.5698],\n",
      "        [0.7417, 0.3435, 0.7490, 0.3558, 0.4348, 0.6821, 0.2605, 0.7050, 0.4885,\n",
      "         0.4053, 0.9446, 0.7992, 0.1579, 0.2238, 0.9323, 0.2280, 0.3739, 0.9380,\n",
      "         0.2260, 0.5552, 0.3572, 0.1725, 0.8390, 0.7161, 0.4383, 0.5953, 0.2631],\n",
      "        [0.5564, 0.0119, 0.3307, 0.4032, 0.2881, 0.7248, 0.6937, 0.2631, 0.2240,\n",
      "         0.2654, 0.1964, 0.8882, 0.9109, 0.0291, 0.4724, 0.4761, 0.2044, 0.8093,\n",
      "         0.5292, 0.3428, 0.4377, 0.7917, 0.2980, 0.4957, 0.0825, 0.0579, 0.8959],\n",
      "        [0.6015, 0.9610, 0.0596, 0.9314, 0.7608, 0.7086, 0.2164, 0.1240, 0.1275,\n",
      "         0.6195, 0.8668, 0.9777, 0.6923, 0.7241, 0.3375, 0.8609, 0.8451, 0.5881,\n",
      "         0.3670, 0.5828, 0.6584, 0.7201, 0.0762, 0.2386, 0.7450, 0.5168, 0.2265],\n",
      "        [0.6789, 0.1176, 0.5579, 0.7799, 0.1871, 0.4591, 0.3236, 0.1972, 0.9303,\n",
      "         0.8169, 0.1663, 0.8978, 0.8638, 0.0632, 0.9376, 0.1740, 0.0424, 0.4199,\n",
      "         0.7781, 0.2123, 0.0161, 0.4063, 0.4158, 0.3290, 0.1308, 0.7942, 0.7480],\n",
      "        [0.3811, 0.7334, 0.6719, 0.6521, 0.3435, 0.7428, 0.4607, 0.6514, 0.2343,\n",
      "         0.3475, 0.0752, 0.4110, 0.3837, 0.4260, 0.0514, 0.8171, 0.4990, 0.5910,\n",
      "         0.4433, 0.2959, 0.5804, 0.2604, 0.5723, 0.4076, 0.2371, 0.9873, 0.4190],\n",
      "        [0.6283, 0.0408, 0.7800, 0.3543, 0.7621, 0.0236, 0.1640, 0.8810, 0.7954,\n",
      "         0.3063, 0.2204, 0.6816, 0.4282, 0.6266, 0.1026, 0.8198, 0.8382, 0.0353,\n",
      "         0.7041, 0.6851, 0.0680, 0.4321, 0.7531, 0.0785, 0.6347, 0.5320, 0.8670],\n",
      "        [0.1101, 0.7494, 0.1971, 0.5780, 0.2083, 0.2424, 0.5747, 0.0577, 0.1647,\n",
      "         0.0060, 0.3248, 0.0780, 0.9231, 0.1606, 0.6992, 0.6455, 0.5497, 0.2170,\n",
      "         0.5130, 0.3594, 0.1099, 0.9269, 0.1389, 0.5757, 0.0636, 0.7119, 0.4667]])\n"
     ]
    }
   ],
   "source": [
    "# Let me create a random probabilities tensor similar to the one we have in the rule based name generator model.\n",
    "# We will use this tensor to calculate the log-likelihood.\n",
    "probs = torch.rand(size=(27, 27), dtype=torch.float32)\n",
    "print(probs.shape)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n"
     ]
    }
   ],
   "source": [
    "char_to_int = {char: idx + 1 for idx, char in enumerate(string.ascii_lowercase)}\n",
    "char_to_int['.'] = 0\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log-likelihood or log_loss: 11.622832298278809\n",
      "Normalized Negative Log-likelihood or Normalized log_loss: 0.8302022814750671\n"
     ]
    }
   ],
   "source": [
    "# Consider the following two names in the dataset.add\n",
    "# 1) 'virat'\n",
    "# 2) 'prabhas'\n",
    "#\n",
    "# Let's calculate the log-likelihood for these two names using the model parameters (random probs created above) we have.\n",
    "# The log-likelihood is the sum of the log of the probabilities of the character-pairs in the name as explained above.\n",
    "log_loss = 0.0\n",
    "num_char_pairs = 0\n",
    "for name in ['virat', 'prabhas']:\n",
    "    num_char_pairs += len(name) + 1\n",
    "    name = '.' + name + '.'\n",
    "    for first_char, second_char in zip(name, name[1:]):\n",
    "        first_char_idx = char_to_int[first_char]\n",
    "        second_char_idx = char_to_int[second_char]\n",
    "        # Calculate the log-likelihood for the character-pair.\n",
    "        log_loss = log_loss + torch.log(probs[first_char_idx, second_char_idx])\n",
    "\n",
    "# Take the negative of the log-likelihood to get the negative log-likelihood.\n",
    "log_loss = -log_loss\n",
    "print(f\"Negative Log-likelihood or log_loss: {log_loss}\")\n",
    "# It is good to calculate the normalized log loss. This sometimes can help in interpreting the log loss value.\n",
    "normalized_log_loss = log_loss / num_char_pairs\n",
    "print(f\"Normalized Negative Log-likelihood or Normalized log_loss: {normalized_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".makemore_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
